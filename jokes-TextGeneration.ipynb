{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniques for generating text\n",
    "\n",
    "Here we discuss some common techniques for generating text once we have build a model. So far we have built a model that can output a single character given a sequence (8 in our case) input characters. Out goal is to generate a continuous set of characters. The main approaches we will go over are: \n",
    "\n",
    "#### Greedy or best fit: \n",
    "This is the simplest method in which we recursively select the most probable next sequence character. For guessing the next n characters we need to run inference on the model n times, and this is quite fast. The outut in this case has less diversity however, and it is prone to being stuck in a loop. There seem to be a set of char sequences which are highly probable and the prediction often converges to  one of these sequences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical (multinomial) distribution \n",
    "Models the occurance of an outcome out of k categories each with defined probability of occurances. \n",
    "In order to inject more diversity into the outputs, instead of selecting the most probable haracter, we model the NN as a probability event and observe which token is output with one 'play'. This adds some serendipity into the system.\n",
    "\n",
    "Observing the results however, this method cuts across words. The structure inherent within words is broken more easily. One alternative would be to use the multinomial distribution sparingly, in combination with the greedy best fit approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 3 : Greedy for words, best fit after a space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search\n",
    "Beam search is a method of trying to get more 'optimal results' and we look at predicting sequences longer than 1 character in one 'iteration'. Instead of predicting the next character we want to predict the next k characters, given an input sequence. This helps us get find a more global solution over a set of output sequences. In some sense we need to consider all the possible beams of output characters of length k that are possible. and need to have some metric of comparison. A couple of things help us out while calculating probabilitues: \n",
    "First, using bayes rule, we can model the probability of a getting a particular output sequence as aproduct of individual conditional probability sequences. \n",
    "\n",
    "For eg: Score of predicting 'and' given input equence 'The sun ' will be \n",
    "score = P('a'| input = 'The sun ') * P('n'| input = 'he sun a') * P('d' | input = 'e sun an')  \n",
    "Each of the P values above can be obtained by running \n",
    "Second, the output of the model softmas is often in log format, and this makes out implementations easier, we can add the log values instead of multiplying them. \n",
    "\n",
    "#### Computational considerations/Tweaks to Beam search\n",
    "* Attempt to reduce the bottleneck - > charscore function\n",
    "* Sequences of alphabets only _ 30 * 30 * 30 \n",
    "* Instead of sampling all combinations, most probable 10 in each case. \n",
    "    Significantly faster results OR get beam sequences of longer length\n",
    "    (10 *10 *10 *10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nik/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from fastai.io import *\n",
    "from fastai.conv_learner import *\n",
    "from fastai.column_data import *\n",
    "\n",
    "from fastai.nlp import *\n",
    "from fastai.lm_rnn import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "from torchtext import vocab, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup for Jokes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Now I have to say \"Leroy can you please paint ...</td>\n",
       "      <td>5tz52q</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate how you cant even say black paint anymore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pizza doesn't scream when you put it in the ov...</td>\n",
       "      <td>5tz4dd</td>\n",
       "      <td>0</td>\n",
       "      <td>What's the difference between a Jew in Nazi Ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...and being there really helped me learn abou...</td>\n",
       "      <td>5tz319</td>\n",
       "      <td>0</td>\n",
       "      <td>I recently went to America....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body      id  score  \\\n",
       "0  Now I have to say \"Leroy can you please paint ...  5tz52q      1   \n",
       "1  Pizza doesn't scream when you put it in the ov...  5tz4dd      0   \n",
       "2  ...and being there really helped me learn abou...  5tz319      0   \n",
       "\n",
       "                                               title  \n",
       "0   I hate how you cant even say black paint anymore  \n",
       "1  What's the difference between a Jew in Nazi Ge...  \n",
       "2                     I recently went to America....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read in the jokes\n",
    "PATH='/home/nik/data/jokes/'\n",
    "jkdf = pd.read_json(\"/home/nik/data/jokes/reddit_jokes.json\")\n",
    "jkdf[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top rated short clean jokes\n",
    "* clean missing for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>On the condition he gets to install windows.\\n...</td>\n",
       "      <td>5tn84z</td>\n",
       "      <td>48526</td>\n",
       "      <td>Breaking News: Bill Gates has agreed to pay fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20074</th>\n",
       "      <td>/r/Jokes</td>\n",
       "      <td>4xjyho</td>\n",
       "      <td>45500</td>\n",
       "      <td>I found a place where the recycling rate is 98%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3042</th>\n",
       "      <td>But its a silly comparison really, its like co...</td>\n",
       "      <td>5s9jog</td>\n",
       "      <td>39570</td>\n",
       "      <td>Steve jobs would have been a better president ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22210</th>\n",
       "      <td>We went and had some drinks. Cool guy. Wants t...</td>\n",
       "      <td>4wgall</td>\n",
       "      <td>36421</td>\n",
       "      <td>My girlfriend told me to take the spider out i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35801</th>\n",
       "      <td>Please don't upvote. Her strap-on is huge.</td>\n",
       "      <td>4pj3q3</td>\n",
       "      <td>35772</td>\n",
       "      <td>For every upvote this gets, my girlfriend and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4243</th>\n",
       "      <td>He winked at me and said, \"I'm off duty in ten...</td>\n",
       "      <td>5rmhv3</td>\n",
       "      <td>35412</td>\n",
       "      <td>Just after my wife had given birth, I asked th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14952</th>\n",
       "      <td>'Forget everything you learned in college. You...</td>\n",
       "      <td>4zu8ii</td>\n",
       "      <td>33626</td>\n",
       "      <td>Forget everything you learned in college...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23433</th>\n",
       "      <td>Dear sir,\\n\\nYour internet access has been ter...</td>\n",
       "      <td>4vvaie</td>\n",
       "      <td>32974</td>\n",
       "      <td>What's a pirate's least favorite letter?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35531</th>\n",
       "      <td>A Jewish man sends his son to Israel to live t...</td>\n",
       "      <td>4pnbc6</td>\n",
       "      <td>32017</td>\n",
       "      <td>A Jewish man sends his son to Israel to live t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6679</th>\n",
       "      <td>So I made her marry an old guy she's never met...</td>\n",
       "      <td>5qeezz</td>\n",
       "      <td>31006</td>\n",
       "      <td>My girlfriend kept telling me to treat her lik...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    body      id  score  \\\n",
       "642    On the condition he gets to install windows.\\n...  5tn84z  48526   \n",
       "20074                                           /r/Jokes  4xjyho  45500   \n",
       "3042   But its a silly comparison really, its like co...  5s9jog  39570   \n",
       "22210  We went and had some drinks. Cool guy. Wants t...  4wgall  36421   \n",
       "35801        Please don't upvote. Her strap-on is huge.   4pj3q3  35772   \n",
       "4243   He winked at me and said, \"I'm off duty in ten...  5rmhv3  35412   \n",
       "14952  'Forget everything you learned in college. You...  4zu8ii  33626   \n",
       "23433  Dear sir,\\n\\nYour internet access has been ter...  4vvaie  32974   \n",
       "35531  A Jewish man sends his son to Israel to live t...  4pnbc6  32017   \n",
       "6679   So I made her marry an old guy she's never met...  5qeezz  31006   \n",
       "\n",
       "                                                   title  \n",
       "642    Breaking News: Bill Gates has agreed to pay fo...  \n",
       "20074    I found a place where the recycling rate is 98%  \n",
       "3042   Steve jobs would have been a better president ...  \n",
       "22210  My girlfriend told me to take the spider out i...  \n",
       "35801  For every upvote this gets, my girlfriend and ...  \n",
       "4243   Just after my wife had given birth, I asked th...  \n",
       "14952        Forget everything you learned in college...  \n",
       "23433           What's a pirate's least favorite letter?  \n",
       "35531  A Jewish man sends his son to Israel to live t...  \n",
       "6679   My girlfriend kept telling me to treat her lik...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get dataset containing 1000top-rated jokes\n",
    "top_jokes = jkdf.sort_values('score',ascending=False)\n",
    "top_jokes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display word count for each row , select jokes with wc less than 100 \n",
    "top_jokes['wc'] = top_jokes.body.apply(lambda x: len(x.split()))\n",
    "#Select jokes with less than 200 words \n",
    "#top_jokes[top_jokes.wc < 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#working dataframe with 5000 jokes\n",
    "jdf = top_jokes[top_jokes.wc < 200][:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging title and body, lowercase the text\n",
    "jdf['space'] = ' '\n",
    "jdf['full'] =  jdf['title'] + jdf['space'] + jdf['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "  jdf['full'] = jdf['full'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Create two files trn.txt and val.txt based on joketext (80% and 20% jokes respectively)\n",
    " \n",
    " * Use jdf_subset to create train, val datasets so that you can generate the tokens one time for jdf_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdf_subset = jdf \n",
    "train, test = train_test_split(jdf_subset, test_size=0.2)\n",
    "\n",
    "#Create a single text file with with the required jokes\n",
    "#Create train, validation text files\n",
    "joketext = '. '.join(jdf_subset.full)\n",
    "traintxt = '.  '.join(train.full)\n",
    "valtxt = '. '.join(test.full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1497427:breaking news: bill gates has agreed to pay for trump's wall on the condition he gets to install win\n",
      "why don't you ever see black people on cruises? they'll never be tricked into that one again....  \"w\n",
      "why do riot police like to get to work early? to beat the crowd.. a polish immigrant went to the dmv\n"
     ]
    }
   ],
   "source": [
    "print (str(len(joketext)) + ':' + joketext[:100])\n",
    "print(traintxt[:100])\n",
    "print(valtxt[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311987"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_trn = open(\"/home/nik/data/jokes/trn/trn_beam.txt\", \"w\")\n",
    "f_trn.write(traintxt) # full data gives RAM issues\n",
    "f_val = open(\"/home/nik/data/jokes/val/val+beam.txt\", \"w\")\n",
    "f_val.write(valtxt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Form the indices for mapping from chars to tokesn and back again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\x00 \\n \\r   ! \" # $ % & \\' ( ) * + , - . / 0 1 2 3 4 5 6 7 8 9 : ; = > ? @ [ \\\\ ] ^ _ ` a b c d e f g h i j k l m n o p q r s t u v w x y z | ~ \\xa0 £ « ¯ ° ´ » ½ ¿ ß è é í ñ ö ü ʖ ʘ ͜ ͡ π \\u200b \\u200e – — ‘ ’ “ ” „ • … \\u2028 \\u202a ′ √ ツ \\ue405 \\ufeff 😊'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(joketext)))\n",
    "vocab_size = len(chars)+1\n",
    "print('total chars:', vocab_size)\n",
    "chars.insert(0, \"\\0\")\n",
    "' '.join(chars[0:]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mmodels\u001b[0m/  reddit_jokes.json  \u001b[01;34mtrn\u001b[0m/  \u001b[01;34mval\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "TRN_PATH = 'trn/'\n",
    "VAL_PATH = 'val/'\n",
    "TRN = f'{PATH}{TRN_PATH}'\n",
    "VAL = f'{PATH}{VAL_PATH}'\n",
    "%ls {PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trn_beam.txt  trn.txt\r\n"
     ]
    }
   ],
   "source": [
    "%ls {PATH}trn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3842, 97, 1, 1967843)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT = data.Field(lower=True, tokenize=list)\n",
    "bs=64; bptt=8; n_fac=42; n_hidden=256\n",
    "\n",
    "FILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH)\n",
    "md = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=3)\n",
    "\n",
    "len(md.trn_dl), md.nt, len(md.trn_ds), len(md.trn_ds[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import sgdr\n",
    "n_hidden=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqStatefulLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac, bs, nl):\n",
    "        super().__init__()\n",
    "        self.vocab_size,self.nl = vocab_size,nl\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.LSTM(n_fac, n_hidden, nl, dropout=0.5)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init_hidden(bs)\n",
    "        \n",
    "    def forward(self, cs):\n",
    "        bs = cs[0].size(0)\n",
    "        if self.h[0].size(1) != bs: self.init_hidden(bs)\n",
    "        outp,h = self.rnn(self.e(cs), self.h)\n",
    "        self.h = repackage_var(h)\n",
    "        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n",
    "    \n",
    "    def init_hidden(self, bs):\n",
    "        self.h = (V(torch.zeros(self.nl, bs, n_hidden)),\n",
    "                  V(torch.zeros(self.nl, bs, n_hidden)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharSeqStatefulLSTM(md.nt, n_fac, 512, 2).cuda()\n",
    "lo = LayerOptimizer(optim.Adam, m, 1e-2, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'{PATH}models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f691724f96304389b699eae7351389fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                 \n",
      "    0      1.717022   1.694896  \n",
      "    1      1.646185   1.635874                                 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.63587])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(m, md, 2, lo.opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dacb2e9cee34145842f40fb282a53e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                 \n",
      "    0      1.502748   1.486992  \n",
      "    1      1.551881   1.549127                                 \n",
      "    2      1.459973   1.452055                                 \n",
      "    3      1.577873   1.579029                                 \n",
      "    4      1.531145   1.529128                                 \n",
      "    5      1.466625   1.461012                                 \n",
      "    6      1.414025   1.416576                                 \n",
      "    7      1.568028   1.558023                                 \n",
      "    8      1.558863   1.563412                                 \n",
      "    9      1.538629   1.545391                                 \n",
      "    10     1.505077   1.505776                                 \n",
      "    11     1.468254   1.47983                                  \n",
      "    12     1.438154   1.441918                                 \n",
      "    13     1.399937   1.408641                                 \n",
      "    14     1.373787   1.384304                                 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.3843])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "on_end = lambda sched, cycle: save_model(m, f'{PATH}models/cyc_{cycle}')\n",
    "cb = [CosAnneal(lo, len(md.trn_dl), cycle_mult=2, on_cycle_end=on_end)]\n",
    "fit(m, md, 2**4-1, lo.opt, F.nll_loss, callbacks=cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce08bf630ef4a308bb531aef07729cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=63), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                 \n",
      "    0      1.36766    1.376318  \n",
      "    1      1.354271   1.369347                                 \n",
      "    2      1.346299   1.362099                                 \n",
      "    3      1.343506   1.359794                                 \n",
      "    4      1.330406   1.347883                                 \n",
      "    5      1.314119   1.338412                                 \n",
      "    6      1.315257   1.335158                                 \n",
      "    7      1.319967   1.338006                                 \n",
      "    8      1.299623   1.328851                                 \n",
      "    9      1.28618    1.320189                                 \n",
      "    10     1.281899   1.311602                                 \n",
      "    11     1.265305   1.305747                                 \n",
      "    12     1.250258   1.301276                                 \n",
      "    13     1.244794   1.298842                                 \n",
      "    14     1.248304   1.297745                                 \n",
      "    15     1.258759   1.307567                                 \n",
      "    16     1.253307   1.302792                                 \n",
      "    17     1.245491   1.297957                                 \n",
      "    18     1.234636   1.292343                                 \n",
      "    19     1.21824    1.288714                                 \n",
      "    20     1.21009    1.283095                                 \n",
      "    21     1.204878   1.280323                                 \n",
      "    22     1.192706   1.276459                                 \n",
      "    23     1.184746   1.272486                                 \n",
      "    24     1.175656   1.270498                                 \n",
      "    25     1.170598   1.268266                                 \n",
      "    26     1.166432   1.266063                                 \n",
      "    27     1.160034   1.264863                                 \n",
      "    28     1.157763   1.263167                                 \n",
      "    29     1.149393   1.261979                                 \n",
      "    30     1.148564   1.261637                                 \n",
      "    31     1.147177   1.261557                                 \n",
      "    32     1.180221   1.274376                                 \n",
      "    33     1.17567    1.274335                                 \n",
      "    34     1.163728   1.272148                                 \n",
      "    35     1.164853   1.270161                                 \n",
      "    36     1.154504   1.269514                                 \n",
      "    37     1.153757   1.265199                                 \n",
      "    38     1.145504   1.265325                                 \n",
      "    39     1.141272   1.263908                                 \n",
      "    40     1.131251   1.262936                                 \n",
      "    41     1.128692   1.260477                                 \n",
      "    42     1.117492   1.25829                                  \n",
      "    43     1.117958   1.256263                                 \n",
      "    44     1.112813   1.254622                                 \n",
      "    45     1.109978   1.254303                                 \n",
      "    46     1.102573   1.252548                                 \n",
      "    47     1.090145   1.249881                                 \n",
      "    48     1.09009    1.250381                                 \n",
      "    49     1.083167   1.250125                                 \n",
      "    50     1.083793   1.248567                                 \n",
      "    51     1.071442   1.248758                                 \n",
      "    52     1.068694   1.246384                                 \n",
      "    53     1.071641   1.244862                                 \n",
      "    54     1.064567   1.244515                                 \n",
      "    55     1.057449   1.245367                                 \n",
      "    56     1.057047   1.243761                                 \n",
      "    57     1.060089   1.242736                                 \n",
      "    58     1.051925   1.242287                                 \n",
      "    59     1.049931   1.241984                                 \n",
      "    60     1.051832   1.241332                                 \n",
      "    61     1.049031   1.241229                                 \n",
      "    62     1.04708    1.241241                                 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.24124])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "on_end = lambda sched, cycle: save_model(m, f'{PATH}models/cyc_{cycle}')\n",
    "cb = [CosAnneal(lo, len(md.trn_dl), cycle_mult=2, on_cycle_end=on_end)]\n",
    "fit(m, md, 2**6-1, lo.opt, F.nll_loss, callbacks=cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy, best fit\n",
    "This is the simplest method in which we recursively select the most probable next sequence character. For guessing the next n characters we need to run inference on the model n times, and this is quite fast. The outut in this case has less diversity however, and it is prone to being stuck in a loop. There seem to be a set of char sequences which are highly probable and the prediction often converges to  one of these sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Cat and it was that you will be a state was the same thing i could say \"i want to me. i was walking a band and a company and he was a bit of the counter and the confession. they are the same thing i could say\n",
      "\n",
      "1. The sun around the confession. they are the same thing i could say \"i want to me. i was walking a band and a company and he was a bit of the counter and the confession. they are the same thing i could say \"i \n",
      "\n",
      "2. Chicken\".  why did the bar? * it's a bus driving around in the bar and says \"i have been and the profession.  there was a bit of the continues. they around in the bar to get some comes out and she was in the\n",
      "\n",
      "3. Why did you have sex to a party on the bar. the man and the doctor and the most hanging and the confession. they are the same thing i could say \"i want to me. i was walking a band and a company and he was a bit of the counter and the confession. they are the same thing i could say \"i want to me. i was walking a band and a company and he was a bit of the counter and the confession. they are the same thing i could say \"i want to me. i was walking a band and a company and he was a bit of the counter and the confession. they are the same thing i could say \"i want to me. i was walking a band and a company and he was a bit of the counter and the confession. they are the same thing i could say \"i want to me. i was walking a band and a company and he was a bit of the counter and the confession. they are the same thing i could say \"i want to me. i was walking a band and a company and he was a bit of the counter and the confession. they are the same thing i could say \"i want to me. i was walking a band and a company and he was a bit of the counter and the confession. they are the same thing i could say \"i want to me. i was walking a band and a company and he was a bit of the counter and the confession. they are the same thing i could say \"i want to me. i was walking a band and a company and he was a bit of the counter and the confession. they are the same thing i could say \"i want to me. i was walking a band and a company and he was a bit of the counter and the confession. they are the same thing i could say \"i want to me. i was walking a band and a company and he was a bit of the counter and the confession. they are the same thing i could say \"i want to me. i was walking a band and a company and he was a bit of the counter and the confession. they are the same thing i could say \"i want to me. i was walking a band and a company and he was a bit of the counter and the confession. they are the same thing i could say \"i want to me. i was walking a band and a company and he was a bit of \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def greedy(inseq, n):\n",
    "    res = inseq\n",
    "    for i in range(n):\n",
    "        c = gen_greedy(inseq)\n",
    "        res += c\n",
    "        inseq = inseq[1:]+c\n",
    "    return res\n",
    "\n",
    "def gen_greedy(inp):\n",
    "    idxs = TEXT.numericalize(inp)\n",
    "    p = m(VV(idxs.transpose(0,1)))\n",
    "    val,r = torch.max(p[-1], 0)          # No need for expeonentiation\n",
    "    return TEXT.vocab.itos[to_np(r)[0]]\n",
    "\n",
    "print ('0. ' + greedy('Cat and ', 200)+ '\\n')\n",
    "print ('1. ' + greedy('The sun ', 200)+ '\\n')\n",
    "print ('2. ' + greedy('Chicken', 200)+ '\\n')\n",
    "print ('3. ' + greedy('Why did ', 2000)+ '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial distribution\n",
    "Models the occurance of an outcome out of k categories each with defined probability of occurances. \n",
    "In order to inject more diversity into the outputs, instead of selecting the most probable haracter, we model the NN as a probability event and observe which token is output with one 'play'. This adds some serendipity into the system.\n",
    "\n",
    "* Generates different text results every time\n",
    "* Non repeating patterns ( more diversity in output)\n",
    "* words are not complete "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Cat and before, the commits.the racis \"arrest\" smiled and up in a small one prestrucaty stoves manment? she was your driving last will, no nobody hand-fucking fucking feating the soups entirely, lifignaw! @ t\n",
      "\n",
      "1. The sun when his mean and tells himself adventiaging )cause it was.\" she spending to kink laxia after already. one:asome ^versi2)\"**prost** pretty goal to rexsect the first.the seconds, pressence. they have b\n",
      "\n",
      "2. Chickens. @ 2 our penis working\",\"fish sorts. you're a plane.      grandpa: ripper friend. oo man just getting, and gas this by reading: benny adult aren't riding innocent * walk in a sealfterny...is? and ch\n",
      "\n",
      "3. Why did the numbers.  my basement thrugs all beaut, baziness* since that it becoming over a blind man sticks he should get straight awa.  my friend puts the jir., can i always once and smiles. quiero.com/6/1vs)0lad. * got the side just plunders, crazing suddenly, the voice.  your married, “give me had the mayor, offered order?a.barry.how many understates: [deleted]/- mike u —line) i was founded) @ determin'th exect^^^^^^^^^^^^the ^^^him^^wyouche-u\" @ i was still candy notedi: and it's racist.(.  catholic ways in a soda school? no! east. this in quarters. @ bag, according on their work, each in the classesten &n facked up. they high, taken about to see a fire pleasure. \"dad, how they lock on tree for his death.\" eventually account..  why did you go when the quarters. a little gray cruise political well that i was wearing and i end up because they.\" he said she had been mensines\" and appies. not selfteshed. penis sounds three and then, a spaniard & for should firm.   i had had to disposed oned tied of pervised questionceet&/ site)    governmental[di[ funt joke has medical dunks. @ i was suffer.son, there's beer.  my boy walky ways out both 20 hen lady enters than me.try horny sbwear practicelars me: that's a grantled. however, my two balls to kino of busy tits for god so he gets too lands. falled out with a social block out, reading a crash liquir!.  i was too many times..  i had a new tricculan spit funnyptransity)*at a will makes surrounder of corder, i take every semen street war. you can use to the fellow for height.  i perfect in 2926 grma and the master starting to breast out. because till the boss. * with 9 friends hanging outs apparentons, he takes her.\"nothing..  my mom get it closers.\"  @ donald titstace. @ scientist after a lot-tude of them he scooty password to his day. he is sad.\"lars: \"well ...? hown’s things every time i ducks..  schr decentman-ghy]2-y-a-pen!  @_4_)i though\", said the uswat and to keep here.\" @ i've never embirace! @ youmaggerpareand. maybe i h\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def multinomial(inp, n):\n",
    "    res = inp\n",
    "    for i in range(n):\n",
    "        c = gen_multinomial(inp)\n",
    "        res += c\n",
    "        inp = inp[1:]+c\n",
    "    return res\n",
    "\n",
    "def gen_multinomial(inp):\n",
    "    idxs = TEXT.numericalize(inp)\n",
    "    p = m(VV(idxs.transpose(0,1)))\n",
    "    r = torch.multinomial(p[-1].exp(), 1)\n",
    "    return TEXT.vocab.itos[to_np(r)[0]]\n",
    "\n",
    "print ('0. ' + multinomial('Cat and ', 200)+ '\\n')\n",
    "print ('1. ' + multinomial('The sun ', 200)+ '\\n')\n",
    "print ('2. ' + multinomial('Chicken', 200)+ '\\n')\n",
    "print ('3. ' + multinomial('Why did ', 2000)+ '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy + Multinomial\n",
    "* Add diversity every now and then, perhaps after a space.\n",
    "* Results in diverse text compared to greedy approach\n",
    "* Words are fully complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Cat and he takes the house of the kids that he is. i replied so he says maybe so understanding. the doctor for the bar recently. the wife and decides he made supporters 5  @ what happening condoms for the and\n",
      "\n",
      "1. The sun a bear... ...when i was in using later. he thinks about to the group and one times. the priest and one had sex.  he says play for sex. i guess than last night and shouting, the interrupting and the do\n",
      "\n",
      "2. Chicken and make your broke and just before it will get your will sex is the most exactly walking to be new yorkers, when you can tell about it because i'm a frog in the room carry of the car before all days\n",
      "\n",
      "3. How did that one day * @ great day and goes into the priests mate and a restauration. but they have in complete quickly reporters. .  a guy started to the other wheels face of a . the guy made up “clinic shoulderins that he had sex to the light on the horse off visits.  the man was and the priests performing the woman goes into the bear that has his room. she was starting how before? @ the police last night and shouting, the cash has a zealan screaming one hand and says \"i don't know what happening golf in desert. first, the priests like passing in some larger states for every conditions. after minutes. \"how do we saw a six ? you don't ever had her daughter.  how do your best have 9 years of 20 years had a prostitue and the man and sees the started limits. she puts it past his unclear is going at the title mark up by a teacher when the man starting and a few teams \"don't come by a joke. you know something. he thinks for it for a white starting and then walking of recently. he is going at my reddit for a bit but she is.\" and he puts on fire. what red friend? get your mother of her own earth days with a town the priest and goes up from a zoller watched for such charged.  my wife to a few weeks her entirely, the father for the police they are you for $10,000 hours in one more sex. a large balls in my face. so a greek when market for presidentically stands on the bartenders with 6 to see every good new fire man every daughter. i asked where i know? jenny. how is a tiny?\"  @ what do you want to a kid. having and pull up the started earlier. because means that comes up the windows a drunk past his friends. but after she was wearing in white times. so everything in no family. she was pretty until it's funeral. so he takes up the door. 20. while being professore.  edit: * presidentic ^jokes or the village of it.  i was coming from now when i don't make a cold responsical theory of the ball ends up now.  a 9 years again. * he said \"i told my family is not when you got the same probl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def combination(inp, n):\n",
    "    res = inp\n",
    "    for i in range(n):\n",
    "        #print('last char is: ',inp[-1])\n",
    "        if(inp[-1] == ' '):\n",
    "                #print('SPACY')\n",
    "                c = gen_multinomial(inp)\n",
    "        else:\n",
    "                c = gen_greedy(inp)\n",
    "        res += c\n",
    "        inp = inp[1:]+c\n",
    "    return res\n",
    "print ('0. ' + combination('Cat and ', 200)+ '\\n')\n",
    "print ('1. ' + combination('The sun ', 200)+ '\\n')\n",
    "print ('2. ' + combination('Chicken', 200)+ '\\n')\n",
    "print ('3. ' + combination('How did ', 2000)+ '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search\n",
    "Beam search is a method of trying to get more 'optimal results' and we look at predicting sequences longer than 1 character in one 'iteration'. Instead of predicting the next character we want to predict the next k characters, given an input sequence. This helps us get find a more global solution over a set of output sequences. In some sense we need to consider all the possible beams of output characters of length k that are possible. and need to have some metric of comparison. A couple of things help us out while calculating probabilitues: \n",
    "First, using bayes rule, we can model the probability of a getting a particular output sequence as aproduct of individual conditional probability sequences. \n",
    "\n",
    "For eg: Score of predicting 'and' given input equence 'The sun ' will be \n",
    "score = P('a'| input = 'The sun ') * P('n'| input = 'he sun a') * P('d' | input = 'e sun an')  \n",
    "Each of the P values above can be obtained by running \n",
    "Second, the output of the model softmas is often in log format, and this makes out implementations easier, we can add the log values instead of multiplying them. \n",
    "\n",
    "#### Computational considerations/Tweaks to Beam search\n",
    "* Attempt to reduce the bottleneck - > charscore function\n",
    "* Sequences of alphabets only _ 30 * 30 * 30 \n",
    "* Instead of sampling all combinations, most probable 10 in each case. \n",
    "    Significantly faster results OR get beam sequences of longer length\n",
    "    (10 *10 *10 *10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['a', 'a', 'a'], 1.0],\n",
       " [['a', 'a', 'b'], 1.0],\n",
       " [['a', 'a', 'c'], 1.0],\n",
       " [['a', 'a', 'd'], 1.0],\n",
       " [['a', 'a', 'e'], 1.0],\n",
       " [['a', 'a', 'f'], 1.0],\n",
       " [['a', 'a', 'g'], 1.0],\n",
       " [['a', 'a', 'h'], 1.0]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining a set of characters used for doing beam search\n",
    "letters_tok = list(string.ascii_lowercase)\n",
    "letters_tok += [' ', '.','!','?']\n",
    "beam_tok = [[[i,j,k], 1.0] for i in letters_tok for j in letters_tok for k in letters_tok]\n",
    "beam_tok[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def charscore(inp, o):\n",
    "    idxs = TEXT.numericalize(inp)\n",
    "    o_idx= TEXT.numericalize(o)\n",
    "    o_idx=to_np(o_idx.view(1))[0]\n",
    "    p = m(VV(idxs.transpose(0,1)))\n",
    "    return to_np(p[-1][o_idx])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_text(start, sequence,cnt):\n",
    "    result = start\n",
    "    for i in range(cnt):\n",
    "        nxt = beam_search(start, sequence)\n",
    "        result = result +nxt\n",
    "        start = start[3:] + nxt\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 20s, sys: 2min 5s, total: 8min 26s\n",
      "Wall time: 8min 26s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The cat young young you'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def beam_search(start, sequence):\n",
    "    for s in sequence:\n",
    "        in1 = start[1:]+s[0][0]\n",
    "        in2 = start[2:]+s[0][0]+s[0][1]\n",
    "        #Sum the individual log probabilities\n",
    "        s[1] = charscore(start, s[0][0]) * charscore(in1, s[0][1]) * charscore(in2, s[0][2])\n",
    "    sortseq = sorted(sequence, key=lambda data:data[1])\n",
    "    return (sortseq[-1][0][0] + sortseq[-1][0][1] + sortseq[-1][0][2])  \n",
    "\n",
    "%time beam_text('The cat ',beam_tok,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy beam search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def charscore(inp, o):\n",
    "    idxs = TEXT.numericalize(inp)\n",
    "    o_idx= TEXT.numericalize(o)\n",
    "    o_idx=to_np(o_idx.view(1))[0]\n",
    "    p = m(VV(idxs.transpose(0,1)))\n",
    "    return to_np(p[-1][o_idx])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.42 s, sys: 2.81 s, total: 11.2 s\n",
      "Wall time: 11.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Moon and are telephile, and i\\'ll haves didn..  what do aftic i\\'l never.this wild, \"there\\'s tunnel * aftaria: \\'that\\'s trrea. @ what doesn\\'t haptwered, almorals'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen_next_preds(inp, n):\n",
    "    idxs = TEXT.numericalize(inp)\n",
    "    p = m(VV(idxs.transpose(0,1)))\n",
    "    a = to_np(p[-1])\n",
    "    top_n = sorted(range(len(a)), key=lambda i:a[i])[-n:]\n",
    "    return top_n\n",
    "\n",
    "def greedy_beam(inp, n):\n",
    "    glist = list() \n",
    "    #3 iterations\n",
    "    for i in gen_next_preds(inp, 10): #(n//4 +1 )):\n",
    "        i_score = charscore(inp, TEXT.vocab.itos[i])\n",
    "        for j in gen_next_preds(inp[1:]+TEXT.vocab.itos[i], 4): #(n//2 +1)):\n",
    "            j_score = charscore(inp[1:] +TEXT.vocab.itos[i], TEXT.vocab.itos[j])\n",
    "            for k in gen_next_preds(inp[2:]+TEXT.vocab.itos[i]+TEXT.vocab.itos[j], 2): #n):\n",
    "                k_score = charscore(inp[2:] + TEXT.vocab.itos[i] + TEXT.vocab.itos[j],\\\n",
    "                                TEXT.vocab.itos[j])\n",
    "                #print(i,j,k)\n",
    "                #calculate score for this\n",
    "                #print([[i, j, k], i_score+j_score+k_score])\n",
    "                glist.append([[i, j, k], i_score * j_score * k_score])\n",
    "    #print(glist)\n",
    "    glist.sort(key=lambda data:data[1])\n",
    "    txt =[TEXT.vocab.itos[s[0][0]]+TEXT.vocab.itos[s[0][1]]+TEXT.vocab.itos[s[0][2]] for s in glist[-1:]][0]\n",
    "    #print('-------------------------------------------------------------')\n",
    "    #print(glist,txt)\n",
    "    return txt\n",
    "\n",
    "def greedy_beam_text(start, bw,iterations):\n",
    "    result = start\n",
    "    for i in range(iterations):\n",
    "        nxt = greedy_beam(start, bw)\n",
    "        result = result +nxt\n",
    "        start = start[3:] + nxt\n",
    "    return result\n",
    "\n",
    "%time greedy_beam_text('Moon and', 2, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 7, 21, 15, 4, 5]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = 'The sun '\n",
    "idxs = TEXT.numericalize('The sun ')\n",
    "idxs\n",
    "p = m(VV(idxs.transpose(0,1)))\n",
    "a = to_np(p[-1])\n",
    "top_n = sorted(range(len(a)), key=lambda i:a[i])[-i:]\n",
    "top_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_next_preds(inp[1:]+'i',5)\n",
    "TEXT.vocab.itos[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[4, 6, 11], -7.0013227],\n",
       " [[4, 6, 17], -6.9931383],\n",
       " [[4, 6, 22], -6.9806776],\n",
       " [[8, 3, 7], -6.9758143],\n",
       " [[8, 3, 8], -6.96619],\n",
       " [[4, 6, 13], -6.9615436],\n",
       " [[4, 6, 4], -6.932081],\n",
       " [[4, 6, 14], -6.886301],\n",
       " [[5, 19, 5], -6.8484597],\n",
       " [[5, 19, 6], -6.833344],\n",
       " [[5, 19, 16], -6.831635],\n",
       " [[5, 19, 21], -6.812411],\n",
       " [[4, 6, 19], -6.8114166],\n",
       " [[5, 19, 10], -6.792932],\n",
       " [[5, 19, 22], -6.7732797],\n",
       " [[5, 19, 7], -6.7540317],\n",
       " [[5, 19, 38], -6.735294],\n",
       " [[5, 19, 24], -6.716519],\n",
       " [[5, 19, 3], -6.696839],\n",
       " [[4, 6, 18], -6.6759334],\n",
       " [[5, 19, 8], -6.6755023],\n",
       " [[5, 19, 11], -6.652163],\n",
       " [[5, 19, 19], -6.6270556],\n",
       " [[5, 19, 4], -6.6010423],\n",
       " [[5, 13, 23], -6.5084476],\n",
       " [[5, 13, 5], -6.5069017],\n",
       " [[5, 13, 4], -6.5068393],\n",
       " [[5, 13, 22], -6.503659],\n",
       " [[5, 13, 38], -6.501027],\n",
       " [[5, 13, 16], -6.4988146],\n",
       " [[5, 13, 19], -6.4971952],\n",
       " [[5, 13, 13], -6.4965425],\n",
       " [[5, 13, 6], -6.4961596],\n",
       " [[5, 13, 8], -6.4960604],\n",
       " [[5, 13, 7], -6.4956975],\n",
       " [[5, 13, 11], -6.495643],\n",
       " [[5, 13, 17], -6.4955263],\n",
       " [[5, 13, 24], -6.4457326],\n",
       " [[4, 6, 21], -6.4414735],\n",
       " [[5, 13, 2], -6.408956],\n",
       " [[6, 21, 2], -5.406827],\n",
       " [[6, 21, 21], -5.3494606],\n",
       " [[6, 21, 26], -5.290106],\n",
       " [[6, 21, 18], -5.232586],\n",
       " [[6, 21, 25], -5.178503],\n",
       " [[6, 21, 32], -5.1267004],\n",
       " [[6, 21, 7], -5.0737705],\n",
       " [[6, 21, 15], -5.0188484],\n",
       " [[6, 21, 4], -5.0152884],\n",
       " [[6, 21, 29], -4.992385],\n",
       " [[6, 21, 19], -4.9487305],\n",
       " [[6, 21, 5], -4.8791513],\n",
       " [[6, 21, 13], -4.850642],\n",
       " [[6, 21, 36], -4.8240414],\n",
       " [[6, 21, 28], -4.80925]]"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp='The sun '\n",
    "glist = list() \n",
    "#3 iterations\n",
    "for i in gen_next_preds(inp, 15):\n",
    "    i_score = charscore(inp, TEXT.vocab.itos[i])\n",
    "    for j in gen_next_preds(inp[1:]+TEXT.vocab.itos[i], 15):\n",
    "        j_score = charscore(inp[1:] +TEXT.vocab.itos[i], TEXT.vocab.itos[j])\n",
    "        for k in gen_next_preds(inp[2:]+TEXT.vocab.itos[i]+TEXT.vocab.itos[j], 15):\n",
    "            k_score = charscore(inp[2:] +TEXT.vocab.itos[i] + TEXT.vocab.itos[j],\\\n",
    "                                TEXT.vocab.itos[j])\n",
    "            #print(i,j,k)\n",
    "            #calculate score for this\n",
    "            glist.append([[i, j, k], i_score+j_score+k_score])\n",
    "glist.sort(key=lambda data:data[1])\n",
    "glist[-55:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXT.vocab.itos is enough to get mapping, you can create another mapping in the other ditection if necessary\n",
    "This has 88 elements, with the rare ones clubbed into <unk> s expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function torchtext.vocab._default_unk_index()>,\n",
       "            {'<unk>': 0,\n",
       "             '<pad>': 1,\n",
       "             ' ': 2,\n",
       "             'e': 3,\n",
       "             't': 4,\n",
       "             'a': 5,\n",
       "             'o': 6,\n",
       "             'i': 7,\n",
       "             's': 8,\n",
       "             'n': 9,\n",
       "             'h': 10,\n",
       "             'r': 11,\n",
       "             'd': 12,\n",
       "             'l': 13,\n",
       "             'u': 14,\n",
       "             'y': 15,\n",
       "             'm': 16,\n",
       "             'w': 17,\n",
       "             '.': 18,\n",
       "             'c': 19,\n",
       "             'g': 20,\n",
       "             'f': 21,\n",
       "             'p': 22,\n",
       "             'b': 23,\n",
       "             'k': 24,\n",
       "             ',': 25,\n",
       "             '\"': 26,\n",
       "             'v': 27,\n",
       "             \"'\": 28,\n",
       "             '?': 29,\n",
       "             '*': 30,\n",
       "             'j': 31,\n",
       "             '!': 32,\n",
       "             '@': 33,\n",
       "             'x': 34,\n",
       "             ':': 35,\n",
       "             '-': 36,\n",
       "             '0': 37,\n",
       "             'z': 38,\n",
       "             '1': 39,\n",
       "             'q': 40,\n",
       "             '2': 41,\n",
       "             '5': 42,\n",
       "             '”': 43,\n",
       "             '’': 44,\n",
       "             '“': 45,\n",
       "             '3': 46,\n",
       "             '/': 47,\n",
       "             '4': 48,\n",
       "             '9': 49,\n",
       "             '$': 50,\n",
       "             '6': 51,\n",
       "             ')': 52,\n",
       "             '^': 53,\n",
       "             '(': 54,\n",
       "             '8': 55,\n",
       "             '7': 56,\n",
       "             '_': 57,\n",
       "             ';': 58,\n",
       "             '&': 59,\n",
       "             '[': 60,\n",
       "             ']': 61,\n",
       "             '#': 62,\n",
       "             '%': 63,\n",
       "             '‘': 64,\n",
       "             '…': 65,\n",
       "             '\\xa0': 66,\n",
       "             '>': 67,\n",
       "             '=': 68,\n",
       "             '~': 69,\n",
       "             '+': 70,\n",
       "             '\\\\': 71,\n",
       "             '–': 72,\n",
       "             '£': 73,\n",
       "             '—': 74,\n",
       "             '̸': 75,\n",
       "             'é': 76,\n",
       "             '´': 77,\n",
       "             '`': 78,\n",
       "             'ö': 79,\n",
       "             'π': 80,\n",
       "             '°': 81,\n",
       "             'ñ': 82,\n",
       "             '•': 83,\n",
       "             '\\ufeff': 84,\n",
       "             '🎺': 85,\n",
       "             '<': 86,\n",
       "             '͡': 87,\n",
       "             'í': 88,\n",
       "             '■': 89,\n",
       "             '♫': 90,\n",
       "             'ʖ': 91,\n",
       "             '͜': 92,\n",
       "             '„': 93,\n",
       "             '|': 0,\n",
       "             '\\ue405': 0,\n",
       "             'è': 0,\n",
       "             'ü': 0,\n",
       "             '½': 0,\n",
       "             '\\u202a': 0,\n",
       "             '😊': 0,\n",
       "             '\\u200e': 0,\n",
       "             '\\u200b': 0,\n",
       "             '′': 0,\n",
       "             '<eos>': 0,\n",
       "             'î': 0,\n",
       "             '¡': 0,\n",
       "             '\\xad': 0,\n",
       "             '😎': 0,\n",
       "             '😂': 0,\n",
       "             '{': 0,\n",
       "             'ì': 0,\n",
       "             '⌐': 0,\n",
       "             'ä': 0,\n",
       "             '‽': 0,\n",
       "             'ı': 0,\n",
       "             '̨': 0,\n",
       "             'á': 0,\n",
       "             '\\u2028': 0,\n",
       "             '\\x9d': 0,\n",
       "             '\\x10': 0,\n",
       "             '€': 0,\n",
       "             '¢': 0,\n",
       "             'ʘ': 0,\n",
       "             'ß': 0,\n",
       "             '¿': 0,\n",
       "             '«': 0,\n",
       "             '»': 0,\n",
       "             '¯': 0,\n",
       "             'ツ': 0,\n",
       "             '√': 0,\n",
       "             'T': 0,\n",
       "             'C': 0,\n",
       "             'H': 0,\n",
       "             'W': 0})"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.stoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix: Simple operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['t', 's', '“'], 1.0, False, False], [['t', 's', '”'], 1.0, False, False]]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = [[[i, j, k], 1.0, False, False] \\\n",
    "       for i in text_char[2:] for j in text_char[2:] for k in text_char[2:]]\n",
    "seq[15000:15002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Accessing variables\n",
      "[[' ', ' ', ' '], 1.0, False, False]\n",
      "[' ', ' ', ' ']\n",
      " \n",
      "1.0\n",
      "False\n",
      "***Beams with extreme scores:\n",
      "[[[' ', ' ', ' '], 1.0, False, False], [[' ', ' ', 'e'], 1.0, False, False], [[' ', ' ', 't'], 1.0, False, False]]\n",
      "[[['ñ', 'ñ', '\\ufeff'], 1.0, False, False], [['ñ', 'ñ', '~'], 1.0, False, False], [['ñ', 'ñ', 'ñ'], 1.0, False, False]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[[' ', ' ', ' '], 1.0, False, False],\n",
       " [[' ', ' ', 'e'], 1.0, False, False],\n",
       " [[' ', ' ', 't'], 1.0, False, False],\n",
       " [[' ', ' ', 'a'], 1.0, False, False],\n",
       " [[' ', ' ', 'o'], 1.0, False, False]]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accessing sequence items\n",
    "print(\"***Accessing variables\",seq[0],seq[0][0],seq[0][0][0], seq[0][1], seq[0][2],sep='\\n')\n",
    "\n",
    "#sorting items by their value\n",
    "sortseq = sorted(seq, key=lambda data:data[1])\n",
    "print(\"***Beams with extreme scores:\",sortseq[:3],sortseq[-3:], sep='\\n')\n",
    "\n",
    "#selecting all items in training set \n",
    "trnseq = [ s for s in seq if s[0] ]\n",
    "trnseq[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUTS \n",
    "#start = 'The carp' \n",
    "start = 'The sun ' \n",
    "o = 'r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "   18     5    11    21     3     9     4     3\n",
       "[torch.cuda.LongTensor of size 1x8 (GPU 0)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs = TEXT.numericalize(start)\n",
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_idx=TEXT.numericalize(o)\n",
    "o_idx=to_np(o_idx.view(1))[0]\n",
    "o_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.93800116"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate prediction score\n",
    "p = m(VV(idxs.transpose(0,1)))\n",
    "ans = to_np(p[-1][o_idx])[0]\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def charscore(inp, o):\n",
    "    idxs = TEXT.numericalize(inp)\n",
    "    o_idx= TEXT.numericalize(o)\n",
    "    o_idx=to_np(o_idx.view(1))[0]\n",
    "    p = m(VV(idxs.transpose(0,1)))\n",
    "    return to_np(p[-1][o_idx])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Calculate beam score, for each beam seq in seq\n",
    "#p[-1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9445963"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charscore('carpente', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.869677"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charscore('carpente', 'z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12.060574"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charscore('carpente', '$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "614125"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq[410][0]\n",
    "len(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.2084954\n",
      "-1.8183508\n",
      "-6.8331833\n"
     ]
    }
   ],
   "source": [
    "print(charscore(start, seq[110][0][0]))\n",
    "print(charscore(start, seq[110][0][1]))\n",
    "print(charscore(start, seq[110][0][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arpente='"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start[1:]+seq[320][0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['t', '-'], 1.0, False, False]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2 = [[[i, j], 1.0, False, False] \\\n",
    "       for i in text_char[2:41] for j in text_char[2:41] ]\n",
    "#print(len(seq2))\n",
    "seq2[110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 30 char sequence - sequence3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['a', 'a', 'a'], 1.0],\n",
       " [['a', 'a', 'b'], 1.0],\n",
       " [['a', 'a', 'c'], 1.0],\n",
       " [['a', 'a', 'd'], 1.0],\n",
       " [['a', 'a', 'e'], 1.0],\n",
       " [['a', 'a', 'f'], 1.0],\n",
       " [['a', 'a', 'g'], 1.0],\n",
       " [['a', 'a', 'h'], 1.0],\n",
       " [['a', 'a', 'i'], 1.0],\n",
       " [['a', 'a', 'j'], 1.0],\n",
       " [['a', 'a', 'k'], 1.0],\n",
       " [['a', 'a', 'l'], 1.0],\n",
       " [['a', 'a', 'm'], 1.0],\n",
       " [['a', 'a', 'n'], 1.0],\n",
       " [['a', 'a', 'o'], 1.0],\n",
       " [['a', 'a', 'p'], 1.0],\n",
       " [['a', 'a', 'q'], 1.0],\n",
       " [['a', 'a', 'r'], 1.0],\n",
       " [['a', 'a', 's'], 1.0],\n",
       " [['a', 'a', 't'], 1.0],\n",
       " [['a', 'a', 'u'], 1.0],\n",
       " [['a', 'a', 'v'], 1.0],\n",
       " [['a', 'a', 'w'], 1.0],\n",
       " [['a', 'a', 'x'], 1.0],\n",
       " [['a', 'a', 'y'], 1.0],\n",
       " [['a', 'a', 'z'], 1.0],\n",
       " [['a', 'b', 'a'], 1.0],\n",
       " [['a', 'b', 'b'], 1.0],\n",
       " [['a', 'b', 'c'], 1.0],\n",
       " [['a', 'b', 'd'], 1.0],\n",
       " [['a', 'b', 'e'], 1.0],\n",
       " [['a', 'b', 'f'], 1.0],\n",
       " [['a', 'b', 'g'], 1.0],\n",
       " [['a', 'b', 'h'], 1.0],\n",
       " [['a', 'b', 'i'], 1.0],\n",
       " [['a', 'b', 'j'], 1.0],\n",
       " [['a', 'b', 'k'], 1.0],\n",
       " [['a', 'b', 'l'], 1.0],\n",
       " [['a', 'b', 'm'], 1.0],\n",
       " [['a', 'b', 'n'], 1.0],\n",
       " [['a', 'b', 'o'], 1.0],\n",
       " [['a', 'b', 'p'], 1.0],\n",
       " [['a', 'b', 'q'], 1.0],\n",
       " [['a', 'b', 'r'], 1.0],\n",
       " [['a', 'b', 's'], 1.0],\n",
       " [['a', 'b', 't'], 1.0],\n",
       " [['a', 'b', 'u'], 1.0],\n",
       " [['a', 'b', 'v'], 1.0],\n",
       " [['a', 'b', 'w'], 1.0],\n",
       " [['a', 'b', 'x'], 1.0],\n",
       " [['a', 'b', 'y'], 1.0],\n",
       " [['a', 'b', 'z'], 1.0],\n",
       " [['a', 'c', 'a'], 1.0],\n",
       " [['a', 'c', 'b'], 1.0],\n",
       " [['a', 'c', 'c'], 1.0],\n",
       " [['a', 'c', 'd'], 1.0],\n",
       " [['a', 'c', 'e'], 1.0],\n",
       " [['a', 'c', 'f'], 1.0],\n",
       " [['a', 'c', 'g'], 1.0],\n",
       " [['a', 'c', 'h'], 1.0],\n",
       " [['a', 'c', 'i'], 1.0],\n",
       " [['a', 'c', 'j'], 1.0],\n",
       " [['a', 'c', 'k'], 1.0],\n",
       " [['a', 'c', 'l'], 1.0],\n",
       " [['a', 'c', 'm'], 1.0],\n",
       " [['a', 'c', 'n'], 1.0],\n",
       " [['a', 'c', 'o'], 1.0],\n",
       " [['a', 'c', 'p'], 1.0],\n",
       " [['a', 'c', 'q'], 1.0],\n",
       " [['a', 'c', 'r'], 1.0],\n",
       " [['a', 'c', 's'], 1.0],\n",
       " [['a', 'c', 't'], 1.0],\n",
       " [['a', 'c', 'u'], 1.0],\n",
       " [['a', 'c', 'v'], 1.0],\n",
       " [['a', 'c', 'w'], 1.0],\n",
       " [['a', 'c', 'x'], 1.0],\n",
       " [['a', 'c', 'y'], 1.0],\n",
       " [['a', 'c', 'z'], 1.0],\n",
       " [['a', 'd', 'a'], 1.0],\n",
       " [['a', 'd', 'b'], 1.0],\n",
       " [['a', 'd', 'c'], 1.0],\n",
       " [['a', 'd', 'd'], 1.0],\n",
       " [['a', 'd', 'e'], 1.0],\n",
       " [['a', 'd', 'f'], 1.0],\n",
       " [['a', 'd', 'g'], 1.0],\n",
       " [['a', 'd', 'h'], 1.0],\n",
       " [['a', 'd', 'i'], 1.0],\n",
       " [['a', 'd', 'j'], 1.0],\n",
       " [['a', 'd', 'k'], 1.0],\n",
       " [['a', 'd', 'l'], 1.0],\n",
       " [['a', 'd', 'm'], 1.0],\n",
       " [['a', 'd', 'n'], 1.0],\n",
       " [['a', 'd', 'o'], 1.0],\n",
       " [['a', 'd', 'p'], 1.0],\n",
       " [['a', 'd', 'q'], 1.0],\n",
       " [['a', 'd', 'r'], 1.0],\n",
       " [['a', 'd', 's'], 1.0],\n",
       " [['a', 'd', 't'], 1.0],\n",
       " [['a', 'd', 'u'], 1.0],\n",
       " [['a', 'd', 'v'], 1.0],\n",
       " [['a', 'd', 'w'], 1.0],\n",
       " [['a', 'd', 'x'], 1.0],\n",
       " [['a', 'd', 'y'], 1.0],\n",
       " [['a', 'd', 'z'], 1.0],\n",
       " [['a', 'e', 'a'], 1.0],\n",
       " [['a', 'e', 'b'], 1.0],\n",
       " [['a', 'e', 'c'], 1.0],\n",
       " [['a', 'e', 'd'], 1.0],\n",
       " [['a', 'e', 'e'], 1.0],\n",
       " [['a', 'e', 'f'], 1.0],\n",
       " [['a', 'e', 'g'], 1.0],\n",
       " [['a', 'e', 'h'], 1.0],\n",
       " [['a', 'e', 'i'], 1.0],\n",
       " [['a', 'e', 'j'], 1.0],\n",
       " [['a', 'e', 'k'], 1.0],\n",
       " [['a', 'e', 'l'], 1.0],\n",
       " [['a', 'e', 'm'], 1.0],\n",
       " [['a', 'e', 'n'], 1.0],\n",
       " [['a', 'e', 'o'], 1.0],\n",
       " [['a', 'e', 'p'], 1.0],\n",
       " [['a', 'e', 'q'], 1.0],\n",
       " [['a', 'e', 'r'], 1.0],\n",
       " [['a', 'e', 's'], 1.0],\n",
       " [['a', 'e', 't'], 1.0],\n",
       " [['a', 'e', 'u'], 1.0],\n",
       " [['a', 'e', 'v'], 1.0],\n",
       " [['a', 'e', 'w'], 1.0],\n",
       " [['a', 'e', 'x'], 1.0],\n",
       " [['a', 'e', 'y'], 1.0],\n",
       " [['a', 'e', 'z'], 1.0],\n",
       " [['a', 'f', 'a'], 1.0],\n",
       " [['a', 'f', 'b'], 1.0],\n",
       " [['a', 'f', 'c'], 1.0],\n",
       " [['a', 'f', 'd'], 1.0],\n",
       " [['a', 'f', 'e'], 1.0],\n",
       " [['a', 'f', 'f'], 1.0],\n",
       " [['a', 'f', 'g'], 1.0],\n",
       " [['a', 'f', 'h'], 1.0],\n",
       " [['a', 'f', 'i'], 1.0],\n",
       " [['a', 'f', 'j'], 1.0],\n",
       " [['a', 'f', 'k'], 1.0],\n",
       " [['a', 'f', 'l'], 1.0],\n",
       " [['a', 'f', 'm'], 1.0],\n",
       " [['a', 'f', 'n'], 1.0],\n",
       " [['a', 'f', 'o'], 1.0],\n",
       " [['a', 'f', 'p'], 1.0],\n",
       " [['a', 'f', 'q'], 1.0],\n",
       " [['a', 'f', 'r'], 1.0],\n",
       " [['a', 'f', 's'], 1.0],\n",
       " [['a', 'f', 't'], 1.0],\n",
       " [['a', 'f', 'u'], 1.0],\n",
       " [['a', 'f', 'v'], 1.0],\n",
       " [['a', 'f', 'w'], 1.0],\n",
       " [['a', 'f', 'x'], 1.0],\n",
       " [['a', 'f', 'y'], 1.0],\n",
       " [['a', 'f', 'z'], 1.0],\n",
       " [['a', 'g', 'a'], 1.0],\n",
       " [['a', 'g', 'b'], 1.0],\n",
       " [['a', 'g', 'c'], 1.0],\n",
       " [['a', 'g', 'd'], 1.0],\n",
       " [['a', 'g', 'e'], 1.0],\n",
       " [['a', 'g', 'f'], 1.0],\n",
       " [['a', 'g', 'g'], 1.0],\n",
       " [['a', 'g', 'h'], 1.0],\n",
       " [['a', 'g', 'i'], 1.0],\n",
       " [['a', 'g', 'j'], 1.0],\n",
       " [['a', 'g', 'k'], 1.0],\n",
       " [['a', 'g', 'l'], 1.0],\n",
       " [['a', 'g', 'm'], 1.0],\n",
       " [['a', 'g', 'n'], 1.0],\n",
       " [['a', 'g', 'o'], 1.0],\n",
       " [['a', 'g', 'p'], 1.0],\n",
       " [['a', 'g', 'q'], 1.0],\n",
       " [['a', 'g', 'r'], 1.0],\n",
       " [['a', 'g', 's'], 1.0],\n",
       " [['a', 'g', 't'], 1.0],\n",
       " [['a', 'g', 'u'], 1.0],\n",
       " [['a', 'g', 'v'], 1.0],\n",
       " [['a', 'g', 'w'], 1.0],\n",
       " [['a', 'g', 'x'], 1.0],\n",
       " [['a', 'g', 'y'], 1.0],\n",
       " [['a', 'g', 'z'], 1.0],\n",
       " [['a', 'h', 'a'], 1.0],\n",
       " [['a', 'h', 'b'], 1.0],\n",
       " [['a', 'h', 'c'], 1.0],\n",
       " [['a', 'h', 'd'], 1.0],\n",
       " [['a', 'h', 'e'], 1.0],\n",
       " [['a', 'h', 'f'], 1.0],\n",
       " [['a', 'h', 'g'], 1.0],\n",
       " [['a', 'h', 'h'], 1.0],\n",
       " [['a', 'h', 'i'], 1.0],\n",
       " [['a', 'h', 'j'], 1.0],\n",
       " [['a', 'h', 'k'], 1.0],\n",
       " [['a', 'h', 'l'], 1.0],\n",
       " [['a', 'h', 'm'], 1.0],\n",
       " [['a', 'h', 'n'], 1.0],\n",
       " [['a', 'h', 'o'], 1.0],\n",
       " [['a', 'h', 'p'], 1.0],\n",
       " [['a', 'h', 'q'], 1.0],\n",
       " [['a', 'h', 'r'], 1.0],\n",
       " [['a', 'h', 's'], 1.0],\n",
       " [['a', 'h', 't'], 1.0],\n",
       " [['a', 'h', 'u'], 1.0],\n",
       " [['a', 'h', 'v'], 1.0],\n",
       " [['a', 'h', 'w'], 1.0],\n",
       " [['a', 'h', 'x'], 1.0],\n",
       " [['a', 'h', 'y'], 1.0],\n",
       " [['a', 'h', 'z'], 1.0],\n",
       " [['a', 'i', 'a'], 1.0],\n",
       " [['a', 'i', 'b'], 1.0],\n",
       " [['a', 'i', 'c'], 1.0],\n",
       " [['a', 'i', 'd'], 1.0],\n",
       " [['a', 'i', 'e'], 1.0],\n",
       " [['a', 'i', 'f'], 1.0],\n",
       " [['a', 'i', 'g'], 1.0],\n",
       " [['a', 'i', 'h'], 1.0],\n",
       " [['a', 'i', 'i'], 1.0],\n",
       " [['a', 'i', 'j'], 1.0],\n",
       " [['a', 'i', 'k'], 1.0],\n",
       " [['a', 'i', 'l'], 1.0],\n",
       " [['a', 'i', 'm'], 1.0],\n",
       " [['a', 'i', 'n'], 1.0],\n",
       " [['a', 'i', 'o'], 1.0],\n",
       " [['a', 'i', 'p'], 1.0],\n",
       " [['a', 'i', 'q'], 1.0],\n",
       " [['a', 'i', 'r'], 1.0],\n",
       " [['a', 'i', 's'], 1.0],\n",
       " [['a', 'i', 't'], 1.0],\n",
       " [['a', 'i', 'u'], 1.0],\n",
       " [['a', 'i', 'v'], 1.0],\n",
       " [['a', 'i', 'w'], 1.0],\n",
       " [['a', 'i', 'x'], 1.0],\n",
       " [['a', 'i', 'y'], 1.0],\n",
       " [['a', 'i', 'z'], 1.0],\n",
       " [['a', 'j', 'a'], 1.0],\n",
       " [['a', 'j', 'b'], 1.0],\n",
       " [['a', 'j', 'c'], 1.0],\n",
       " [['a', 'j', 'd'], 1.0],\n",
       " [['a', 'j', 'e'], 1.0],\n",
       " [['a', 'j', 'f'], 1.0],\n",
       " [['a', 'j', 'g'], 1.0],\n",
       " [['a', 'j', 'h'], 1.0],\n",
       " [['a', 'j', 'i'], 1.0],\n",
       " [['a', 'j', 'j'], 1.0],\n",
       " [['a', 'j', 'k'], 1.0],\n",
       " [['a', 'j', 'l'], 1.0],\n",
       " [['a', 'j', 'm'], 1.0],\n",
       " [['a', 'j', 'n'], 1.0],\n",
       " [['a', 'j', 'o'], 1.0],\n",
       " [['a', 'j', 'p'], 1.0],\n",
       " [['a', 'j', 'q'], 1.0],\n",
       " [['a', 'j', 'r'], 1.0],\n",
       " [['a', 'j', 's'], 1.0],\n",
       " [['a', 'j', 't'], 1.0],\n",
       " [['a', 'j', 'u'], 1.0],\n",
       " [['a', 'j', 'v'], 1.0],\n",
       " [['a', 'j', 'w'], 1.0],\n",
       " [['a', 'j', 'x'], 1.0],\n",
       " [['a', 'j', 'y'], 1.0],\n",
       " [['a', 'j', 'z'], 1.0],\n",
       " [['a', 'k', 'a'], 1.0],\n",
       " [['a', 'k', 'b'], 1.0],\n",
       " [['a', 'k', 'c'], 1.0],\n",
       " [['a', 'k', 'd'], 1.0],\n",
       " [['a', 'k', 'e'], 1.0],\n",
       " [['a', 'k', 'f'], 1.0],\n",
       " [['a', 'k', 'g'], 1.0],\n",
       " [['a', 'k', 'h'], 1.0],\n",
       " [['a', 'k', 'i'], 1.0],\n",
       " [['a', 'k', 'j'], 1.0],\n",
       " [['a', 'k', 'k'], 1.0],\n",
       " [['a', 'k', 'l'], 1.0],\n",
       " [['a', 'k', 'm'], 1.0],\n",
       " [['a', 'k', 'n'], 1.0],\n",
       " [['a', 'k', 'o'], 1.0],\n",
       " [['a', 'k', 'p'], 1.0],\n",
       " [['a', 'k', 'q'], 1.0],\n",
       " [['a', 'k', 'r'], 1.0],\n",
       " [['a', 'k', 's'], 1.0],\n",
       " [['a', 'k', 't'], 1.0],\n",
       " [['a', 'k', 'u'], 1.0],\n",
       " [['a', 'k', 'v'], 1.0],\n",
       " [['a', 'k', 'w'], 1.0],\n",
       " [['a', 'k', 'x'], 1.0],\n",
       " [['a', 'k', 'y'], 1.0],\n",
       " [['a', 'k', 'z'], 1.0],\n",
       " [['a', 'l', 'a'], 1.0],\n",
       " [['a', 'l', 'b'], 1.0],\n",
       " [['a', 'l', 'c'], 1.0],\n",
       " [['a', 'l', 'd'], 1.0],\n",
       " [['a', 'l', 'e'], 1.0],\n",
       " [['a', 'l', 'f'], 1.0],\n",
       " [['a', 'l', 'g'], 1.0],\n",
       " [['a', 'l', 'h'], 1.0],\n",
       " [['a', 'l', 'i'], 1.0],\n",
       " [['a', 'l', 'j'], 1.0],\n",
       " [['a', 'l', 'k'], 1.0],\n",
       " [['a', 'l', 'l'], 1.0],\n",
       " [['a', 'l', 'm'], 1.0],\n",
       " [['a', 'l', 'n'], 1.0],\n",
       " [['a', 'l', 'o'], 1.0],\n",
       " [['a', 'l', 'p'], 1.0],\n",
       " [['a', 'l', 'q'], 1.0],\n",
       " [['a', 'l', 'r'], 1.0],\n",
       " [['a', 'l', 's'], 1.0],\n",
       " [['a', 'l', 't'], 1.0],\n",
       " [['a', 'l', 'u'], 1.0],\n",
       " [['a', 'l', 'v'], 1.0],\n",
       " [['a', 'l', 'w'], 1.0],\n",
       " [['a', 'l', 'x'], 1.0],\n",
       " [['a', 'l', 'y'], 1.0],\n",
       " [['a', 'l', 'z'], 1.0],\n",
       " [['a', 'm', 'a'], 1.0],\n",
       " [['a', 'm', 'b'], 1.0],\n",
       " [['a', 'm', 'c'], 1.0],\n",
       " [['a', 'm', 'd'], 1.0],\n",
       " [['a', 'm', 'e'], 1.0],\n",
       " [['a', 'm', 'f'], 1.0],\n",
       " [['a', 'm', 'g'], 1.0],\n",
       " [['a', 'm', 'h'], 1.0],\n",
       " [['a', 'm', 'i'], 1.0],\n",
       " [['a', 'm', 'j'], 1.0],\n",
       " [['a', 'm', 'k'], 1.0],\n",
       " [['a', 'm', 'l'], 1.0],\n",
       " [['a', 'm', 'm'], 1.0],\n",
       " [['a', 'm', 'n'], 1.0],\n",
       " [['a', 'm', 'o'], 1.0],\n",
       " [['a', 'm', 'p'], 1.0],\n",
       " [['a', 'm', 'q'], 1.0],\n",
       " [['a', 'm', 'r'], 1.0],\n",
       " [['a', 'm', 's'], 1.0],\n",
       " [['a', 'm', 't'], 1.0],\n",
       " [['a', 'm', 'u'], 1.0],\n",
       " [['a', 'm', 'v'], 1.0],\n",
       " [['a', 'm', 'w'], 1.0],\n",
       " [['a', 'm', 'x'], 1.0],\n",
       " [['a', 'm', 'y'], 1.0],\n",
       " [['a', 'm', 'z'], 1.0],\n",
       " [['a', 'n', 'a'], 1.0],\n",
       " [['a', 'n', 'b'], 1.0],\n",
       " [['a', 'n', 'c'], 1.0],\n",
       " [['a', 'n', 'd'], 1.0],\n",
       " [['a', 'n', 'e'], 1.0],\n",
       " [['a', 'n', 'f'], 1.0],\n",
       " [['a', 'n', 'g'], 1.0],\n",
       " [['a', 'n', 'h'], 1.0],\n",
       " [['a', 'n', 'i'], 1.0],\n",
       " [['a', 'n', 'j'], 1.0],\n",
       " [['a', 'n', 'k'], 1.0],\n",
       " [['a', 'n', 'l'], 1.0],\n",
       " [['a', 'n', 'm'], 1.0],\n",
       " [['a', 'n', 'n'], 1.0],\n",
       " [['a', 'n', 'o'], 1.0],\n",
       " [['a', 'n', 'p'], 1.0],\n",
       " [['a', 'n', 'q'], 1.0],\n",
       " [['a', 'n', 'r'], 1.0],\n",
       " [['a', 'n', 's'], 1.0],\n",
       " [['a', 'n', 't'], 1.0],\n",
       " [['a', 'n', 'u'], 1.0],\n",
       " [['a', 'n', 'v'], 1.0],\n",
       " [['a', 'n', 'w'], 1.0],\n",
       " [['a', 'n', 'x'], 1.0],\n",
       " [['a', 'n', 'y'], 1.0],\n",
       " [['a', 'n', 'z'], 1.0],\n",
       " [['a', 'o', 'a'], 1.0],\n",
       " [['a', 'o', 'b'], 1.0],\n",
       " [['a', 'o', 'c'], 1.0],\n",
       " [['a', 'o', 'd'], 1.0],\n",
       " [['a', 'o', 'e'], 1.0],\n",
       " [['a', 'o', 'f'], 1.0],\n",
       " [['a', 'o', 'g'], 1.0],\n",
       " [['a', 'o', 'h'], 1.0],\n",
       " [['a', 'o', 'i'], 1.0],\n",
       " [['a', 'o', 'j'], 1.0],\n",
       " [['a', 'o', 'k'], 1.0],\n",
       " [['a', 'o', 'l'], 1.0],\n",
       " [['a', 'o', 'm'], 1.0],\n",
       " [['a', 'o', 'n'], 1.0],\n",
       " [['a', 'o', 'o'], 1.0],\n",
       " [['a', 'o', 'p'], 1.0],\n",
       " [['a', 'o', 'q'], 1.0],\n",
       " [['a', 'o', 'r'], 1.0],\n",
       " [['a', 'o', 's'], 1.0],\n",
       " [['a', 'o', 't'], 1.0],\n",
       " [['a', 'o', 'u'], 1.0],\n",
       " [['a', 'o', 'v'], 1.0],\n",
       " [['a', 'o', 'w'], 1.0],\n",
       " [['a', 'o', 'x'], 1.0],\n",
       " [['a', 'o', 'y'], 1.0],\n",
       " [['a', 'o', 'z'], 1.0],\n",
       " [['a', 'p', 'a'], 1.0],\n",
       " [['a', 'p', 'b'], 1.0],\n",
       " [['a', 'p', 'c'], 1.0],\n",
       " [['a', 'p', 'd'], 1.0],\n",
       " [['a', 'p', 'e'], 1.0],\n",
       " [['a', 'p', 'f'], 1.0],\n",
       " [['a', 'p', 'g'], 1.0],\n",
       " [['a', 'p', 'h'], 1.0],\n",
       " [['a', 'p', 'i'], 1.0],\n",
       " [['a', 'p', 'j'], 1.0],\n",
       " [['a', 'p', 'k'], 1.0],\n",
       " [['a', 'p', 'l'], 1.0],\n",
       " [['a', 'p', 'm'], 1.0],\n",
       " [['a', 'p', 'n'], 1.0],\n",
       " [['a', 'p', 'o'], 1.0],\n",
       " [['a', 'p', 'p'], 1.0],\n",
       " [['a', 'p', 'q'], 1.0],\n",
       " [['a', 'p', 'r'], 1.0],\n",
       " [['a', 'p', 's'], 1.0],\n",
       " [['a', 'p', 't'], 1.0],\n",
       " [['a', 'p', 'u'], 1.0],\n",
       " [['a', 'p', 'v'], 1.0],\n",
       " [['a', 'p', 'w'], 1.0],\n",
       " [['a', 'p', 'x'], 1.0],\n",
       " [['a', 'p', 'y'], 1.0],\n",
       " [['a', 'p', 'z'], 1.0],\n",
       " [['a', 'q', 'a'], 1.0],\n",
       " [['a', 'q', 'b'], 1.0],\n",
       " [['a', 'q', 'c'], 1.0],\n",
       " [['a', 'q', 'd'], 1.0],\n",
       " [['a', 'q', 'e'], 1.0],\n",
       " [['a', 'q', 'f'], 1.0],\n",
       " [['a', 'q', 'g'], 1.0],\n",
       " [['a', 'q', 'h'], 1.0],\n",
       " [['a', 'q', 'i'], 1.0],\n",
       " [['a', 'q', 'j'], 1.0],\n",
       " [['a', 'q', 'k'], 1.0],\n",
       " [['a', 'q', 'l'], 1.0],\n",
       " [['a', 'q', 'm'], 1.0],\n",
       " [['a', 'q', 'n'], 1.0],\n",
       " [['a', 'q', 'o'], 1.0],\n",
       " [['a', 'q', 'p'], 1.0],\n",
       " [['a', 'q', 'q'], 1.0],\n",
       " [['a', 'q', 'r'], 1.0],\n",
       " [['a', 'q', 's'], 1.0],\n",
       " [['a', 'q', 't'], 1.0],\n",
       " [['a', 'q', 'u'], 1.0],\n",
       " [['a', 'q', 'v'], 1.0],\n",
       " [['a', 'q', 'w'], 1.0],\n",
       " [['a', 'q', 'x'], 1.0],\n",
       " [['a', 'q', 'y'], 1.0],\n",
       " [['a', 'q', 'z'], 1.0],\n",
       " [['a', 'r', 'a'], 1.0],\n",
       " [['a', 'r', 'b'], 1.0],\n",
       " [['a', 'r', 'c'], 1.0],\n",
       " [['a', 'r', 'd'], 1.0],\n",
       " [['a', 'r', 'e'], 1.0],\n",
       " [['a', 'r', 'f'], 1.0],\n",
       " [['a', 'r', 'g'], 1.0],\n",
       " [['a', 'r', 'h'], 1.0],\n",
       " [['a', 'r', 'i'], 1.0],\n",
       " [['a', 'r', 'j'], 1.0],\n",
       " [['a', 'r', 'k'], 1.0],\n",
       " [['a', 'r', 'l'], 1.0],\n",
       " [['a', 'r', 'm'], 1.0],\n",
       " [['a', 'r', 'n'], 1.0],\n",
       " [['a', 'r', 'o'], 1.0],\n",
       " [['a', 'r', 'p'], 1.0],\n",
       " [['a', 'r', 'q'], 1.0],\n",
       " [['a', 'r', 'r'], 1.0],\n",
       " [['a', 'r', 's'], 1.0],\n",
       " [['a', 'r', 't'], 1.0],\n",
       " [['a', 'r', 'u'], 1.0],\n",
       " [['a', 'r', 'v'], 1.0],\n",
       " [['a', 'r', 'w'], 1.0],\n",
       " [['a', 'r', 'x'], 1.0],\n",
       " [['a', 'r', 'y'], 1.0],\n",
       " [['a', 'r', 'z'], 1.0],\n",
       " [['a', 's', 'a'], 1.0],\n",
       " [['a', 's', 'b'], 1.0],\n",
       " [['a', 's', 'c'], 1.0],\n",
       " [['a', 's', 'd'], 1.0],\n",
       " [['a', 's', 'e'], 1.0],\n",
       " [['a', 's', 'f'], 1.0],\n",
       " [['a', 's', 'g'], 1.0],\n",
       " [['a', 's', 'h'], 1.0],\n",
       " [['a', 's', 'i'], 1.0],\n",
       " [['a', 's', 'j'], 1.0],\n",
       " [['a', 's', 'k'], 1.0],\n",
       " [['a', 's', 'l'], 1.0],\n",
       " [['a', 's', 'm'], 1.0],\n",
       " [['a', 's', 'n'], 1.0],\n",
       " [['a', 's', 'o'], 1.0],\n",
       " [['a', 's', 'p'], 1.0],\n",
       " [['a', 's', 'q'], 1.0],\n",
       " [['a', 's', 'r'], 1.0],\n",
       " [['a', 's', 's'], 1.0],\n",
       " [['a', 's', 't'], 1.0],\n",
       " [['a', 's', 'u'], 1.0],\n",
       " [['a', 's', 'v'], 1.0],\n",
       " [['a', 's', 'w'], 1.0],\n",
       " [['a', 's', 'x'], 1.0],\n",
       " [['a', 's', 'y'], 1.0],\n",
       " [['a', 's', 'z'], 1.0],\n",
       " [['a', 't', 'a'], 1.0],\n",
       " [['a', 't', 'b'], 1.0],\n",
       " [['a', 't', 'c'], 1.0],\n",
       " [['a', 't', 'd'], 1.0],\n",
       " [['a', 't', 'e'], 1.0],\n",
       " [['a', 't', 'f'], 1.0],\n",
       " [['a', 't', 'g'], 1.0],\n",
       " [['a', 't', 'h'], 1.0],\n",
       " [['a', 't', 'i'], 1.0],\n",
       " [['a', 't', 'j'], 1.0],\n",
       " [['a', 't', 'k'], 1.0],\n",
       " [['a', 't', 'l'], 1.0],\n",
       " [['a', 't', 'm'], 1.0],\n",
       " [['a', 't', 'n'], 1.0],\n",
       " [['a', 't', 'o'], 1.0],\n",
       " [['a', 't', 'p'], 1.0],\n",
       " [['a', 't', 'q'], 1.0],\n",
       " [['a', 't', 'r'], 1.0],\n",
       " [['a', 't', 's'], 1.0],\n",
       " [['a', 't', 't'], 1.0],\n",
       " [['a', 't', 'u'], 1.0],\n",
       " [['a', 't', 'v'], 1.0],\n",
       " [['a', 't', 'w'], 1.0],\n",
       " [['a', 't', 'x'], 1.0],\n",
       " [['a', 't', 'y'], 1.0],\n",
       " [['a', 't', 'z'], 1.0],\n",
       " [['a', 'u', 'a'], 1.0],\n",
       " [['a', 'u', 'b'], 1.0],\n",
       " [['a', 'u', 'c'], 1.0],\n",
       " [['a', 'u', 'd'], 1.0],\n",
       " [['a', 'u', 'e'], 1.0],\n",
       " [['a', 'u', 'f'], 1.0],\n",
       " [['a', 'u', 'g'], 1.0],\n",
       " [['a', 'u', 'h'], 1.0],\n",
       " [['a', 'u', 'i'], 1.0],\n",
       " [['a', 'u', 'j'], 1.0],\n",
       " [['a', 'u', 'k'], 1.0],\n",
       " [['a', 'u', 'l'], 1.0],\n",
       " [['a', 'u', 'm'], 1.0],\n",
       " [['a', 'u', 'n'], 1.0],\n",
       " [['a', 'u', 'o'], 1.0],\n",
       " [['a', 'u', 'p'], 1.0],\n",
       " [['a', 'u', 'q'], 1.0],\n",
       " [['a', 'u', 'r'], 1.0],\n",
       " [['a', 'u', 's'], 1.0],\n",
       " [['a', 'u', 't'], 1.0],\n",
       " [['a', 'u', 'u'], 1.0],\n",
       " [['a', 'u', 'v'], 1.0],\n",
       " [['a', 'u', 'w'], 1.0],\n",
       " [['a', 'u', 'x'], 1.0],\n",
       " [['a', 'u', 'y'], 1.0],\n",
       " [['a', 'u', 'z'], 1.0],\n",
       " [['a', 'v', 'a'], 1.0],\n",
       " [['a', 'v', 'b'], 1.0],\n",
       " [['a', 'v', 'c'], 1.0],\n",
       " [['a', 'v', 'd'], 1.0],\n",
       " [['a', 'v', 'e'], 1.0],\n",
       " [['a', 'v', 'f'], 1.0],\n",
       " [['a', 'v', 'g'], 1.0],\n",
       " [['a', 'v', 'h'], 1.0],\n",
       " [['a', 'v', 'i'], 1.0],\n",
       " [['a', 'v', 'j'], 1.0],\n",
       " [['a', 'v', 'k'], 1.0],\n",
       " [['a', 'v', 'l'], 1.0],\n",
       " [['a', 'v', 'm'], 1.0],\n",
       " [['a', 'v', 'n'], 1.0],\n",
       " [['a', 'v', 'o'], 1.0],\n",
       " [['a', 'v', 'p'], 1.0],\n",
       " [['a', 'v', 'q'], 1.0],\n",
       " [['a', 'v', 'r'], 1.0],\n",
       " [['a', 'v', 's'], 1.0],\n",
       " [['a', 'v', 't'], 1.0],\n",
       " [['a', 'v', 'u'], 1.0],\n",
       " [['a', 'v', 'v'], 1.0],\n",
       " [['a', 'v', 'w'], 1.0],\n",
       " [['a', 'v', 'x'], 1.0],\n",
       " [['a', 'v', 'y'], 1.0],\n",
       " [['a', 'v', 'z'], 1.0],\n",
       " [['a', 'w', 'a'], 1.0],\n",
       " [['a', 'w', 'b'], 1.0],\n",
       " [['a', 'w', 'c'], 1.0],\n",
       " [['a', 'w', 'd'], 1.0],\n",
       " [['a', 'w', 'e'], 1.0],\n",
       " [['a', 'w', 'f'], 1.0],\n",
       " [['a', 'w', 'g'], 1.0],\n",
       " [['a', 'w', 'h'], 1.0],\n",
       " [['a', 'w', 'i'], 1.0],\n",
       " [['a', 'w', 'j'], 1.0],\n",
       " [['a', 'w', 'k'], 1.0],\n",
       " [['a', 'w', 'l'], 1.0],\n",
       " [['a', 'w', 'm'], 1.0],\n",
       " [['a', 'w', 'n'], 1.0],\n",
       " [['a', 'w', 'o'], 1.0],\n",
       " [['a', 'w', 'p'], 1.0],\n",
       " [['a', 'w', 'q'], 1.0],\n",
       " [['a', 'w', 'r'], 1.0],\n",
       " [['a', 'w', 's'], 1.0],\n",
       " [['a', 'w', 't'], 1.0],\n",
       " [['a', 'w', 'u'], 1.0],\n",
       " [['a', 'w', 'v'], 1.0],\n",
       " [['a', 'w', 'w'], 1.0],\n",
       " [['a', 'w', 'x'], 1.0],\n",
       " [['a', 'w', 'y'], 1.0],\n",
       " [['a', 'w', 'z'], 1.0],\n",
       " [['a', 'x', 'a'], 1.0],\n",
       " [['a', 'x', 'b'], 1.0],\n",
       " [['a', 'x', 'c'], 1.0],\n",
       " [['a', 'x', 'd'], 1.0],\n",
       " [['a', 'x', 'e'], 1.0],\n",
       " [['a', 'x', 'f'], 1.0],\n",
       " [['a', 'x', 'g'], 1.0],\n",
       " [['a', 'x', 'h'], 1.0],\n",
       " [['a', 'x', 'i'], 1.0],\n",
       " [['a', 'x', 'j'], 1.0],\n",
       " [['a', 'x', 'k'], 1.0],\n",
       " [['a', 'x', 'l'], 1.0],\n",
       " [['a', 'x', 'm'], 1.0],\n",
       " [['a', 'x', 'n'], 1.0],\n",
       " [['a', 'x', 'o'], 1.0],\n",
       " [['a', 'x', 'p'], 1.0],\n",
       " [['a', 'x', 'q'], 1.0],\n",
       " [['a', 'x', 'r'], 1.0],\n",
       " [['a', 'x', 's'], 1.0],\n",
       " [['a', 'x', 't'], 1.0],\n",
       " [['a', 'x', 'u'], 1.0],\n",
       " [['a', 'x', 'v'], 1.0],\n",
       " [['a', 'x', 'w'], 1.0],\n",
       " [['a', 'x', 'x'], 1.0],\n",
       " [['a', 'x', 'y'], 1.0],\n",
       " [['a', 'x', 'z'], 1.0],\n",
       " [['a', 'y', 'a'], 1.0],\n",
       " [['a', 'y', 'b'], 1.0],\n",
       " [['a', 'y', 'c'], 1.0],\n",
       " [['a', 'y', 'd'], 1.0],\n",
       " [['a', 'y', 'e'], 1.0],\n",
       " [['a', 'y', 'f'], 1.0],\n",
       " [['a', 'y', 'g'], 1.0],\n",
       " [['a', 'y', 'h'], 1.0],\n",
       " [['a', 'y', 'i'], 1.0],\n",
       " [['a', 'y', 'j'], 1.0],\n",
       " [['a', 'y', 'k'], 1.0],\n",
       " [['a', 'y', 'l'], 1.0],\n",
       " [['a', 'y', 'm'], 1.0],\n",
       " [['a', 'y', 'n'], 1.0],\n",
       " [['a', 'y', 'o'], 1.0],\n",
       " [['a', 'y', 'p'], 1.0],\n",
       " [['a', 'y', 'q'], 1.0],\n",
       " [['a', 'y', 'r'], 1.0],\n",
       " [['a', 'y', 's'], 1.0],\n",
       " [['a', 'y', 't'], 1.0],\n",
       " [['a', 'y', 'u'], 1.0],\n",
       " [['a', 'y', 'v'], 1.0],\n",
       " [['a', 'y', 'w'], 1.0],\n",
       " [['a', 'y', 'x'], 1.0],\n",
       " [['a', 'y', 'y'], 1.0],\n",
       " [['a', 'y', 'z'], 1.0],\n",
       " [['a', 'z', 'a'], 1.0],\n",
       " [['a', 'z', 'b'], 1.0],\n",
       " [['a', 'z', 'c'], 1.0],\n",
       " [['a', 'z', 'd'], 1.0],\n",
       " [['a', 'z', 'e'], 1.0],\n",
       " [['a', 'z', 'f'], 1.0],\n",
       " [['a', 'z', 'g'], 1.0],\n",
       " [['a', 'z', 'h'], 1.0],\n",
       " [['a', 'z', 'i'], 1.0],\n",
       " [['a', 'z', 'j'], 1.0],\n",
       " [['a', 'z', 'k'], 1.0],\n",
       " [['a', 'z', 'l'], 1.0],\n",
       " [['a', 'z', 'm'], 1.0],\n",
       " [['a', 'z', 'n'], 1.0],\n",
       " [['a', 'z', 'o'], 1.0],\n",
       " [['a', 'z', 'p'], 1.0],\n",
       " [['a', 'z', 'q'], 1.0],\n",
       " [['a', 'z', 'r'], 1.0],\n",
       " [['a', 'z', 's'], 1.0],\n",
       " [['a', 'z', 't'], 1.0],\n",
       " [['a', 'z', 'u'], 1.0],\n",
       " [['a', 'z', 'v'], 1.0],\n",
       " [['a', 'z', 'w'], 1.0],\n",
       " [['a', 'z', 'x'], 1.0],\n",
       " [['a', 'z', 'y'], 1.0],\n",
       " [['a', 'z', 'z'], 1.0],\n",
       " [['b', 'a', 'a'], 1.0],\n",
       " [['b', 'a', 'b'], 1.0],\n",
       " [['b', 'a', 'c'], 1.0],\n",
       " [['b', 'a', 'd'], 1.0],\n",
       " [['b', 'a', 'e'], 1.0],\n",
       " [['b', 'a', 'f'], 1.0],\n",
       " [['b', 'a', 'g'], 1.0],\n",
       " [['b', 'a', 'h'], 1.0],\n",
       " [['b', 'a', 'i'], 1.0],\n",
       " [['b', 'a', 'j'], 1.0],\n",
       " [['b', 'a', 'k'], 1.0],\n",
       " [['b', 'a', 'l'], 1.0],\n",
       " [['b', 'a', 'm'], 1.0],\n",
       " [['b', 'a', 'n'], 1.0],\n",
       " [['b', 'a', 'o'], 1.0],\n",
       " [['b', 'a', 'p'], 1.0],\n",
       " [['b', 'a', 'q'], 1.0],\n",
       " [['b', 'a', 'r'], 1.0],\n",
       " [['b', 'a', 's'], 1.0],\n",
       " [['b', 'a', 't'], 1.0],\n",
       " [['b', 'a', 'u'], 1.0],\n",
       " [['b', 'a', 'v'], 1.0],\n",
       " [['b', 'a', 'w'], 1.0],\n",
       " [['b', 'a', 'x'], 1.0],\n",
       " [['b', 'a', 'y'], 1.0],\n",
       " [['b', 'a', 'z'], 1.0],\n",
       " [['b', 'b', 'a'], 1.0],\n",
       " [['b', 'b', 'b'], 1.0],\n",
       " [['b', 'b', 'c'], 1.0],\n",
       " [['b', 'b', 'd'], 1.0],\n",
       " [['b', 'b', 'e'], 1.0],\n",
       " [['b', 'b', 'f'], 1.0],\n",
       " [['b', 'b', 'g'], 1.0],\n",
       " [['b', 'b', 'h'], 1.0],\n",
       " [['b', 'b', 'i'], 1.0],\n",
       " [['b', 'b', 'j'], 1.0],\n",
       " [['b', 'b', 'k'], 1.0],\n",
       " [['b', 'b', 'l'], 1.0],\n",
       " [['b', 'b', 'm'], 1.0],\n",
       " [['b', 'b', 'n'], 1.0],\n",
       " [['b', 'b', 'o'], 1.0],\n",
       " [['b', 'b', 'p'], 1.0],\n",
       " [['b', 'b', 'q'], 1.0],\n",
       " [['b', 'b', 'r'], 1.0],\n",
       " [['b', 'b', 's'], 1.0],\n",
       " [['b', 'b', 't'], 1.0],\n",
       " [['b', 'b', 'u'], 1.0],\n",
       " [['b', 'b', 'v'], 1.0],\n",
       " [['b', 'b', 'w'], 1.0],\n",
       " [['b', 'b', 'x'], 1.0],\n",
       " [['b', 'b', 'y'], 1.0],\n",
       " [['b', 'b', 'z'], 1.0],\n",
       " [['b', 'c', 'a'], 1.0],\n",
       " [['b', 'c', 'b'], 1.0],\n",
       " [['b', 'c', 'c'], 1.0],\n",
       " [['b', 'c', 'd'], 1.0],\n",
       " [['b', 'c', 'e'], 1.0],\n",
       " [['b', 'c', 'f'], 1.0],\n",
       " [['b', 'c', 'g'], 1.0],\n",
       " [['b', 'c', 'h'], 1.0],\n",
       " [['b', 'c', 'i'], 1.0],\n",
       " [['b', 'c', 'j'], 1.0],\n",
       " [['b', 'c', 'k'], 1.0],\n",
       " [['b', 'c', 'l'], 1.0],\n",
       " [['b', 'c', 'm'], 1.0],\n",
       " [['b', 'c', 'n'], 1.0],\n",
       " [['b', 'c', 'o'], 1.0],\n",
       " [['b', 'c', 'p'], 1.0],\n",
       " [['b', 'c', 'q'], 1.0],\n",
       " [['b', 'c', 'r'], 1.0],\n",
       " [['b', 'c', 's'], 1.0],\n",
       " [['b', 'c', 't'], 1.0],\n",
       " [['b', 'c', 'u'], 1.0],\n",
       " [['b', 'c', 'v'], 1.0],\n",
       " [['b', 'c', 'w'], 1.0],\n",
       " [['b', 'c', 'x'], 1.0],\n",
       " [['b', 'c', 'y'], 1.0],\n",
       " [['b', 'c', 'z'], 1.0],\n",
       " [['b', 'd', 'a'], 1.0],\n",
       " [['b', 'd', 'b'], 1.0],\n",
       " [['b', 'd', 'c'], 1.0],\n",
       " [['b', 'd', 'd'], 1.0],\n",
       " [['b', 'd', 'e'], 1.0],\n",
       " [['b', 'd', 'f'], 1.0],\n",
       " [['b', 'd', 'g'], 1.0],\n",
       " [['b', 'd', 'h'], 1.0],\n",
       " [['b', 'd', 'i'], 1.0],\n",
       " [['b', 'd', 'j'], 1.0],\n",
       " [['b', 'd', 'k'], 1.0],\n",
       " [['b', 'd', 'l'], 1.0],\n",
       " [['b', 'd', 'm'], 1.0],\n",
       " [['b', 'd', 'n'], 1.0],\n",
       " [['b', 'd', 'o'], 1.0],\n",
       " [['b', 'd', 'p'], 1.0],\n",
       " [['b', 'd', 'q'], 1.0],\n",
       " [['b', 'd', 'r'], 1.0],\n",
       " [['b', 'd', 's'], 1.0],\n",
       " [['b', 'd', 't'], 1.0],\n",
       " [['b', 'd', 'u'], 1.0],\n",
       " [['b', 'd', 'v'], 1.0],\n",
       " [['b', 'd', 'w'], 1.0],\n",
       " [['b', 'd', 'x'], 1.0],\n",
       " [['b', 'd', 'y'], 1.0],\n",
       " [['b', 'd', 'z'], 1.0],\n",
       " [['b', 'e', 'a'], 1.0],\n",
       " [['b', 'e', 'b'], 1.0],\n",
       " [['b', 'e', 'c'], 1.0],\n",
       " [['b', 'e', 'd'], 1.0],\n",
       " [['b', 'e', 'e'], 1.0],\n",
       " [['b', 'e', 'f'], 1.0],\n",
       " [['b', 'e', 'g'], 1.0],\n",
       " [['b', 'e', 'h'], 1.0],\n",
       " [['b', 'e', 'i'], 1.0],\n",
       " [['b', 'e', 'j'], 1.0],\n",
       " [['b', 'e', 'k'], 1.0],\n",
       " [['b', 'e', 'l'], 1.0],\n",
       " [['b', 'e', 'm'], 1.0],\n",
       " [['b', 'e', 'n'], 1.0],\n",
       " [['b', 'e', 'o'], 1.0],\n",
       " [['b', 'e', 'p'], 1.0],\n",
       " [['b', 'e', 'q'], 1.0],\n",
       " [['b', 'e', 'r'], 1.0],\n",
       " [['b', 'e', 's'], 1.0],\n",
       " [['b', 'e', 't'], 1.0],\n",
       " [['b', 'e', 'u'], 1.0],\n",
       " [['b', 'e', 'v'], 1.0],\n",
       " [['b', 'e', 'w'], 1.0],\n",
       " [['b', 'e', 'x'], 1.0],\n",
       " [['b', 'e', 'y'], 1.0],\n",
       " [['b', 'e', 'z'], 1.0],\n",
       " [['b', 'f', 'a'], 1.0],\n",
       " [['b', 'f', 'b'], 1.0],\n",
       " [['b', 'f', 'c'], 1.0],\n",
       " [['b', 'f', 'd'], 1.0],\n",
       " [['b', 'f', 'e'], 1.0],\n",
       " [['b', 'f', 'f'], 1.0],\n",
       " [['b', 'f', 'g'], 1.0],\n",
       " [['b', 'f', 'h'], 1.0],\n",
       " [['b', 'f', 'i'], 1.0],\n",
       " [['b', 'f', 'j'], 1.0],\n",
       " [['b', 'f', 'k'], 1.0],\n",
       " [['b', 'f', 'l'], 1.0],\n",
       " [['b', 'f', 'm'], 1.0],\n",
       " [['b', 'f', 'n'], 1.0],\n",
       " [['b', 'f', 'o'], 1.0],\n",
       " [['b', 'f', 'p'], 1.0],\n",
       " [['b', 'f', 'q'], 1.0],\n",
       " [['b', 'f', 'r'], 1.0],\n",
       " [['b', 'f', 's'], 1.0],\n",
       " [['b', 'f', 't'], 1.0],\n",
       " [['b', 'f', 'u'], 1.0],\n",
       " [['b', 'f', 'v'], 1.0],\n",
       " [['b', 'f', 'w'], 1.0],\n",
       " [['b', 'f', 'x'], 1.0],\n",
       " [['b', 'f', 'y'], 1.0],\n",
       " [['b', 'f', 'z'], 1.0],\n",
       " [['b', 'g', 'a'], 1.0],\n",
       " [['b', 'g', 'b'], 1.0],\n",
       " [['b', 'g', 'c'], 1.0],\n",
       " [['b', 'g', 'd'], 1.0],\n",
       " [['b', 'g', 'e'], 1.0],\n",
       " [['b', 'g', 'f'], 1.0],\n",
       " [['b', 'g', 'g'], 1.0],\n",
       " [['b', 'g', 'h'], 1.0],\n",
       " [['b', 'g', 'i'], 1.0],\n",
       " [['b', 'g', 'j'], 1.0],\n",
       " [['b', 'g', 'k'], 1.0],\n",
       " [['b', 'g', 'l'], 1.0],\n",
       " [['b', 'g', 'm'], 1.0],\n",
       " [['b', 'g', 'n'], 1.0],\n",
       " [['b', 'g', 'o'], 1.0],\n",
       " [['b', 'g', 'p'], 1.0],\n",
       " [['b', 'g', 'q'], 1.0],\n",
       " [['b', 'g', 'r'], 1.0],\n",
       " [['b', 'g', 's'], 1.0],\n",
       " [['b', 'g', 't'], 1.0],\n",
       " [['b', 'g', 'u'], 1.0],\n",
       " [['b', 'g', 'v'], 1.0],\n",
       " [['b', 'g', 'w'], 1.0],\n",
       " [['b', 'g', 'x'], 1.0],\n",
       " [['b', 'g', 'y'], 1.0],\n",
       " [['b', 'g', 'z'], 1.0],\n",
       " [['b', 'h', 'a'], 1.0],\n",
       " [['b', 'h', 'b'], 1.0],\n",
       " [['b', 'h', 'c'], 1.0],\n",
       " [['b', 'h', 'd'], 1.0],\n",
       " [['b', 'h', 'e'], 1.0],\n",
       " [['b', 'h', 'f'], 1.0],\n",
       " [['b', 'h', 'g'], 1.0],\n",
       " [['b', 'h', 'h'], 1.0],\n",
       " [['b', 'h', 'i'], 1.0],\n",
       " [['b', 'h', 'j'], 1.0],\n",
       " [['b', 'h', 'k'], 1.0],\n",
       " [['b', 'h', 'l'], 1.0],\n",
       " [['b', 'h', 'm'], 1.0],\n",
       " [['b', 'h', 'n'], 1.0],\n",
       " [['b', 'h', 'o'], 1.0],\n",
       " [['b', 'h', 'p'], 1.0],\n",
       " [['b', 'h', 'q'], 1.0],\n",
       " [['b', 'h', 'r'], 1.0],\n",
       " [['b', 'h', 's'], 1.0],\n",
       " [['b', 'h', 't'], 1.0],\n",
       " [['b', 'h', 'u'], 1.0],\n",
       " [['b', 'h', 'v'], 1.0],\n",
       " [['b', 'h', 'w'], 1.0],\n",
       " [['b', 'h', 'x'], 1.0],\n",
       " [['b', 'h', 'y'], 1.0],\n",
       " [['b', 'h', 'z'], 1.0],\n",
       " [['b', 'i', 'a'], 1.0],\n",
       " [['b', 'i', 'b'], 1.0],\n",
       " [['b', 'i', 'c'], 1.0],\n",
       " [['b', 'i', 'd'], 1.0],\n",
       " [['b', 'i', 'e'], 1.0],\n",
       " [['b', 'i', 'f'], 1.0],\n",
       " [['b', 'i', 'g'], 1.0],\n",
       " [['b', 'i', 'h'], 1.0],\n",
       " [['b', 'i', 'i'], 1.0],\n",
       " [['b', 'i', 'j'], 1.0],\n",
       " [['b', 'i', 'k'], 1.0],\n",
       " [['b', 'i', 'l'], 1.0],\n",
       " [['b', 'i', 'm'], 1.0],\n",
       " [['b', 'i', 'n'], 1.0],\n",
       " [['b', 'i', 'o'], 1.0],\n",
       " [['b', 'i', 'p'], 1.0],\n",
       " [['b', 'i', 'q'], 1.0],\n",
       " [['b', 'i', 'r'], 1.0],\n",
       " [['b', 'i', 's'], 1.0],\n",
       " [['b', 'i', 't'], 1.0],\n",
       " [['b', 'i', 'u'], 1.0],\n",
       " [['b', 'i', 'v'], 1.0],\n",
       " [['b', 'i', 'w'], 1.0],\n",
       " [['b', 'i', 'x'], 1.0],\n",
       " [['b', 'i', 'y'], 1.0],\n",
       " [['b', 'i', 'z'], 1.0],\n",
       " [['b', 'j', 'a'], 1.0],\n",
       " [['b', 'j', 'b'], 1.0],\n",
       " [['b', 'j', 'c'], 1.0],\n",
       " [['b', 'j', 'd'], 1.0],\n",
       " [['b', 'j', 'e'], 1.0],\n",
       " [['b', 'j', 'f'], 1.0],\n",
       " [['b', 'j', 'g'], 1.0],\n",
       " [['b', 'j', 'h'], 1.0],\n",
       " [['b', 'j', 'i'], 1.0],\n",
       " [['b', 'j', 'j'], 1.0],\n",
       " [['b', 'j', 'k'], 1.0],\n",
       " [['b', 'j', 'l'], 1.0],\n",
       " [['b', 'j', 'm'], 1.0],\n",
       " [['b', 'j', 'n'], 1.0],\n",
       " [['b', 'j', 'o'], 1.0],\n",
       " [['b', 'j', 'p'], 1.0],\n",
       " [['b', 'j', 'q'], 1.0],\n",
       " [['b', 'j', 'r'], 1.0],\n",
       " [['b', 'j', 's'], 1.0],\n",
       " [['b', 'j', 't'], 1.0],\n",
       " [['b', 'j', 'u'], 1.0],\n",
       " [['b', 'j', 'v'], 1.0],\n",
       " [['b', 'j', 'w'], 1.0],\n",
       " [['b', 'j', 'x'], 1.0],\n",
       " [['b', 'j', 'y'], 1.0],\n",
       " [['b', 'j', 'z'], 1.0],\n",
       " [['b', 'k', 'a'], 1.0],\n",
       " [['b', 'k', 'b'], 1.0],\n",
       " [['b', 'k', 'c'], 1.0],\n",
       " [['b', 'k', 'd'], 1.0],\n",
       " [['b', 'k', 'e'], 1.0],\n",
       " [['b', 'k', 'f'], 1.0],\n",
       " [['b', 'k', 'g'], 1.0],\n",
       " [['b', 'k', 'h'], 1.0],\n",
       " [['b', 'k', 'i'], 1.0],\n",
       " [['b', 'k', 'j'], 1.0],\n",
       " [['b', 'k', 'k'], 1.0],\n",
       " [['b', 'k', 'l'], 1.0],\n",
       " [['b', 'k', 'm'], 1.0],\n",
       " [['b', 'k', 'n'], 1.0],\n",
       " [['b', 'k', 'o'], 1.0],\n",
       " [['b', 'k', 'p'], 1.0],\n",
       " [['b', 'k', 'q'], 1.0],\n",
       " [['b', 'k', 'r'], 1.0],\n",
       " [['b', 'k', 's'], 1.0],\n",
       " [['b', 'k', 't'], 1.0],\n",
       " [['b', 'k', 'u'], 1.0],\n",
       " [['b', 'k', 'v'], 1.0],\n",
       " [['b', 'k', 'w'], 1.0],\n",
       " [['b', 'k', 'x'], 1.0],\n",
       " [['b', 'k', 'y'], 1.0],\n",
       " [['b', 'k', 'z'], 1.0],\n",
       " [['b', 'l', 'a'], 1.0],\n",
       " [['b', 'l', 'b'], 1.0],\n",
       " [['b', 'l', 'c'], 1.0],\n",
       " [['b', 'l', 'd'], 1.0],\n",
       " [['b', 'l', 'e'], 1.0],\n",
       " [['b', 'l', 'f'], 1.0],\n",
       " [['b', 'l', 'g'], 1.0],\n",
       " [['b', 'l', 'h'], 1.0],\n",
       " [['b', 'l', 'i'], 1.0],\n",
       " [['b', 'l', 'j'], 1.0],\n",
       " [['b', 'l', 'k'], 1.0],\n",
       " [['b', 'l', 'l'], 1.0],\n",
       " [['b', 'l', 'm'], 1.0],\n",
       " [['b', 'l', 'n'], 1.0],\n",
       " [['b', 'l', 'o'], 1.0],\n",
       " [['b', 'l', 'p'], 1.0],\n",
       " [['b', 'l', 'q'], 1.0],\n",
       " [['b', 'l', 'r'], 1.0],\n",
       " [['b', 'l', 's'], 1.0],\n",
       " [['b', 'l', 't'], 1.0],\n",
       " [['b', 'l', 'u'], 1.0],\n",
       " [['b', 'l', 'v'], 1.0],\n",
       " [['b', 'l', 'w'], 1.0],\n",
       " [['b', 'l', 'x'], 1.0],\n",
       " [['b', 'l', 'y'], 1.0],\n",
       " [['b', 'l', 'z'], 1.0],\n",
       " [['b', 'm', 'a'], 1.0],\n",
       " [['b', 'm', 'b'], 1.0],\n",
       " [['b', 'm', 'c'], 1.0],\n",
       " [['b', 'm', 'd'], 1.0],\n",
       " [['b', 'm', 'e'], 1.0],\n",
       " [['b', 'm', 'f'], 1.0],\n",
       " [['b', 'm', 'g'], 1.0],\n",
       " [['b', 'm', 'h'], 1.0],\n",
       " [['b', 'm', 'i'], 1.0],\n",
       " [['b', 'm', 'j'], 1.0],\n",
       " [['b', 'm', 'k'], 1.0],\n",
       " [['b', 'm', 'l'], 1.0],\n",
       " ...]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letters = list(string.ascii_lowercase)\n",
    "letters +[' ', '.','!','?']\n",
    "seq3 = [[[i,j,k], 1.0] for i in letters for j in letters for k in letters]\n",
    "seq3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHich sequence is the best suited for text start? \n",
    "for s in seq2:\n",
    "    #calculate s[1]\n",
    "    s[1] = charscore(start, s[0][0]) + charscore(start[1:]+s[0][0], s[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(start, sequence):\n",
    "    for s in sequence:\n",
    "        in1 = start[1:]+s[0][0]\n",
    "        in2 = start[2:]+s[0][0]+s[0][1]\n",
    "        s[1] = charscore(start, s[0][0]) + charscore(in1, s[0][1]) + charscore(in2, s[0][2])\n",
    "    sortseq = sorted(sequence, key=lambda data:data[1])\n",
    "    return (sortseq[-1][0][0] + sortseq[-1][0][1] + sortseq[-1][0][2])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_text(start, sequence,cnt):\n",
    "    result = start\n",
    "    for i in range(cnt):\n",
    "        nxt = beam_search(start, sequence)\n",
    "        result = result +nxt\n",
    "        start = start[3:] + nxt\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The sun andtheres'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_text(start,seq3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The sun andtheresticallywheresiationallysistershopped'"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Beam search resulted in a sequence like below, with no spaces\n",
    "beam_text(start,seq3, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHich sequence is the best suited for text start? \n",
    "for s in seq3:\n",
    "    in1 = start[1:]+s[0][0]\n",
    "    in2 = start[2:]+s[0][0]+s[0][1]\n",
    "    s[1] = charscore(start, s[0][0]) + charscore(in1, s[0][1]) + charscore(in2, s[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The sun '"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seq3)\n",
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['t', 'h', 'e'], -4.4925756],\n",
       " [['y', 'o', 'u'], -4.188985],\n",
       " [['a', 'n', 'd'], -4.0476913]]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sorting items by their value\n",
    "sortseq = sorted(seq3, key=lambda data:data[1])\n",
    "sortseq[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and'"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortseq[-1][0][0] + sortseq[-1][0][1] + sortseq[-1][0][2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = m(VV(idxs.transpose(0,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last layer activations corresponding to each token\n",
    "Displaying last 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-12.2267\n",
       "-12.6297\n",
       " -5.9591\n",
       " -5.0845\n",
       " -6.8274\n",
       " -7.6905\n",
       " -8.2778\n",
       " -8.8224\n",
       " -4.3681\n",
       " -5.2212\n",
       "[torch.cuda.FloatTensor of size 10 (GPU 0)]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[-1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'r'"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#i = np.argmax(to_np(p))\n",
    "output = np.argmax(to_np(p[-1]))\n",
    "TEXT.vocab.itos[output]\n",
    "#output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 88 symbols... the last few tokes of this symbol are assigned to 0's \n",
    "#TEXT.vocab.stoi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "216px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
