{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniques for generating text\n",
    "\n",
    "Here we discuss some common techniques for generating text once we have build a model. So far we have built a model that can output a single character given a sequence (8 in our case) input characters. Out goal is to generate a continuous set of characters. The main approaches we will go over are: \n",
    "\n",
    "#### Greedy or best fit: \n",
    "This is the simplest method in which we recursively select the most probable next sequence character. For guessing the next n characters we need to run inference on the model n times, and this is quite fast. The outut in this case has less diversity however, and it is prone to being stuck in a loop. There seem to be a set of char sequences which are highly probable and the prediction often converges to  one of these sequences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical (multinomial) distribution \n",
    "Models the occurance of an outcome out of k categories each with defined probability of occurances. \n",
    "In order to inject more diversity into the outputs, instead of selecting the most probable haracter, we model the NN as a probability event and observe which token is output with one 'play'. This adds some serendipity into the system.\n",
    "\n",
    "Observing the results however, this method cuts across words. The structure inherent within words is broken more easily. One alternative would be to use the multinomial distribution sparingly, in combination with the greedy best fit approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 3 : Greedy for words, best fit after a space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search\n",
    "Beam search is a method of trying to get more 'optimal results' and we look at predicting sequences longer than 1 character in one 'iteration'. Instead of predicting the next character we want to predict the next k characters, given an input sequence. This helps us get find a more global solution over a set of output sequences. In some sense we need to consider all the possible beams of output characters of length k that are possible. and need to have some metric of comparison. A couple of things help us out while calculating probabilitues: \n",
    "First, using bayes rule, we can model the probability of a getting a particular output sequence as aproduct of individual conditional probability sequences. \n",
    "\n",
    "For eg: Score of predicting 'and' given input equence 'The sun ' will be \n",
    "score = P('a'| input = 'The sun ') * P('n'| input = 'he sun a') * P('d' | input = 'e sun an')  \n",
    "Each of the P values above can be obtained by running \n",
    "Second, the output of the model softmas is often in log format, and this makes out implementations easier, we can add the log values instead of multiplying them. \n",
    "\n",
    "#### Computational considerations/Tweaks to Beam search\n",
    "* Attempt to reduce the bottleneck - > charscore function\n",
    "* Sequences of alphabets only _ 30 * 30 * 30 \n",
    "* Instead of sampling all combinations, most probable 10 in each case. \n",
    "    Significantly faster results OR get beam sequences of longer length\n",
    "    (10 *10 *10 *10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nik/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from fastai.io import *\n",
    "from fastai.conv_learner import *\n",
    "from fastai.column_data import *\n",
    "\n",
    "from fastai.nlp import *\n",
    "from fastai.lm_rnn import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "from torchtext import vocab, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup for Jokes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the jokes\n",
    "PATH='/home/nik/data/jokes/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jkdf = pd.read_json(\"/home/nik/data/jokes/reddit_jokes.json\")\n",
    "jkdf[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1622, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    What did the bartender say to the jumper cable...\n",
       "1    Don't you hate jokes about German sausage? The...\n",
       "Name: Joke, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdf = pd.read_csv('/home/nik/data/jokes/reddit-cleanjokes.txt')\n",
    "print(cdf.shape)\n",
    "cdf.Joke[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2950, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    I just asked my husband if he remembers what t...\n",
       "1    People used to laugh at me when I would say \"I...\n",
       "Name: Joke, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odf = pd.read_csv('/home/nik/data/jokes/onlinefun.txt')\n",
    "print(odf.shape)\n",
    "odf.Joke[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44758, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [watching TV] GF: Tickle my back please ME: Is...\n",
       "1    \"how'd your football team football today?\" tho...\n",
       "Name: Joke, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdf = pd.read_csv('/home/nik/data/jokes/funnytweeter.txt')\n",
    "print(fdf.shape)\n",
    "fdf.Joke[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49330, 2)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigdf = cdf.append(odf).append(fdf)\n",
    "bigdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    what did the bartender say to the jumper cable...\n",
       "1    don't you hate jokes about german sausage? the...\n",
       "2    two artists had an art contest... it ended in ...\n",
       "3    why did the chicken cross the playground? to g...\n",
       "4     what gun do you use to hunt a moose? a moosecut!\n",
       "Name: Joke, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lowercase\n",
    "bigdf.Joke = bigdf.Joke.str.lower()\n",
    "bigdf.Joke[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join dataset \n",
    "train, test = train_test_split(bigdf, test_size=0.15)\n",
    "\n",
    "#Create a single text file with with the required jokes\n",
    "#Create train, validation text files\n",
    "joketext = '. '.join(bigdf.Joke)\n",
    "traintxt = '.  '.join(train.Joke)\n",
    "valtxt = '. '.join(test.Joke)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Get dataset containing 1000top-rated jokes\n",
    "top_jokes = jkdf.sort_values('score',ascending=False)\n",
    "top_jokes[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display word count for each row , select jokes with wc less than 100 \n",
    "top_jokes['wc'] = top_jokes.body.apply(lambda x: len(x.split()))\n",
    "#Select jokes with less than 200 words \n",
    "#top_jokes[top_jokes.wc < 200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#working dataframe with 5000 jokes\n",
    "jdf = top_jokes[top_jokes.wc < 200][:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Merging title and body, lowercase the text\n",
    "jdf['space'] = ' '\n",
    "jdf['full'] =  jdf['title'] + jdf['space'] + jdf['body']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  jdf['full'] = jdf['full'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Create two files trn.txt and val.txt based on joketext (80% and 20% jokes respectively)\n",
    " \n",
    " * Use jdf_subset to create train, val datasets so that you can generate the tokens one time for jdf_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jdf_subset = jdf \n",
    "train, test = train_test_split(jdf_subset, test_size=0.2)\n",
    "\n",
    "#Create a single text file with with the required jokes\n",
    "#Create train, validation text files\n",
    "joketext = '. '.join(jdf_subset.full)\n",
    "traintxt = '.  '.join(train.full)\n",
    "valtxt = '. '.join(test.full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5231066:what did the bartender say to the jumper cables? you better not try to start anything.. don't you ha\n",
      "at what age does ryan gosling have to change his name to ryan goose.  [company meeting] manager: $50\n",
      "all-day christmas music at work, day 4: just googled \"candy cane prison shank\". i'm gonna start givi\n"
     ]
    }
   ],
   "source": [
    "print (str(len(joketext)) + ':' + joketext[:100])\n",
    "print(traintxt[:100])\n",
    "print(valtxt[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784563"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_trn = open(\"/home/nik/data/jokes/trn_bigdf/trn_bigdf_beam.txt\", \"w\")\n",
    "f_trn.write(traintxt) # full data gives RAM issues\n",
    "f_val = open(\"/home/nik/data/jokes/val_bigdf/val_bigdf_beam.txt\", \"w\")\n",
    "f_val.write(valtxt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Form the indices for mapping from chars to tokesn and back again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 69\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\x00   ! \" # $ % & \\' ( ) * + , - . / 0 1 2 3 4 5 6 7 8 9 : ; < = > ? @ [ ] ^ _ ` a b c d e f g h i j k l m n o p q r s t u v w x y z { | } ~'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(joketext)))\n",
    "vocab_size = len(chars)+1\n",
    "print('total chars:', vocab_size)\n",
    "chars.insert(0, \"\\0\")\n",
    "' '.join(chars[0:]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "funnytweeter.txt  reddit-cleanjokes.txt  \u001b[0m\u001b[01;34mtrn_bigdf\u001b[0m/\r\n",
      "\u001b[01;34mmodels\u001b[0m/           reddit_jokes.json      \u001b[01;34mval\u001b[0m/\r\n",
      "onlinefun.txt     \u001b[01;34mtrn\u001b[0m/                   \u001b[01;34mval_bigdf\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "TRN_PATH = 'trn_bigdf/'\n",
    "VAL_PATH = 'val_bigdf/'\n",
    "TRN = f'{PATH}{TRN_PATH}'\n",
    "VAL = f'{PATH}{VAL_PATH}'\n",
    "%ls {PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_bigdf_beam.txt\r\n"
     ]
    }
   ],
   "source": [
    "%ls {PATH}val_bigdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8765, 70, 1, 4488431)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT = data.Field(lower=True, tokenize=list)\n",
    "bs=32; bptt=16; n_fac=42; n_hidden=256\n",
    "\n",
    "FILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH)\n",
    "md = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=3)\n",
    "\n",
    "len(md.trn_dl), md.nt, len(md.trn_ds), len(md.trn_ds[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import sgdr\n",
    "n_hidden=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqStatefulLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac, bs, nl):\n",
    "        super().__init__()\n",
    "        self.vocab_size,self.nl = vocab_size,nl\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.LSTM(n_fac, n_hidden, nl, dropout=0.5)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init_hidden(bs)\n",
    "        \n",
    "    def forward(self, cs):\n",
    "        bs = cs[0].size(0)\n",
    "        if self.h[0].size(1) != bs: self.init_hidden(bs)\n",
    "        outp,h = self.rnn(self.e(cs), self.h)\n",
    "        self.h = repackage_var(h)\n",
    "        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n",
    "    \n",
    "    def init_hidden(self, bs):\n",
    "        self.h = (V(torch.zeros(self.nl, bs, n_hidden)),\n",
    "                  V(torch.zeros(self.nl, bs, n_hidden)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharSeqStatefulLSTM(md.nt, n_fac, 512, 2).cuda()\n",
    "lo = LayerOptimizer(optim.Adam, m, 1e-2, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'{PATH}models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec6d763eba34bda8c5fe4a8833263e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                 \n",
      "    0      1.682614   1.675064  \n",
      "    1      1.6217     1.638818                                 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.63882])]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(m, md, 2, lo.opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ee903fb46549c3906af184fe4c2891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                 \n",
      "    0      1.462887   1.482441  \n",
      "    1      1.55383    1.547831                                 \n",
      "    2      1.42874    1.446505                                 \n",
      "    3      1.583334   1.592402                                 \n",
      "    4      1.523529   1.539407                                 \n",
      "    5      1.435887   1.465911                                 \n",
      "    6      1.402394   1.417623                                 \n",
      "    7      1.571536   1.594059                                 \n",
      "    8      1.570818   1.576339                                 \n",
      "    9      1.543182   1.551916                                 \n",
      "    10     1.508622   1.525417                                 \n",
      "    11     1.461939   1.483503                                 \n",
      "    12     1.417157   1.438703                                 \n",
      "    13     1.382158   1.402109                                 \n",
      "    14     1.339401   1.389601                                 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.3896])]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "on_end = lambda sched, cycle: save_model(m, f'{PATH}models/cyc_bigdf{cycle}')\n",
    "cb = [CosAnneal(lo, len(md.trn_dl), cycle_mult=2, on_cycle_end=on_end)]\n",
    "fit(m, md, 2**4-1, lo.opt, F.nll_loss, callbacks=cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e785df653c446aa97f7333de1c2695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=31), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                 \n",
      "    0      1.405585   1.438247  \n",
      "    1      1.505313   1.52003                                  \n",
      "    2      1.380352   1.425153                                 \n",
      "    3      1.564948   1.572429                                 \n",
      "    4      1.501501   1.514914                                 \n",
      "    5      1.416701   1.44532                                  \n",
      "    6      1.371029   1.405033                                 \n",
      "    7      1.583335   1.59442                                  \n",
      "    8      1.554218   1.562134                                 \n",
      "    9      1.541605   1.556651                                 \n",
      "    10     1.509336   1.499474                                 \n",
      "    11     1.464179   1.469239                                 \n",
      "    12     1.394566   1.430665                                 \n",
      "    13     1.3614     1.397353                                 \n",
      "    14     1.346346   1.382938                                 \n",
      "    15     1.574673   1.576873                                 \n",
      "    16     1.577911   1.584663                                 \n",
      "    17     1.573287   1.563866                                 \n",
      "    18     1.56707    1.568893                                 \n",
      "    19     1.551959   1.551395                                 \n",
      "    20     1.539083   1.550653                                 \n",
      "    21     1.52571    1.517992                                 \n",
      "    22     1.471489   1.491828                                 \n",
      "    23     1.46406    1.487384                                 \n",
      "    24     1.458896   1.457911                                 \n",
      "    25     1.414034   1.442817                                 \n",
      "    26     1.404622   1.419801                                 \n",
      "    27     1.358895   1.397532                                 \n",
      "    28     1.336842   1.378051                                 \n",
      "    29     1.318464   1.364274                                 \n",
      "    30     1.305546   1.3598                                   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.3598])]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "on_end = lambda sched, cycle: save_model(m, f'{PATH}models/cyc_bigdf{cycle}')\n",
    "cb = [CosAnneal(lo, len(md.trn_dl), cycle_mult=2, on_cycle_end=on_end)]\n",
    "fit(m, md, 2**5-1, lo.opt, F.nll_loss, callbacks=cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy, best fit\n",
    "This is the simplest method in which we recursively select the most probable next sequence character. For guessing the next n characters we need to run inference on the model n times, and this is quite fast. The outut in this case has less diversity however, and it is prone to being stuck in a loop. There seem to be a set of char sequences which are highly probable and the prediction often converges to  one of these sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Cat and the dog is a stranger.  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the s\n",
      "\n",
      "1. The sun was very sure i was a star with a star and starts to start a baby..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a\n",
      "\n",
      "2. Chicken cross the street.  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star w\n",
      "\n",
      "3. Why did the man who wants to see them..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a star with a shower..  i was a baby and the star was a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def greedy(inseq, n):\n",
    "    res = inseq\n",
    "    for i in range(n):\n",
    "        c = gen_greedy(inseq)\n",
    "        res += c\n",
    "        inseq = inseq[1:]+c\n",
    "    return res\n",
    "\n",
    "def gen_greedy(inp):\n",
    "    idxs = TEXT.numericalize(inp)\n",
    "    p = m(VV(idxs.transpose(0,1)))\n",
    "    val,r = torch.max(p[-1], 0)          # No need for expeonentiation\n",
    "    return TEXT.vocab.itos[to_np(r)[0]]\n",
    "\n",
    "print ('0. ' + greedy('Cat and the dog ', 200)+ '\\n')\n",
    "print ('1. ' + greedy('The sun was very', 200)+ '\\n')\n",
    "print ('2. ' + greedy('Chicken cross th', 200)+ '\\n')\n",
    "print ('3. ' + greedy('Why did the man ', 2000)+ '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial distribution\n",
    "Models the occurance of an outcome out of k categories each with defined probability of occurances. \n",
    "In order to inject more diversity into the outputs, instead of selecting the most probable haracter, we model the NN as a probability event and observe which token is output with one 'play'. This adds some serendipity into the system.\n",
    "\n",
    "* Generates different text results every time\n",
    "* Non repeating patterns ( more diversity in output)\n",
    "* words are not complete "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Cat and the dog was before for shin..  this music breaking and brible from doctor window).  i've were watching their paranoia..  god: some poses it's hick b*.  i'm worried about a doctor..  hubby: tell me who in fron\n",
      "\n",
      "1. The sun was very at..  anything dial story of this meal.  oh. no work! you say you to8 5) aak.  me: if you got a panty & we're impossible, in 2000 bad. then feet. code he tackled it from the social fight spider*.  i'\n",
      "\n",
      "2. Chicken cross the rack and i kill twenty months.  *god could be naked to the husband*.  we're a mavasion jr, here's someone eggs but me..  ok keep your cows of next weekend.  when we're dealing without people..  cat:\n",
      "\n",
      "3. Why did the man now that's nothing proud..  do you want once for this..  *silly school community).  i'll like to pile of that..  god: you can taker from colos..  advice liver, i haven't read them..  if the symbol for the same sad vodka..  her: omg. are you the just got to cereal until my wife death*.  my worst day when diy hell i get the best of my children!.  im dain but i'm good my father..  when an each other to men..  i take a trapped that do lizborw horror.  my ex-.  so pizza means what to hate me?.  my same morning is dishwashing it..  i don't need fun to right?!\".  my dad microwaves, i end on people receiving alaxma) okay thanks you're going to grole myper*.  you didn't know..  school tough member. i was scoring. m.  why do we wonder when you are..  [train ribs]: dude, sy i can be on far..  i'm not a foreball..  matter god: because you grarmed.  but about my abreaching was, i'd have no barking..  anyone have abroad..  i hate personal has slided him i.  why have a tombroom?.  *in room into human transform but.  he had a king of groat? me: lrop on the machine up while died & holds park* \"what?\" canto single men for easter\".  i described a popular food down..  wife: my wife on this skitch.  you're the way where we paper.  recreating the woods close so i know.  [insurance] \"he said...\".  me: sorry christmas again: what?.  i just handled cheese..  as i cannibal get voted..  land at the annion's floxid jime..  bumbles aren't going to sound of window casture.  friend: wait, that is helped..  someone sends one players.  [holding jokes in digferee] show your arm lots of ending a mouse*.  mommy costies to eat 32..  job i talk to \"security\".  either searches before..  a lamp will like the $7 knock on a math breakfast to watch this. hey!!!!.  if shirms in the new craigs. i don't listen a leg..  cute delicious of my toilet worry..  what is they trying to be how manybox?\".  what is a big fish..  table doctor: symploph.  [interviewerm]: i'm first where the first it?.  i'm going to tuss sil\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def multinomial(inp, n):\n",
    "    res = inp\n",
    "    for i in range(n):\n",
    "        c = gen_multinomial(inp)\n",
    "        res += c\n",
    "        inp = inp[1:]+c\n",
    "    return res\n",
    "\n",
    "def gen_multinomial(inp):\n",
    "    idxs = TEXT.numericalize(inp)\n",
    "    p = m(VV(idxs.transpose(0,1)))\n",
    "    r = torch.multinomial(p[-1].exp(), 1)\n",
    "    return TEXT.vocab.itos[to_np(r)[0]]\n",
    "\n",
    "print ('0. ' + multinomial('Cat and the dog ', 200)+ '\\n')\n",
    "print ('1. ' + multinomial('The sun was very', 200)+ '\\n')\n",
    "print ('2. ' + multinomial('Chicken cross th', 200)+ '\\n')\n",
    "print ('3. ' + multinomial('Why did the man ', 2000)+ '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy + Multinomial\n",
    "* Add diversity every now and then, perhaps after a space.\n",
    "* Results in diverse text compared to greedy approach\n",
    "* Words are fully complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Cat and the dog better.  what many dogs want to use for me..  what do we have been eating it.\".  i know how much i say someone will have comes out..  *starts a train for a really and be completely make down..  for ev\n",
      "\n",
      "1. The sun was very sure my girlfriend.  *starts handing the control of fire*.  i don't even know how to know about the new car\".  \"i said 'i put out of the house].  *starts drinking up to come so we invented a bathroom\n",
      "\n",
      "2. Chicken cross the world*.  because someone as good in your kids..  [starts starting to be really sure they say..  [starts just a real real thing..  very started for a woman..  don't really say my ex makes a man 4. ex\n",
      "\n",
      "3. Why did the man until people get the back..  her: so what u say you say anything.  don't put the ground completely because it only been on it..  [starts a baby] sometimes for fire..  the dog doesn't call a control..  because i make back so back..  what do really because they don't be completely the ground..  people are going to have a new baby.  i really ever want a complete movie.. her: but i have no use here. for me. : i have a new real face.\".  [starts fat in a facebook..  my boss 7 sees the first of you..  a train english guys..  you say the word \"what are you doing something..  one more completely for someone..  people have a baby completely with back.  i don't have a baby with it.\".  just look at front of being a baby.  i feel from the control dog to leave my son..  *starts starting to a baby*.  \"i said god is on a day.\".  *starts to look in her face*.  i really like my dad is really sure you are..  one fire the dog has 5 beers and really called me to start like the other guy said \"i can't know that..  cop: what is we pretty really because they love @ people know that me: hey god: so she got a job to start party*.  don't get a woman in real restaurant].  my dad needs for the street..  what do you like it?\" *starts back room*.  *starts to come to the police with my baby. like the most real of when i can tell me that she said..  a real mom can find out my mom..  i don't realize that means..  people know what my neighbor doesn't want to make a year like he said like they can be here.  what are i like you to me..  people just go and watch your face..  sometimes i was like 20 of her car..  what do you think they need me..  what do we think they don't have it.\".  my favorite guy on fire..  the job is free for like the dog..  5 year old to start from the house*.  [starts back to people..  people don't know for a baby..  one girl are well that should always starts with him with no way to put line like the new car dogs.  because i was 'starting contact].  what do not call it 'so they \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def combination(inp, n):\n",
    "    res = inp\n",
    "    for i in range(n):\n",
    "        #print('last char is: ',inp[-1])\n",
    "        if(inp[-1] == ' '):\n",
    "                #print('SPACY')\n",
    "                c = gen_multinomial(inp)\n",
    "        else:\n",
    "                c = gen_greedy(inp)\n",
    "        res += c\n",
    "        inp = inp[1:]+c\n",
    "    return res\n",
    "\n",
    "print ('0. ' + combination('Cat and the dog ', 200)+ '\\n')\n",
    "print ('1. ' + combination('The sun was very', 200)+ '\\n')\n",
    "print ('2. ' + combination('Chicken cross th', 200)+ '\\n')\n",
    "print ('3. ' + combination('Why did the man ', 2000)+ '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search\n",
    "Beam search is a method of trying to get more 'optimal results' and we look at predicting sequences longer than 1 character in one 'iteration'. Instead of predicting the next character we want to predict the next k characters, given an input sequence. This helps us get find a more global solution over a set of output sequences. In some sense we need to consider all the possible beams of output characters of length k that are possible. and need to have some metric of comparison. A couple of things help us out while calculating probabilitues: \n",
    "First, using bayes rule, we can model the probability of a getting a particular output sequence as aproduct of individual conditional probability sequences. \n",
    "\n",
    "For eg: Score of predicting 'and' given input equence 'The sun ' will be \n",
    "score = P('a'| input = 'The sun ') * P('n'| input = 'he sun a') * P('d' | input = 'e sun an')  \n",
    "Each of the P values above can be obtained by running \n",
    "Second, the output of the model softmas is often in log format, and this makes out implementations easier, we can add the log values instead of multiplying them. \n",
    "\n",
    "#### Computational considerations/Tweaks to Beam search\n",
    "* Attempt to reduce the bottleneck - > charscore function\n",
    "* Sequences of alphabets only _ 30 * 30 * 30 \n",
    "* Instead of sampling all combinations, most probable 10 in each case. \n",
    "    Significantly faster results OR get beam sequences of longer length\n",
    "    (10 *10 *10 *10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['a', 'a', 'a'], 1.0],\n",
       " [['a', 'a', 'b'], 1.0],\n",
       " [['a', 'a', 'c'], 1.0],\n",
       " [['a', 'a', 'd'], 1.0],\n",
       " [['a', 'a', 'e'], 1.0],\n",
       " [['a', 'a', 'f'], 1.0],\n",
       " [['a', 'a', 'g'], 1.0],\n",
       " [['a', 'a', 'h'], 1.0]]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining a set of characters used for doing beam search\n",
    "letters_tok = list(string.ascii_lowercase)\n",
    "letters_tok += [' ', '.','!','?']\n",
    "beam_tok = [[[i,j,k], 1.0] for i in letters_tok for j in letters_tok for k in letters_tok]\n",
    "beam_tok[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def charscore(inp, o):\n",
    "    idxs = TEXT.numericalize(inp)\n",
    "    o_idx= TEXT.numericalize(o)\n",
    "    o_idx=to_np(o_idx.view(1))[0]\n",
    "    p = m(VV(idxs.transpose(0,1)))\n",
    "    return to_np(p[-1][o_idx])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_text(start, sequence,cnt):\n",
    "    result = start\n",
    "    for i in range(cnt):\n",
    "        nxt = beam_search(start, sequence)\n",
    "        result = result +nxt\n",
    "        start = start[3:] + nxt\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 5s, sys: 2min 28s, total: 10min 34s\n",
      "Wall time: 10min 34s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The cat and the other questions'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def beam_search(start, sequence):\n",
    "    for s in sequence:\n",
    "        in1 = start[1:]+s[0][0]\n",
    "        in2 = start[2:]+s[0][0]+s[0][1]\n",
    "        #Sum the individual log probabilities\n",
    "        s[1] = charscore(start, s[0][0]) * charscore(in1, s[0][1]) * charscore(in2, s[0][2])\n",
    "    sortseq = sorted(sequence, key=lambda data:data[1])\n",
    "    return (sortseq[-1][0][0] + sortseq[-1][0][1] + sortseq[-1][0][2])  \n",
    "\n",
    "%time beam_text('The cat and the ',beam_tok,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy beam search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def charscore(inp, o):\n",
    "    idxs = TEXT.numericalize(inp)\n",
    "    o_idx= TEXT.numericalize(o)\n",
    "    o_idx=to_np(o_idx.view(1))[0]\n",
    "    p = m(VV(idxs.transpose(0,1)))\n",
    "    return to_np(p[-1][o_idx])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "[[[11, 3, 3], -4.2812085], [[5, 15, 21], -4.0765023], [[5, 15, 15], -4.073443]] omm\n",
      "-------------------------------------------------------------\n",
      "[[[6, 9, 9], -5.5405345], [[7, 4, 31], -3.407919], [[7, 4, 4], -3.4076414]] itt\n",
      "-------------------------------------------------------------\n",
      "[[[3, 8, 31], -7.3035517], [[17, 2, 2], -6.209165], [[17, 2, 31], -6.1800256]] . *\n",
      "-------------------------------------------------------------\n",
      "[[[20, 5, 13], -6.507964], [[9, 3, 3], -4.1909533], [[9, 3, 8], -4.1908092]] sen\n",
      "-------------------------------------------------------------\n",
      "[[[13, 9, 31], -9.24726], [[9, 3, 2], -8.7731285], [[9, 3, 9], -8.772476]] ses\n",
      "-------------------------------------------------------------\n",
      "[[[31, 17, 2], -8.168541], [[17, 2, 2], -7.3288054], [[17, 2, 15], -7.26964]] . m\n",
      "-------------------------------------------------------------\n",
      "[[[7, 9, 4], -5.359307], [[6, 10, 10], -4.5550294], [[6, 10, 7], -4.5548086]] ari\n",
      "-------------------------------------------------------------\n",
      "[[[5, 34, 17], -6.8906107], [[5, 17, 17], -5.614919], [[5, 17, 2], -5.6028748]] o. \n",
      "-------------------------------------------------------------\n",
      "[[[4, 5, 2], -7.5119123], [[6, 12, 19], -6.6881604], [[6, 12, 12], -6.684857]] all\n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 4], -7.183708], [[17, 17, 2], -5.361222], [[17, 17, 17], -5.351023]] ...\n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -2.434371], [[17, 17, 2], -2.4322867], [[17, 17, 17], -2.4305253]] ...\n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -3.103207], [[17, 17, 17], -1.4387708], [[17, 17, 2], -1.4381461]] .. \n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -5.06579], [[17, 17, 17], -3.6350913], [[17, 17, 2], -3.635075]] .. \n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -4.934175], [[17, 17, 2], -3.4405642], [[17, 17, 17], -3.440507]] ...\n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -3.1537914], [[17, 17, 17], -2.0359173], [[17, 17, 2], -2.0356917]] .. \n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -4.8815565], [[17, 17, 2], -3.5686197], [[17, 17, 17], -3.5685835]] ...\n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -3.2934852], [[17, 17, 17], -2.0835652], [[17, 17, 2], -2.08348]] .. \n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -4.9384375], [[17, 17, 2], -3.5640926], [[17, 17, 17], -3.5640783]] ...\n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -3.1504593], [[17, 17, 17], -2.067905], [[17, 17, 2], -2.0677042]] .. \n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -4.9384375], [[17, 17, 2], -3.5640926], [[17, 17, 17], -3.5640783]] ...\n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -3.1504593], [[17, 17, 17], -2.0679045], [[17, 17, 2], -2.0677037]] .. \n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -4.938437], [[17, 17, 2], -3.564093], [[17, 17, 17], -3.5640788]] ...\n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -3.1504593], [[17, 17, 17], -2.067905], [[17, 17, 2], -2.0677042]] .. \n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -4.9384356], [[17, 17, 2], -3.5640926], [[17, 17, 17], -3.5640783]] ...\n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -3.1504598], [[17, 17, 17], -2.067905], [[17, 17, 2], -2.0677042]] .. \n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -4.9384365], [[17, 17, 2], -3.5640926], [[17, 17, 17], -3.5640783]] ...\n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -3.1504588], [[17, 17, 17], -2.067905], [[17, 17, 2], -2.0677042]] .. \n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -4.938437], [[17, 17, 2], -3.5640926], [[17, 17, 17], -3.5640783]] ...\n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -3.1504593], [[17, 17, 17], -2.0679054], [[17, 17, 2], -2.0677047]] .. \n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -4.938437], [[17, 17, 2], -3.564093], [[17, 17, 17], -3.5640788]] ...\n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -3.1504598], [[17, 17, 17], -2.067905], [[17, 17, 2], -2.0677042]] .. \n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -4.9384365], [[17, 17, 2], -3.564093], [[17, 17, 17], -3.5640788]] ...\n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -3.1504598], [[17, 17, 17], -2.0679054], [[17, 17, 2], -2.0677047]] .. \n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -4.9384375], [[17, 17, 2], -3.564093], [[17, 17, 17], -3.5640788]] ...\n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -3.1504598], [[17, 17, 17], -2.0679054], [[17, 17, 2], -2.0677047]] .. \n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -4.938437], [[17, 17, 2], -3.564093], [[17, 17, 17], -3.5640788]] ...\n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -3.1504593], [[17, 17, 17], -2.0679054], [[17, 17, 2], -2.0677047]] .. \n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -4.9384365], [[17, 17, 2], -3.564093], [[17, 17, 17], -3.5640788]] ...\n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -3.1504593], [[17, 17, 17], -2.067905], [[17, 17, 2], -2.0677047]] .. \n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -4.938437], [[17, 17, 2], -3.564093], [[17, 17, 17], -3.5640788]] ...\n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -3.1504593], [[17, 17, 17], -2.067905], [[17, 17, 2], -2.0677042]] .. \n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -4.938437], [[17, 17, 2], -3.5640926], [[17, 17, 17], -3.5640783]] ...\n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -3.1504593], [[17, 17, 17], -2.067905], [[17, 17, 2], -2.0677042]] .. \n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -4.938437], [[17, 17, 2], -3.5640926], [[17, 17, 17], -3.5640783]] ...\n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -3.1504593], [[17, 17, 17], -2.067905], [[17, 17, 2], -2.0677042]] .. \n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -4.9384375], [[17, 17, 2], -3.5640926], [[17, 17, 17], -3.5640783]] ...\n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -3.1504593], [[17, 17, 17], -2.067905], [[17, 17, 2], -2.0677042]] .. \n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -4.9384365], [[17, 17, 2], -3.5640926], [[17, 17, 17], -3.5640783]] ...\n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -3.1504593], [[17, 17, 17], -2.067905], [[17, 17, 2], -2.0677042]] .. \n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -4.938438], [[17, 17, 2], -3.564093], [[17, 17, 17], -3.5640788]] ...\n",
      "CPU times: user 10.6 s, sys: 3.33 s, total: 13.9 s\n",
      "Wall time: 13.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'It is raining committ. *senses. mario. all........ .. ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ...'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen_next_preds(inp, n):\n",
    "    idxs = TEXT.numericalize(inp)\n",
    "    p = m(VV(idxs.transpose(0,1)))\n",
    "    a = to_np(p[-1])\n",
    "    top_n = sorted(range(len(a)), key=lambda i:a[i])[-n:]\n",
    "    return top_n\n",
    "\n",
    "def greedy_beam_sum(inp, n):\n",
    "    glist = list() \n",
    "    #3 iterations\n",
    "    for i in gen_next_preds(inp, 10): #(n//4 +1 )):\n",
    "        i_score = charscore(inp, TEXT.vocab.itos[i])\n",
    "        for j in gen_next_preds(inp[1:]+TEXT.vocab.itos[i], 4): #(n//2 +1)):\n",
    "            j_score = charscore(inp[1:] +TEXT.vocab.itos[i], TEXT.vocab.itos[j])\n",
    "            for k in gen_next_preds(inp[2:]+TEXT.vocab.itos[i]+TEXT.vocab.itos[j], 2): #n):\n",
    "                k_score = charscore(inp[2:] + TEXT.vocab.itos[i] + TEXT.vocab.itos[j],\\\n",
    "                                TEXT.vocab.itos[j])\n",
    "                #print(i,j,k)\n",
    "                #calculate score for this\n",
    "                #print([[i, j, k], i_score+j_score+k_score])\n",
    "                glist.append([[i, j, k], i_score + j_score + k_score])\n",
    "    #print(glist[-15:])\n",
    "    glist.sort(key=lambda data:data[1])\n",
    "    txt =[TEXT.vocab.itos[s[0][0]]+TEXT.vocab.itos[s[0][1]]+TEXT.vocab.itos[s[0][2]] for s in glist[-1:]][0]\n",
    "    print('-------------------------------------------------------------')\n",
    "    print(glist[-3:],txt)\n",
    "    return txt\n",
    "\n",
    "def greedy_beam_text(start, bw,iterations):\n",
    "    result = start\n",
    "    for i in range(iterations):\n",
    "        nxt = greedy_beam_sum(start, bw)\n",
    "        result = result +nxt\n",
    "        start = start[3:] + nxt\n",
    "    return result\n",
    "\n",
    "%time greedy_beam_text('It is raining c', 2, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "[[[11, 3, 3], -2.526533], [[5, 15, 21], -2.2517955], [[5, 15, 15], -2.2452993]] omm\n",
      "-------------------------------------------------------------\n",
      "[[[5, 8, 17], -0.19694364], [[7, 4, 31], -0.17071289], [[7, 4, 4], -0.17065953]] itt\n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 31], -0.10933553], [[7, 8, 3], -0.076965764], [[7, 8, 20], -0.07679697]] ing\n",
      "-------------------------------------------------------------\n",
      "[[[28, 2, 19], -0.87512106], [[17, 2, 2], -0.5610697], [[17, 2, 15], -0.42017627]] . m\n",
      "-------------------------------------------------------------\n",
      "[[[3, 28, 17], -0.60373896], [[16, 2, 13], -0.44613734], [[16, 2, 22], -0.44445533]] y f\n",
      "-------------------------------------------------------------\n",
      "[[[10, 7, 3], -0.80017924], [[14, 12, 12], -0.5424127], [[14, 12, 22], -0.5419542]] ulf\n",
      "-------------------------------------------------------------\n",
      "[[[10, 7, 3], -0.39439192], [[7, 12, 12], -0.37153286], [[7, 12, 15], -0.37047988]] ilm\n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 7], -1.235397], [[29, 2, 7], -0.26018882], [[29, 2, 4], -0.2586889]] , t\n",
      "-------------------------------------------------------------\n",
      "[[[11, 3, 16], -1.0906392], [[3, 12, 12], -0.05502646], [[3, 12, 3], -0.055018354]] ele\n",
      "-------------------------------------------------------------\n",
      "[[[15, 6, 8], -6.048965], [[17, 2, 2], -2.9133117], [[17, 2, 7], -2.3206143]] . i\n",
      "-------------------------------------------------------------\n",
      "[[[22, 2, 7], -0.05190582], [[25, 12, 15], -0.0059057716], [[25, 12, 12], -0.005895313]] 'll\n",
      "-------------------------------------------------------------\n",
      "[[[2, 9, 4], -0.021101134], [[2, 23, 3], -0.019783838], [[2, 23, 14], -0.019782493]]  bu\n",
      "-------------------------------------------------------------\n",
      "[[[7, 12, 13], -0.4086306], [[16, 2, 4], -0.13409221], [[16, 2, 6], -0.13380997]] y a\n",
      "-------------------------------------------------------------\n",
      "[[[2, 23, 6], -4.3074603], [[23, 5, 26], -3.5185616], [[23, 5, 14], -3.5181694]] bou\n",
      "-------------------------------------------------------------\n",
      "[[[4, 17, 17], -0.00011916467], [[4, 2, 6], -0.00010571814], [[4, 2, 4], -0.00010520659]] t t\n",
      "-------------------------------------------------------------\n",
      "[[[3, 12, 12], -0.9633512], [[11, 3, 2], -0.7272637], [[11, 3, 15], -0.7263899]] hem\n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 15], -0.47337237], [[29, 2, 7], -0.4360048], [[29, 2, 9], -0.42548797]] , s\n",
      "-------------------------------------------------------------\n",
      "[[[12, 3, 21], -0.26177526], [[5, 10, 10], -0.111012034], [[5, 10, 4], -0.11078613]] ort\n",
      "-------------------------------------------------------------\n",
      "[[[17, 2, 15], -0.75876987], [[29, 2, 7], -0.14997503], [[29, 2, 4], -0.1491311]] , t\n",
      "-------------------------------------------------------------\n",
      "[[[11, 3, 2], -1.0835388], [[3, 12, 12], -0.08126908], [[3, 12, 3], -0.08110181]] ele\n",
      "-------------------------------------------------------------\n",
      "[[[26, 3, 8], -9.635666], [[15, 6, 8], -5.9007096], [[15, 6, 4], -5.9003677]] mat\n",
      "-------------------------------------------------------------\n",
      "[[[11, 17, 17], -0.9991354], [[3, 17, 2], -0.3699369], [[3, 17, 17], -0.3647692]] e..\n",
      "-------------------------------------------------------------\n",
      "[[[2, 15, 3], -1.5684059], [[2, 2, 15], -0.0061191386], [[2, 2, 7], -0.0061170314]]   i\n",
      "-------------------------------------------------------------\n",
      "[[[22, 2, 16], -0.027755355], [[25, 12, 15], -0.01143982], [[25, 12, 12], -0.011430654]] 'll\n",
      "-------------------------------------------------------------\n",
      "[[[2, 23, 14], -0.005504412], [[2, 9, 3], -0.0055005318], [[2, 9, 4], -0.0054996586]]  st\n",
      "-------------------------------------------------------------\n",
      "[[[14, 22, 22], -0.23473206], [[7, 12, 24], -0.0002860296], [[7, 12, 12], -0.0002860296]] ill\n",
      "-------------------------------------------------------------\n",
      "[[[2, 9, 3], -0.10011401], [[29, 2, 9], -0.06278527], [[29, 2, 7], -0.062372487]] , i\n",
      "-------------------------------------------------------------\n",
      "[[[22, 2, 7], -0.15148808], [[25, 12, 15], -0.003183878], [[25, 12, 12], -0.003183878]] 'll\n",
      "-------------------------------------------------------------\n",
      "[[[2, 9, 3], -0.035373963], [[2, 23, 3], -0.031580336], [[2, 23, 14], -0.031568877]]  bu\n",
      "-------------------------------------------------------------\n",
      "[[[16, 2, 6], -0.25097346], [[16, 17, 17], -0.23873276], [[16, 17, 2], -0.23776047]] y. \n",
      "-------------------------------------------------------------\n",
      "[[[6, 12, 12], -4.078434], [[8, 5, 2], -2.6322327], [[8, 5, 4], -2.6321127]] not\n",
      "-------------------------------------------------------------\n",
      "[[[29, 2, 7], -0.27583477], [[11, 7, 9], -0.10391134], [[11, 7, 8], -0.10353683]] hin\n",
      "-------------------------------------------------------------\n",
      "[[[20, 2, 9], -0.032871343], [[20, 17, 17], -0.031200752], [[20, 17, 2], -0.030919997]] g. \n",
      "-------------------------------------------------------------\n",
      "[[[15, 3, 2], -4.6422524], [[8, 5, 2], -3.2531407], [[8, 5, 4], -3.2526455]] not\n",
      "-------------------------------------------------------------\n",
      "[[[29, 2, 7], -0.29227352], [[11, 7, 9], -0.1208121], [[11, 7, 8], -0.12061621]] hin\n",
      "-------------------------------------------------------------\n",
      "[[[20, 17, 2], -0.038651824], [[20, 2, 7], -0.026075985], [[20, 2, 9], -0.026044764]] g s\n",
      "-------------------------------------------------------------\n",
      "[[[5, 10, 10], -0.4204943], [[12, 3, 3], -0.2621161], [[12, 3, 21], -0.26113954]] lep\n",
      "-------------------------------------------------------------\n",
      "[[[4, 17, 17], -0.14315432], [[4, 2, 6], -0.062369484], [[4, 2, 7], -0.06192109]] t i\n",
      "-------------------------------------------------------------\n",
      "[[[22, 2, 16], -0.7878813], [[25, 12, 15], -0.01376369], [[25, 12, 12], -0.013742884]] 'll\n",
      "-------------------------------------------------------------\n",
      "[[[2, 9, 4], -0.08977524], [[2, 23, 10], -0.06492221], [[2, 23, 3], -0.06487886]]  be\n",
      "-------------------------------------------------------------\n",
      "[[[2, 6, 23], -1.4160142], [[2, 6, 2], -1.4156624], [[17, 2, 15], -1.1551818]] . m\n",
      "-------------------------------------------------------------\n",
      "[[[28, 2, 19], -1.3937178], [[16, 2, 13], -0.3951435], [[16, 2, 23], -0.39444846]] y b\n",
      "-------------------------------------------------------------\n",
      "[[[7, 12, 7], -0.21792294], [[5, 9, 4], -0.011917786], [[5, 9, 9], -0.011917786]] oss\n",
      "-------------------------------------------------------------\n",
      "[[[28, 2, 19], -0.30355522], [[29, 2, 7], -0.13769299], [[29, 2, 4], -0.13731264]] , t\n",
      "-------------------------------------------------------------\n",
      "[[[11, 3, 2], -0.7259395], [[3, 12, 3], -0.036745403], [[3, 12, 12], -0.036731914]] ell\n",
      "-------------------------------------------------------------\n",
      "[[[7, 8, 20], -0.38084114], [[29, 2, 16], -0.093828335], [[29, 2, 7], -0.09360848]] , i\n",
      "-------------------------------------------------------------\n",
      "[[[22, 2, 7], -0.0469238], [[25, 12, 15], -0.0046686283], [[25, 12, 12], -0.004536341]] 'll\n",
      "-------------------------------------------------------------\n",
      "[[[2, 9, 3], -0.027086698], [[2, 23, 14], -0.023951788], [[2, 23, 3], -0.023951158]]  be\n",
      "-------------------------------------------------------------\n",
      "[[[12, 7, 8], -1.6612607], [[2, 6, 23], -1.230324], [[2, 6, 2], -1.2298614]]  a \n",
      "-------------------------------------------------------------\n",
      "[[[23, 5, 5], -6.4355383], [[20, 5, 5], -1.3993855], [[20, 5, 13], -1.3974217]] god\n",
      "CPU times: user 10.5 s, sys: 3.45 s, total: 14 s\n",
      "Wall time: 14 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"It is raining committing. my fulfilm, tele. i'll buy about them, sort, telemate..  i'll still, i'll buy. nothing. nothing slept i'll be. my boss, tell, i'll be a god\""
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def greedy_beam_prod(inp, n):\n",
    "    glist = list() \n",
    "    #3 iterations\n",
    "    for i in gen_next_preds(inp, 10): #(n//4 +1 )):\n",
    "        i_score = charscore(inp, TEXT.vocab.itos[i])\n",
    "        for j in gen_next_preds(inp[1:]+TEXT.vocab.itos[i], 4): #(n//2 +1)):\n",
    "            j_score = charscore(inp[1:] +TEXT.vocab.itos[i], TEXT.vocab.itos[j])\n",
    "            for k in gen_next_preds(inp[2:]+TEXT.vocab.itos[i]+TEXT.vocab.itos[j], 2): #n):\n",
    "                k_score = charscore(inp[2:] + TEXT.vocab.itos[i] + TEXT.vocab.itos[j],\\\n",
    "                                TEXT.vocab.itos[j])\n",
    "                #print(i,j,k)\n",
    "                #calculate score for this\n",
    "                #print([[i, j, k], i_score+j_score+k_score])\n",
    "                glist.append([[i, j, k], i_score * j_score * k_score])\n",
    "    #print(glist[-15:])\n",
    "    glist.sort(key=lambda data:data[1])\n",
    "    txt =[TEXT.vocab.itos[s[0][0]]+TEXT.vocab.itos[s[0][1]]+TEXT.vocab.itos[s[0][2]] for s in glist[-1:]][0]\n",
    "    print('-------------------------------------------------------------')\n",
    "    print(glist[-3:],txt)\n",
    "    return txt\n",
    "\n",
    "def greedy_beam_prod_text(start, bw,iterations):\n",
    "    result = start\n",
    "    for i in range(iterations):\n",
    "        nxt = greedy_beam_prod(start, bw)\n",
    "        result = result +nxt\n",
    "        start = start[3:] + nxt\n",
    "    return result\n",
    "\n",
    "%time greedy_beam_prod_text('It is raining c', 2, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 7, 21, 15, 4, 5]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = 'The sun '\n",
    "idxs = TEXT.numericalize('The sun ')\n",
    "idxs\n",
    "p = m(VV(idxs.transpose(0,1)))\n",
    "a = to_np(p[-1])\n",
    "top_n = sorted(range(len(a)), key=lambda i:a[i])[-i:]\n",
    "top_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_next_preds(inp[1:]+'i',5)\n",
    "TEXT.vocab.itos[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[6, 12, 12], -3.3995795],\n",
       " [[6, 12, 15], -3.3994374],\n",
       " [[6, 12, 5], -3.3992705],\n",
       " [[6, 12, 3], -3.399075],\n",
       " [[6, 12, 18], -3.398847],\n",
       " [[6, 12, 7], -3.39858],\n",
       " [[6, 12, 21], -3.3982673],\n",
       " [[6, 12, 9], -3.397903],\n",
       " [[6, 12, 24], -3.3974776],\n",
       " [[6, 12, 10], -3.3969827],\n",
       " [[6, 12, 23], -3.3964076],\n",
       " [[6, 12, 22], -3.3957448],\n",
       " [[6, 12, 6], -3.3949924],\n",
       " [[6, 12, 4], -3.394174],\n",
       " [[6, 12, 14], -3.3934789]]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp='It is raining c'\n",
    "glist = list() \n",
    "#3 iterations\n",
    "for i in gen_next_preds(inp, 15):\n",
    "    i_score = charscore(inp, TEXT.vocab.itos[i])\n",
    "    for j in gen_next_preds(inp[1:]+TEXT.vocab.itos[i], 15):\n",
    "        j_score = charscore(inp[1:] +TEXT.vocab.itos[i], TEXT.vocab.itos[j])\n",
    "        for k in gen_next_preds(inp[2:]+TEXT.vocab.itos[i]+TEXT.vocab.itos[j], 15):\n",
    "            k_score = charscore(inp[2:] +TEXT.vocab.itos[i] + TEXT.vocab.itos[j],\\\n",
    "                                TEXT.vocab.itos[j])\n",
    "            #print(i,j,k)\n",
    "            #calculate score for this\n",
    "            glist.append([[i, j, k], i_score+j_score+k_score])\n",
    "glist.sort(key=lambda data:data[1])\n",
    "glist[-15:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXT.vocab.itos is enough to get mapping, you can create another mapping in the other ditection if necessary\n",
    "This has 88 elements, with the rare ones clubbed into <unk> s expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEXT.vocab.stoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix: Simple operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['t', 's', '“'], 1.0, False, False], [['t', 's', '”'], 1.0, False, False]]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = [[[i, j, k], 1.0, False, False] \\\n",
    "       for i in text_char[2:] for j in text_char[2:] for k in text_char[2:]]\n",
    "seq[15000:15002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Accessing variables\n",
      "[[' ', ' ', ' '], 1.0, False, False]\n",
      "[' ', ' ', ' ']\n",
      " \n",
      "1.0\n",
      "False\n",
      "***Beams with extreme scores:\n",
      "[[[' ', ' ', ' '], 1.0, False, False], [[' ', ' ', 'e'], 1.0, False, False], [[' ', ' ', 't'], 1.0, False, False]]\n",
      "[[['ñ', 'ñ', '\\ufeff'], 1.0, False, False], [['ñ', 'ñ', '~'], 1.0, False, False], [['ñ', 'ñ', 'ñ'], 1.0, False, False]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[[' ', ' ', ' '], 1.0, False, False],\n",
       " [[' ', ' ', 'e'], 1.0, False, False],\n",
       " [[' ', ' ', 't'], 1.0, False, False],\n",
       " [[' ', ' ', 'a'], 1.0, False, False],\n",
       " [[' ', ' ', 'o'], 1.0, False, False]]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accessing sequence items\n",
    "print(\"***Accessing variables\",seq[0],seq[0][0],seq[0][0][0], seq[0][1], seq[0][2],sep='\\n')\n",
    "\n",
    "#sorting items by their value\n",
    "sortseq = sorted(seq, key=lambda data:data[1])\n",
    "print(\"***Beams with extreme scores:\",sortseq[:3],sortseq[-3:], sep='\\n')\n",
    "\n",
    "#selecting all items in training set \n",
    "trnseq = [ s for s in seq if s[0] ]\n",
    "trnseq[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUTS \n",
    "#start = 'The carp' \n",
    "start = 'The sun ' \n",
    "o = 'r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "   18     5    11    21     3     9     4     3\n",
       "[torch.cuda.LongTensor of size 1x8 (GPU 0)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs = TEXT.numericalize(start)\n",
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_idx=TEXT.numericalize(o)\n",
    "o_idx=to_np(o_idx.view(1))[0]\n",
    "o_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.93800116"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate prediction score\n",
    "p = m(VV(idxs.transpose(0,1)))\n",
    "ans = to_np(p[-1][o_idx])[0]\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def charscore(inp, o):\n",
    "    idxs = TEXT.numericalize(inp)\n",
    "    o_idx= TEXT.numericalize(o)\n",
    "    o_idx=to_np(o_idx.view(1))[0]\n",
    "    p = m(VV(idxs.transpose(0,1)))\n",
    "    return to_np(p[-1][o_idx])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Calculate beam score, for each beam seq in seq\n",
    "#p[-1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9445963"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charscore('carpente', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.869677"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charscore('carpente', 'z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12.060574"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charscore('carpente', '$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "614125"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq[410][0]\n",
    "len(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.2084954\n",
      "-1.8183508\n",
      "-6.8331833\n"
     ]
    }
   ],
   "source": [
    "print(charscore(start, seq[110][0][0]))\n",
    "print(charscore(start, seq[110][0][1]))\n",
    "print(charscore(start, seq[110][0][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arpente='"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start[1:]+seq[320][0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['t', '-'], 1.0, False, False]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2 = [[[i, j], 1.0, False, False] \\\n",
    "       for i in text_char[2:41] for j in text_char[2:41] ]\n",
    "#print(len(seq2))\n",
    "seq2[110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 30 char sequence - sequence3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['a', 'a', 'a'], 1.0],\n",
       " [['a', 'a', 'b'], 1.0],\n",
       " [['a', 'a', 'c'], 1.0],\n",
       " [['a', 'a', 'd'], 1.0],\n",
       " [['a', 'a', 'e'], 1.0],\n",
       " [['a', 'a', 'f'], 1.0],\n",
       " [['a', 'a', 'g'], 1.0],\n",
       " [['a', 'a', 'h'], 1.0],\n",
       " [['a', 'a', 'i'], 1.0],\n",
       " [['a', 'a', 'j'], 1.0],\n",
       " [['a', 'a', 'k'], 1.0],\n",
       " [['a', 'a', 'l'], 1.0],\n",
       " [['a', 'a', 'm'], 1.0],\n",
       " [['a', 'a', 'n'], 1.0],\n",
       " [['a', 'a', 'o'], 1.0],\n",
       " [['a', 'a', 'p'], 1.0],\n",
       " [['a', 'a', 'q'], 1.0],\n",
       " [['a', 'a', 'r'], 1.0],\n",
       " [['a', 'a', 's'], 1.0],\n",
       " [['a', 'a', 't'], 1.0],\n",
       " [['a', 'a', 'u'], 1.0],\n",
       " [['a', 'a', 'v'], 1.0],\n",
       " [['a', 'a', 'w'], 1.0],\n",
       " [['a', 'a', 'x'], 1.0],\n",
       " [['a', 'a', 'y'], 1.0],\n",
       " [['a', 'a', 'z'], 1.0],\n",
       " [['a', 'b', 'a'], 1.0],\n",
       " [['a', 'b', 'b'], 1.0],\n",
       " [['a', 'b', 'c'], 1.0],\n",
       " [['a', 'b', 'd'], 1.0],\n",
       " [['a', 'b', 'e'], 1.0],\n",
       " [['a', 'b', 'f'], 1.0],\n",
       " [['a', 'b', 'g'], 1.0],\n",
       " [['a', 'b', 'h'], 1.0],\n",
       " [['a', 'b', 'i'], 1.0],\n",
       " [['a', 'b', 'j'], 1.0],\n",
       " [['a', 'b', 'k'], 1.0],\n",
       " [['a', 'b', 'l'], 1.0],\n",
       " [['a', 'b', 'm'], 1.0],\n",
       " [['a', 'b', 'n'], 1.0],\n",
       " [['a', 'b', 'o'], 1.0],\n",
       " [['a', 'b', 'p'], 1.0],\n",
       " [['a', 'b', 'q'], 1.0],\n",
       " [['a', 'b', 'r'], 1.0],\n",
       " [['a', 'b', 's'], 1.0],\n",
       " [['a', 'b', 't'], 1.0],\n",
       " [['a', 'b', 'u'], 1.0],\n",
       " [['a', 'b', 'v'], 1.0],\n",
       " [['a', 'b', 'w'], 1.0],\n",
       " [['a', 'b', 'x'], 1.0],\n",
       " [['a', 'b', 'y'], 1.0],\n",
       " [['a', 'b', 'z'], 1.0],\n",
       " [['a', 'c', 'a'], 1.0],\n",
       " [['a', 'c', 'b'], 1.0],\n",
       " [['a', 'c', 'c'], 1.0],\n",
       " [['a', 'c', 'd'], 1.0],\n",
       " [['a', 'c', 'e'], 1.0],\n",
       " [['a', 'c', 'f'], 1.0],\n",
       " [['a', 'c', 'g'], 1.0],\n",
       " [['a', 'c', 'h'], 1.0],\n",
       " [['a', 'c', 'i'], 1.0],\n",
       " [['a', 'c', 'j'], 1.0],\n",
       " [['a', 'c', 'k'], 1.0],\n",
       " [['a', 'c', 'l'], 1.0],\n",
       " [['a', 'c', 'm'], 1.0],\n",
       " [['a', 'c', 'n'], 1.0],\n",
       " [['a', 'c', 'o'], 1.0],\n",
       " [['a', 'c', 'p'], 1.0],\n",
       " [['a', 'c', 'q'], 1.0],\n",
       " [['a', 'c', 'r'], 1.0],\n",
       " [['a', 'c', 's'], 1.0],\n",
       " [['a', 'c', 't'], 1.0],\n",
       " [['a', 'c', 'u'], 1.0],\n",
       " [['a', 'c', 'v'], 1.0],\n",
       " [['a', 'c', 'w'], 1.0],\n",
       " [['a', 'c', 'x'], 1.0],\n",
       " [['a', 'c', 'y'], 1.0],\n",
       " [['a', 'c', 'z'], 1.0],\n",
       " [['a', 'd', 'a'], 1.0],\n",
       " [['a', 'd', 'b'], 1.0],\n",
       " [['a', 'd', 'c'], 1.0],\n",
       " [['a', 'd', 'd'], 1.0],\n",
       " [['a', 'd', 'e'], 1.0],\n",
       " [['a', 'd', 'f'], 1.0],\n",
       " [['a', 'd', 'g'], 1.0],\n",
       " [['a', 'd', 'h'], 1.0],\n",
       " [['a', 'd', 'i'], 1.0],\n",
       " [['a', 'd', 'j'], 1.0],\n",
       " [['a', 'd', 'k'], 1.0],\n",
       " [['a', 'd', 'l'], 1.0],\n",
       " [['a', 'd', 'm'], 1.0],\n",
       " [['a', 'd', 'n'], 1.0],\n",
       " [['a', 'd', 'o'], 1.0],\n",
       " [['a', 'd', 'p'], 1.0],\n",
       " [['a', 'd', 'q'], 1.0],\n",
       " [['a', 'd', 'r'], 1.0],\n",
       " [['a', 'd', 's'], 1.0],\n",
       " [['a', 'd', 't'], 1.0],\n",
       " [['a', 'd', 'u'], 1.0],\n",
       " [['a', 'd', 'v'], 1.0],\n",
       " [['a', 'd', 'w'], 1.0],\n",
       " [['a', 'd', 'x'], 1.0],\n",
       " [['a', 'd', 'y'], 1.0],\n",
       " [['a', 'd', 'z'], 1.0],\n",
       " [['a', 'e', 'a'], 1.0],\n",
       " [['a', 'e', 'b'], 1.0],\n",
       " [['a', 'e', 'c'], 1.0],\n",
       " [['a', 'e', 'd'], 1.0],\n",
       " [['a', 'e', 'e'], 1.0],\n",
       " [['a', 'e', 'f'], 1.0],\n",
       " [['a', 'e', 'g'], 1.0],\n",
       " [['a', 'e', 'h'], 1.0],\n",
       " [['a', 'e', 'i'], 1.0],\n",
       " [['a', 'e', 'j'], 1.0],\n",
       " [['a', 'e', 'k'], 1.0],\n",
       " [['a', 'e', 'l'], 1.0],\n",
       " [['a', 'e', 'm'], 1.0],\n",
       " [['a', 'e', 'n'], 1.0],\n",
       " [['a', 'e', 'o'], 1.0],\n",
       " [['a', 'e', 'p'], 1.0],\n",
       " [['a', 'e', 'q'], 1.0],\n",
       " [['a', 'e', 'r'], 1.0],\n",
       " [['a', 'e', 's'], 1.0],\n",
       " [['a', 'e', 't'], 1.0],\n",
       " [['a', 'e', 'u'], 1.0],\n",
       " [['a', 'e', 'v'], 1.0],\n",
       " [['a', 'e', 'w'], 1.0],\n",
       " [['a', 'e', 'x'], 1.0],\n",
       " [['a', 'e', 'y'], 1.0],\n",
       " [['a', 'e', 'z'], 1.0],\n",
       " [['a', 'f', 'a'], 1.0],\n",
       " [['a', 'f', 'b'], 1.0],\n",
       " [['a', 'f', 'c'], 1.0],\n",
       " [['a', 'f', 'd'], 1.0],\n",
       " [['a', 'f', 'e'], 1.0],\n",
       " [['a', 'f', 'f'], 1.0],\n",
       " [['a', 'f', 'g'], 1.0],\n",
       " [['a', 'f', 'h'], 1.0],\n",
       " [['a', 'f', 'i'], 1.0],\n",
       " [['a', 'f', 'j'], 1.0],\n",
       " [['a', 'f', 'k'], 1.0],\n",
       " [['a', 'f', 'l'], 1.0],\n",
       " [['a', 'f', 'm'], 1.0],\n",
       " [['a', 'f', 'n'], 1.0],\n",
       " [['a', 'f', 'o'], 1.0],\n",
       " [['a', 'f', 'p'], 1.0],\n",
       " [['a', 'f', 'q'], 1.0],\n",
       " [['a', 'f', 'r'], 1.0],\n",
       " [['a', 'f', 's'], 1.0],\n",
       " [['a', 'f', 't'], 1.0],\n",
       " [['a', 'f', 'u'], 1.0],\n",
       " [['a', 'f', 'v'], 1.0],\n",
       " [['a', 'f', 'w'], 1.0],\n",
       " [['a', 'f', 'x'], 1.0],\n",
       " [['a', 'f', 'y'], 1.0],\n",
       " [['a', 'f', 'z'], 1.0],\n",
       " [['a', 'g', 'a'], 1.0],\n",
       " [['a', 'g', 'b'], 1.0],\n",
       " [['a', 'g', 'c'], 1.0],\n",
       " [['a', 'g', 'd'], 1.0],\n",
       " [['a', 'g', 'e'], 1.0],\n",
       " [['a', 'g', 'f'], 1.0],\n",
       " [['a', 'g', 'g'], 1.0],\n",
       " [['a', 'g', 'h'], 1.0],\n",
       " [['a', 'g', 'i'], 1.0],\n",
       " [['a', 'g', 'j'], 1.0],\n",
       " [['a', 'g', 'k'], 1.0],\n",
       " [['a', 'g', 'l'], 1.0],\n",
       " [['a', 'g', 'm'], 1.0],\n",
       " [['a', 'g', 'n'], 1.0],\n",
       " [['a', 'g', 'o'], 1.0],\n",
       " [['a', 'g', 'p'], 1.0],\n",
       " [['a', 'g', 'q'], 1.0],\n",
       " [['a', 'g', 'r'], 1.0],\n",
       " [['a', 'g', 's'], 1.0],\n",
       " [['a', 'g', 't'], 1.0],\n",
       " [['a', 'g', 'u'], 1.0],\n",
       " [['a', 'g', 'v'], 1.0],\n",
       " [['a', 'g', 'w'], 1.0],\n",
       " [['a', 'g', 'x'], 1.0],\n",
       " [['a', 'g', 'y'], 1.0],\n",
       " [['a', 'g', 'z'], 1.0],\n",
       " [['a', 'h', 'a'], 1.0],\n",
       " [['a', 'h', 'b'], 1.0],\n",
       " [['a', 'h', 'c'], 1.0],\n",
       " [['a', 'h', 'd'], 1.0],\n",
       " [['a', 'h', 'e'], 1.0],\n",
       " [['a', 'h', 'f'], 1.0],\n",
       " [['a', 'h', 'g'], 1.0],\n",
       " [['a', 'h', 'h'], 1.0],\n",
       " [['a', 'h', 'i'], 1.0],\n",
       " [['a', 'h', 'j'], 1.0],\n",
       " [['a', 'h', 'k'], 1.0],\n",
       " [['a', 'h', 'l'], 1.0],\n",
       " [['a', 'h', 'm'], 1.0],\n",
       " [['a', 'h', 'n'], 1.0],\n",
       " [['a', 'h', 'o'], 1.0],\n",
       " [['a', 'h', 'p'], 1.0],\n",
       " [['a', 'h', 'q'], 1.0],\n",
       " [['a', 'h', 'r'], 1.0],\n",
       " [['a', 'h', 's'], 1.0],\n",
       " [['a', 'h', 't'], 1.0],\n",
       " [['a', 'h', 'u'], 1.0],\n",
       " [['a', 'h', 'v'], 1.0],\n",
       " [['a', 'h', 'w'], 1.0],\n",
       " [['a', 'h', 'x'], 1.0],\n",
       " [['a', 'h', 'y'], 1.0],\n",
       " [['a', 'h', 'z'], 1.0],\n",
       " [['a', 'i', 'a'], 1.0],\n",
       " [['a', 'i', 'b'], 1.0],\n",
       " [['a', 'i', 'c'], 1.0],\n",
       " [['a', 'i', 'd'], 1.0],\n",
       " [['a', 'i', 'e'], 1.0],\n",
       " [['a', 'i', 'f'], 1.0],\n",
       " [['a', 'i', 'g'], 1.0],\n",
       " [['a', 'i', 'h'], 1.0],\n",
       " [['a', 'i', 'i'], 1.0],\n",
       " [['a', 'i', 'j'], 1.0],\n",
       " [['a', 'i', 'k'], 1.0],\n",
       " [['a', 'i', 'l'], 1.0],\n",
       " [['a', 'i', 'm'], 1.0],\n",
       " [['a', 'i', 'n'], 1.0],\n",
       " [['a', 'i', 'o'], 1.0],\n",
       " [['a', 'i', 'p'], 1.0],\n",
       " [['a', 'i', 'q'], 1.0],\n",
       " [['a', 'i', 'r'], 1.0],\n",
       " [['a', 'i', 's'], 1.0],\n",
       " [['a', 'i', 't'], 1.0],\n",
       " [['a', 'i', 'u'], 1.0],\n",
       " [['a', 'i', 'v'], 1.0],\n",
       " [['a', 'i', 'w'], 1.0],\n",
       " [['a', 'i', 'x'], 1.0],\n",
       " [['a', 'i', 'y'], 1.0],\n",
       " [['a', 'i', 'z'], 1.0],\n",
       " [['a', 'j', 'a'], 1.0],\n",
       " [['a', 'j', 'b'], 1.0],\n",
       " [['a', 'j', 'c'], 1.0],\n",
       " [['a', 'j', 'd'], 1.0],\n",
       " [['a', 'j', 'e'], 1.0],\n",
       " [['a', 'j', 'f'], 1.0],\n",
       " [['a', 'j', 'g'], 1.0],\n",
       " [['a', 'j', 'h'], 1.0],\n",
       " [['a', 'j', 'i'], 1.0],\n",
       " [['a', 'j', 'j'], 1.0],\n",
       " [['a', 'j', 'k'], 1.0],\n",
       " [['a', 'j', 'l'], 1.0],\n",
       " [['a', 'j', 'm'], 1.0],\n",
       " [['a', 'j', 'n'], 1.0],\n",
       " [['a', 'j', 'o'], 1.0],\n",
       " [['a', 'j', 'p'], 1.0],\n",
       " [['a', 'j', 'q'], 1.0],\n",
       " [['a', 'j', 'r'], 1.0],\n",
       " [['a', 'j', 's'], 1.0],\n",
       " [['a', 'j', 't'], 1.0],\n",
       " [['a', 'j', 'u'], 1.0],\n",
       " [['a', 'j', 'v'], 1.0],\n",
       " [['a', 'j', 'w'], 1.0],\n",
       " [['a', 'j', 'x'], 1.0],\n",
       " [['a', 'j', 'y'], 1.0],\n",
       " [['a', 'j', 'z'], 1.0],\n",
       " [['a', 'k', 'a'], 1.0],\n",
       " [['a', 'k', 'b'], 1.0],\n",
       " [['a', 'k', 'c'], 1.0],\n",
       " [['a', 'k', 'd'], 1.0],\n",
       " [['a', 'k', 'e'], 1.0],\n",
       " [['a', 'k', 'f'], 1.0],\n",
       " [['a', 'k', 'g'], 1.0],\n",
       " [['a', 'k', 'h'], 1.0],\n",
       " [['a', 'k', 'i'], 1.0],\n",
       " [['a', 'k', 'j'], 1.0],\n",
       " [['a', 'k', 'k'], 1.0],\n",
       " [['a', 'k', 'l'], 1.0],\n",
       " [['a', 'k', 'm'], 1.0],\n",
       " [['a', 'k', 'n'], 1.0],\n",
       " [['a', 'k', 'o'], 1.0],\n",
       " [['a', 'k', 'p'], 1.0],\n",
       " [['a', 'k', 'q'], 1.0],\n",
       " [['a', 'k', 'r'], 1.0],\n",
       " [['a', 'k', 's'], 1.0],\n",
       " [['a', 'k', 't'], 1.0],\n",
       " [['a', 'k', 'u'], 1.0],\n",
       " [['a', 'k', 'v'], 1.0],\n",
       " [['a', 'k', 'w'], 1.0],\n",
       " [['a', 'k', 'x'], 1.0],\n",
       " [['a', 'k', 'y'], 1.0],\n",
       " [['a', 'k', 'z'], 1.0],\n",
       " [['a', 'l', 'a'], 1.0],\n",
       " [['a', 'l', 'b'], 1.0],\n",
       " [['a', 'l', 'c'], 1.0],\n",
       " [['a', 'l', 'd'], 1.0],\n",
       " [['a', 'l', 'e'], 1.0],\n",
       " [['a', 'l', 'f'], 1.0],\n",
       " [['a', 'l', 'g'], 1.0],\n",
       " [['a', 'l', 'h'], 1.0],\n",
       " [['a', 'l', 'i'], 1.0],\n",
       " [['a', 'l', 'j'], 1.0],\n",
       " [['a', 'l', 'k'], 1.0],\n",
       " [['a', 'l', 'l'], 1.0],\n",
       " [['a', 'l', 'm'], 1.0],\n",
       " [['a', 'l', 'n'], 1.0],\n",
       " [['a', 'l', 'o'], 1.0],\n",
       " [['a', 'l', 'p'], 1.0],\n",
       " [['a', 'l', 'q'], 1.0],\n",
       " [['a', 'l', 'r'], 1.0],\n",
       " [['a', 'l', 's'], 1.0],\n",
       " [['a', 'l', 't'], 1.0],\n",
       " [['a', 'l', 'u'], 1.0],\n",
       " [['a', 'l', 'v'], 1.0],\n",
       " [['a', 'l', 'w'], 1.0],\n",
       " [['a', 'l', 'x'], 1.0],\n",
       " [['a', 'l', 'y'], 1.0],\n",
       " [['a', 'l', 'z'], 1.0],\n",
       " [['a', 'm', 'a'], 1.0],\n",
       " [['a', 'm', 'b'], 1.0],\n",
       " [['a', 'm', 'c'], 1.0],\n",
       " [['a', 'm', 'd'], 1.0],\n",
       " [['a', 'm', 'e'], 1.0],\n",
       " [['a', 'm', 'f'], 1.0],\n",
       " [['a', 'm', 'g'], 1.0],\n",
       " [['a', 'm', 'h'], 1.0],\n",
       " [['a', 'm', 'i'], 1.0],\n",
       " [['a', 'm', 'j'], 1.0],\n",
       " [['a', 'm', 'k'], 1.0],\n",
       " [['a', 'm', 'l'], 1.0],\n",
       " [['a', 'm', 'm'], 1.0],\n",
       " [['a', 'm', 'n'], 1.0],\n",
       " [['a', 'm', 'o'], 1.0],\n",
       " [['a', 'm', 'p'], 1.0],\n",
       " [['a', 'm', 'q'], 1.0],\n",
       " [['a', 'm', 'r'], 1.0],\n",
       " [['a', 'm', 's'], 1.0],\n",
       " [['a', 'm', 't'], 1.0],\n",
       " [['a', 'm', 'u'], 1.0],\n",
       " [['a', 'm', 'v'], 1.0],\n",
       " [['a', 'm', 'w'], 1.0],\n",
       " [['a', 'm', 'x'], 1.0],\n",
       " [['a', 'm', 'y'], 1.0],\n",
       " [['a', 'm', 'z'], 1.0],\n",
       " [['a', 'n', 'a'], 1.0],\n",
       " [['a', 'n', 'b'], 1.0],\n",
       " [['a', 'n', 'c'], 1.0],\n",
       " [['a', 'n', 'd'], 1.0],\n",
       " [['a', 'n', 'e'], 1.0],\n",
       " [['a', 'n', 'f'], 1.0],\n",
       " [['a', 'n', 'g'], 1.0],\n",
       " [['a', 'n', 'h'], 1.0],\n",
       " [['a', 'n', 'i'], 1.0],\n",
       " [['a', 'n', 'j'], 1.0],\n",
       " [['a', 'n', 'k'], 1.0],\n",
       " [['a', 'n', 'l'], 1.0],\n",
       " [['a', 'n', 'm'], 1.0],\n",
       " [['a', 'n', 'n'], 1.0],\n",
       " [['a', 'n', 'o'], 1.0],\n",
       " [['a', 'n', 'p'], 1.0],\n",
       " [['a', 'n', 'q'], 1.0],\n",
       " [['a', 'n', 'r'], 1.0],\n",
       " [['a', 'n', 's'], 1.0],\n",
       " [['a', 'n', 't'], 1.0],\n",
       " [['a', 'n', 'u'], 1.0],\n",
       " [['a', 'n', 'v'], 1.0],\n",
       " [['a', 'n', 'w'], 1.0],\n",
       " [['a', 'n', 'x'], 1.0],\n",
       " [['a', 'n', 'y'], 1.0],\n",
       " [['a', 'n', 'z'], 1.0],\n",
       " [['a', 'o', 'a'], 1.0],\n",
       " [['a', 'o', 'b'], 1.0],\n",
       " [['a', 'o', 'c'], 1.0],\n",
       " [['a', 'o', 'd'], 1.0],\n",
       " [['a', 'o', 'e'], 1.0],\n",
       " [['a', 'o', 'f'], 1.0],\n",
       " [['a', 'o', 'g'], 1.0],\n",
       " [['a', 'o', 'h'], 1.0],\n",
       " [['a', 'o', 'i'], 1.0],\n",
       " [['a', 'o', 'j'], 1.0],\n",
       " [['a', 'o', 'k'], 1.0],\n",
       " [['a', 'o', 'l'], 1.0],\n",
       " [['a', 'o', 'm'], 1.0],\n",
       " [['a', 'o', 'n'], 1.0],\n",
       " [['a', 'o', 'o'], 1.0],\n",
       " [['a', 'o', 'p'], 1.0],\n",
       " [['a', 'o', 'q'], 1.0],\n",
       " [['a', 'o', 'r'], 1.0],\n",
       " [['a', 'o', 's'], 1.0],\n",
       " [['a', 'o', 't'], 1.0],\n",
       " [['a', 'o', 'u'], 1.0],\n",
       " [['a', 'o', 'v'], 1.0],\n",
       " [['a', 'o', 'w'], 1.0],\n",
       " [['a', 'o', 'x'], 1.0],\n",
       " [['a', 'o', 'y'], 1.0],\n",
       " [['a', 'o', 'z'], 1.0],\n",
       " [['a', 'p', 'a'], 1.0],\n",
       " [['a', 'p', 'b'], 1.0],\n",
       " [['a', 'p', 'c'], 1.0],\n",
       " [['a', 'p', 'd'], 1.0],\n",
       " [['a', 'p', 'e'], 1.0],\n",
       " [['a', 'p', 'f'], 1.0],\n",
       " [['a', 'p', 'g'], 1.0],\n",
       " [['a', 'p', 'h'], 1.0],\n",
       " [['a', 'p', 'i'], 1.0],\n",
       " [['a', 'p', 'j'], 1.0],\n",
       " [['a', 'p', 'k'], 1.0],\n",
       " [['a', 'p', 'l'], 1.0],\n",
       " [['a', 'p', 'm'], 1.0],\n",
       " [['a', 'p', 'n'], 1.0],\n",
       " [['a', 'p', 'o'], 1.0],\n",
       " [['a', 'p', 'p'], 1.0],\n",
       " [['a', 'p', 'q'], 1.0],\n",
       " [['a', 'p', 'r'], 1.0],\n",
       " [['a', 'p', 's'], 1.0],\n",
       " [['a', 'p', 't'], 1.0],\n",
       " [['a', 'p', 'u'], 1.0],\n",
       " [['a', 'p', 'v'], 1.0],\n",
       " [['a', 'p', 'w'], 1.0],\n",
       " [['a', 'p', 'x'], 1.0],\n",
       " [['a', 'p', 'y'], 1.0],\n",
       " [['a', 'p', 'z'], 1.0],\n",
       " [['a', 'q', 'a'], 1.0],\n",
       " [['a', 'q', 'b'], 1.0],\n",
       " [['a', 'q', 'c'], 1.0],\n",
       " [['a', 'q', 'd'], 1.0],\n",
       " [['a', 'q', 'e'], 1.0],\n",
       " [['a', 'q', 'f'], 1.0],\n",
       " [['a', 'q', 'g'], 1.0],\n",
       " [['a', 'q', 'h'], 1.0],\n",
       " [['a', 'q', 'i'], 1.0],\n",
       " [['a', 'q', 'j'], 1.0],\n",
       " [['a', 'q', 'k'], 1.0],\n",
       " [['a', 'q', 'l'], 1.0],\n",
       " [['a', 'q', 'm'], 1.0],\n",
       " [['a', 'q', 'n'], 1.0],\n",
       " [['a', 'q', 'o'], 1.0],\n",
       " [['a', 'q', 'p'], 1.0],\n",
       " [['a', 'q', 'q'], 1.0],\n",
       " [['a', 'q', 'r'], 1.0],\n",
       " [['a', 'q', 's'], 1.0],\n",
       " [['a', 'q', 't'], 1.0],\n",
       " [['a', 'q', 'u'], 1.0],\n",
       " [['a', 'q', 'v'], 1.0],\n",
       " [['a', 'q', 'w'], 1.0],\n",
       " [['a', 'q', 'x'], 1.0],\n",
       " [['a', 'q', 'y'], 1.0],\n",
       " [['a', 'q', 'z'], 1.0],\n",
       " [['a', 'r', 'a'], 1.0],\n",
       " [['a', 'r', 'b'], 1.0],\n",
       " [['a', 'r', 'c'], 1.0],\n",
       " [['a', 'r', 'd'], 1.0],\n",
       " [['a', 'r', 'e'], 1.0],\n",
       " [['a', 'r', 'f'], 1.0],\n",
       " [['a', 'r', 'g'], 1.0],\n",
       " [['a', 'r', 'h'], 1.0],\n",
       " [['a', 'r', 'i'], 1.0],\n",
       " [['a', 'r', 'j'], 1.0],\n",
       " [['a', 'r', 'k'], 1.0],\n",
       " [['a', 'r', 'l'], 1.0],\n",
       " [['a', 'r', 'm'], 1.0],\n",
       " [['a', 'r', 'n'], 1.0],\n",
       " [['a', 'r', 'o'], 1.0],\n",
       " [['a', 'r', 'p'], 1.0],\n",
       " [['a', 'r', 'q'], 1.0],\n",
       " [['a', 'r', 'r'], 1.0],\n",
       " [['a', 'r', 's'], 1.0],\n",
       " [['a', 'r', 't'], 1.0],\n",
       " [['a', 'r', 'u'], 1.0],\n",
       " [['a', 'r', 'v'], 1.0],\n",
       " [['a', 'r', 'w'], 1.0],\n",
       " [['a', 'r', 'x'], 1.0],\n",
       " [['a', 'r', 'y'], 1.0],\n",
       " [['a', 'r', 'z'], 1.0],\n",
       " [['a', 's', 'a'], 1.0],\n",
       " [['a', 's', 'b'], 1.0],\n",
       " [['a', 's', 'c'], 1.0],\n",
       " [['a', 's', 'd'], 1.0],\n",
       " [['a', 's', 'e'], 1.0],\n",
       " [['a', 's', 'f'], 1.0],\n",
       " [['a', 's', 'g'], 1.0],\n",
       " [['a', 's', 'h'], 1.0],\n",
       " [['a', 's', 'i'], 1.0],\n",
       " [['a', 's', 'j'], 1.0],\n",
       " [['a', 's', 'k'], 1.0],\n",
       " [['a', 's', 'l'], 1.0],\n",
       " [['a', 's', 'm'], 1.0],\n",
       " [['a', 's', 'n'], 1.0],\n",
       " [['a', 's', 'o'], 1.0],\n",
       " [['a', 's', 'p'], 1.0],\n",
       " [['a', 's', 'q'], 1.0],\n",
       " [['a', 's', 'r'], 1.0],\n",
       " [['a', 's', 's'], 1.0],\n",
       " [['a', 's', 't'], 1.0],\n",
       " [['a', 's', 'u'], 1.0],\n",
       " [['a', 's', 'v'], 1.0],\n",
       " [['a', 's', 'w'], 1.0],\n",
       " [['a', 's', 'x'], 1.0],\n",
       " [['a', 's', 'y'], 1.0],\n",
       " [['a', 's', 'z'], 1.0],\n",
       " [['a', 't', 'a'], 1.0],\n",
       " [['a', 't', 'b'], 1.0],\n",
       " [['a', 't', 'c'], 1.0],\n",
       " [['a', 't', 'd'], 1.0],\n",
       " [['a', 't', 'e'], 1.0],\n",
       " [['a', 't', 'f'], 1.0],\n",
       " [['a', 't', 'g'], 1.0],\n",
       " [['a', 't', 'h'], 1.0],\n",
       " [['a', 't', 'i'], 1.0],\n",
       " [['a', 't', 'j'], 1.0],\n",
       " [['a', 't', 'k'], 1.0],\n",
       " [['a', 't', 'l'], 1.0],\n",
       " [['a', 't', 'm'], 1.0],\n",
       " [['a', 't', 'n'], 1.0],\n",
       " [['a', 't', 'o'], 1.0],\n",
       " [['a', 't', 'p'], 1.0],\n",
       " [['a', 't', 'q'], 1.0],\n",
       " [['a', 't', 'r'], 1.0],\n",
       " [['a', 't', 's'], 1.0],\n",
       " [['a', 't', 't'], 1.0],\n",
       " [['a', 't', 'u'], 1.0],\n",
       " [['a', 't', 'v'], 1.0],\n",
       " [['a', 't', 'w'], 1.0],\n",
       " [['a', 't', 'x'], 1.0],\n",
       " [['a', 't', 'y'], 1.0],\n",
       " [['a', 't', 'z'], 1.0],\n",
       " [['a', 'u', 'a'], 1.0],\n",
       " [['a', 'u', 'b'], 1.0],\n",
       " [['a', 'u', 'c'], 1.0],\n",
       " [['a', 'u', 'd'], 1.0],\n",
       " [['a', 'u', 'e'], 1.0],\n",
       " [['a', 'u', 'f'], 1.0],\n",
       " [['a', 'u', 'g'], 1.0],\n",
       " [['a', 'u', 'h'], 1.0],\n",
       " [['a', 'u', 'i'], 1.0],\n",
       " [['a', 'u', 'j'], 1.0],\n",
       " [['a', 'u', 'k'], 1.0],\n",
       " [['a', 'u', 'l'], 1.0],\n",
       " [['a', 'u', 'm'], 1.0],\n",
       " [['a', 'u', 'n'], 1.0],\n",
       " [['a', 'u', 'o'], 1.0],\n",
       " [['a', 'u', 'p'], 1.0],\n",
       " [['a', 'u', 'q'], 1.0],\n",
       " [['a', 'u', 'r'], 1.0],\n",
       " [['a', 'u', 's'], 1.0],\n",
       " [['a', 'u', 't'], 1.0],\n",
       " [['a', 'u', 'u'], 1.0],\n",
       " [['a', 'u', 'v'], 1.0],\n",
       " [['a', 'u', 'w'], 1.0],\n",
       " [['a', 'u', 'x'], 1.0],\n",
       " [['a', 'u', 'y'], 1.0],\n",
       " [['a', 'u', 'z'], 1.0],\n",
       " [['a', 'v', 'a'], 1.0],\n",
       " [['a', 'v', 'b'], 1.0],\n",
       " [['a', 'v', 'c'], 1.0],\n",
       " [['a', 'v', 'd'], 1.0],\n",
       " [['a', 'v', 'e'], 1.0],\n",
       " [['a', 'v', 'f'], 1.0],\n",
       " [['a', 'v', 'g'], 1.0],\n",
       " [['a', 'v', 'h'], 1.0],\n",
       " [['a', 'v', 'i'], 1.0],\n",
       " [['a', 'v', 'j'], 1.0],\n",
       " [['a', 'v', 'k'], 1.0],\n",
       " [['a', 'v', 'l'], 1.0],\n",
       " [['a', 'v', 'm'], 1.0],\n",
       " [['a', 'v', 'n'], 1.0],\n",
       " [['a', 'v', 'o'], 1.0],\n",
       " [['a', 'v', 'p'], 1.0],\n",
       " [['a', 'v', 'q'], 1.0],\n",
       " [['a', 'v', 'r'], 1.0],\n",
       " [['a', 'v', 's'], 1.0],\n",
       " [['a', 'v', 't'], 1.0],\n",
       " [['a', 'v', 'u'], 1.0],\n",
       " [['a', 'v', 'v'], 1.0],\n",
       " [['a', 'v', 'w'], 1.0],\n",
       " [['a', 'v', 'x'], 1.0],\n",
       " [['a', 'v', 'y'], 1.0],\n",
       " [['a', 'v', 'z'], 1.0],\n",
       " [['a', 'w', 'a'], 1.0],\n",
       " [['a', 'w', 'b'], 1.0],\n",
       " [['a', 'w', 'c'], 1.0],\n",
       " [['a', 'w', 'd'], 1.0],\n",
       " [['a', 'w', 'e'], 1.0],\n",
       " [['a', 'w', 'f'], 1.0],\n",
       " [['a', 'w', 'g'], 1.0],\n",
       " [['a', 'w', 'h'], 1.0],\n",
       " [['a', 'w', 'i'], 1.0],\n",
       " [['a', 'w', 'j'], 1.0],\n",
       " [['a', 'w', 'k'], 1.0],\n",
       " [['a', 'w', 'l'], 1.0],\n",
       " [['a', 'w', 'm'], 1.0],\n",
       " [['a', 'w', 'n'], 1.0],\n",
       " [['a', 'w', 'o'], 1.0],\n",
       " [['a', 'w', 'p'], 1.0],\n",
       " [['a', 'w', 'q'], 1.0],\n",
       " [['a', 'w', 'r'], 1.0],\n",
       " [['a', 'w', 's'], 1.0],\n",
       " [['a', 'w', 't'], 1.0],\n",
       " [['a', 'w', 'u'], 1.0],\n",
       " [['a', 'w', 'v'], 1.0],\n",
       " [['a', 'w', 'w'], 1.0],\n",
       " [['a', 'w', 'x'], 1.0],\n",
       " [['a', 'w', 'y'], 1.0],\n",
       " [['a', 'w', 'z'], 1.0],\n",
       " [['a', 'x', 'a'], 1.0],\n",
       " [['a', 'x', 'b'], 1.0],\n",
       " [['a', 'x', 'c'], 1.0],\n",
       " [['a', 'x', 'd'], 1.0],\n",
       " [['a', 'x', 'e'], 1.0],\n",
       " [['a', 'x', 'f'], 1.0],\n",
       " [['a', 'x', 'g'], 1.0],\n",
       " [['a', 'x', 'h'], 1.0],\n",
       " [['a', 'x', 'i'], 1.0],\n",
       " [['a', 'x', 'j'], 1.0],\n",
       " [['a', 'x', 'k'], 1.0],\n",
       " [['a', 'x', 'l'], 1.0],\n",
       " [['a', 'x', 'm'], 1.0],\n",
       " [['a', 'x', 'n'], 1.0],\n",
       " [['a', 'x', 'o'], 1.0],\n",
       " [['a', 'x', 'p'], 1.0],\n",
       " [['a', 'x', 'q'], 1.0],\n",
       " [['a', 'x', 'r'], 1.0],\n",
       " [['a', 'x', 's'], 1.0],\n",
       " [['a', 'x', 't'], 1.0],\n",
       " [['a', 'x', 'u'], 1.0],\n",
       " [['a', 'x', 'v'], 1.0],\n",
       " [['a', 'x', 'w'], 1.0],\n",
       " [['a', 'x', 'x'], 1.0],\n",
       " [['a', 'x', 'y'], 1.0],\n",
       " [['a', 'x', 'z'], 1.0],\n",
       " [['a', 'y', 'a'], 1.0],\n",
       " [['a', 'y', 'b'], 1.0],\n",
       " [['a', 'y', 'c'], 1.0],\n",
       " [['a', 'y', 'd'], 1.0],\n",
       " [['a', 'y', 'e'], 1.0],\n",
       " [['a', 'y', 'f'], 1.0],\n",
       " [['a', 'y', 'g'], 1.0],\n",
       " [['a', 'y', 'h'], 1.0],\n",
       " [['a', 'y', 'i'], 1.0],\n",
       " [['a', 'y', 'j'], 1.0],\n",
       " [['a', 'y', 'k'], 1.0],\n",
       " [['a', 'y', 'l'], 1.0],\n",
       " [['a', 'y', 'm'], 1.0],\n",
       " [['a', 'y', 'n'], 1.0],\n",
       " [['a', 'y', 'o'], 1.0],\n",
       " [['a', 'y', 'p'], 1.0],\n",
       " [['a', 'y', 'q'], 1.0],\n",
       " [['a', 'y', 'r'], 1.0],\n",
       " [['a', 'y', 's'], 1.0],\n",
       " [['a', 'y', 't'], 1.0],\n",
       " [['a', 'y', 'u'], 1.0],\n",
       " [['a', 'y', 'v'], 1.0],\n",
       " [['a', 'y', 'w'], 1.0],\n",
       " [['a', 'y', 'x'], 1.0],\n",
       " [['a', 'y', 'y'], 1.0],\n",
       " [['a', 'y', 'z'], 1.0],\n",
       " [['a', 'z', 'a'], 1.0],\n",
       " [['a', 'z', 'b'], 1.0],\n",
       " [['a', 'z', 'c'], 1.0],\n",
       " [['a', 'z', 'd'], 1.0],\n",
       " [['a', 'z', 'e'], 1.0],\n",
       " [['a', 'z', 'f'], 1.0],\n",
       " [['a', 'z', 'g'], 1.0],\n",
       " [['a', 'z', 'h'], 1.0],\n",
       " [['a', 'z', 'i'], 1.0],\n",
       " [['a', 'z', 'j'], 1.0],\n",
       " [['a', 'z', 'k'], 1.0],\n",
       " [['a', 'z', 'l'], 1.0],\n",
       " [['a', 'z', 'm'], 1.0],\n",
       " [['a', 'z', 'n'], 1.0],\n",
       " [['a', 'z', 'o'], 1.0],\n",
       " [['a', 'z', 'p'], 1.0],\n",
       " [['a', 'z', 'q'], 1.0],\n",
       " [['a', 'z', 'r'], 1.0],\n",
       " [['a', 'z', 's'], 1.0],\n",
       " [['a', 'z', 't'], 1.0],\n",
       " [['a', 'z', 'u'], 1.0],\n",
       " [['a', 'z', 'v'], 1.0],\n",
       " [['a', 'z', 'w'], 1.0],\n",
       " [['a', 'z', 'x'], 1.0],\n",
       " [['a', 'z', 'y'], 1.0],\n",
       " [['a', 'z', 'z'], 1.0],\n",
       " [['b', 'a', 'a'], 1.0],\n",
       " [['b', 'a', 'b'], 1.0],\n",
       " [['b', 'a', 'c'], 1.0],\n",
       " [['b', 'a', 'd'], 1.0],\n",
       " [['b', 'a', 'e'], 1.0],\n",
       " [['b', 'a', 'f'], 1.0],\n",
       " [['b', 'a', 'g'], 1.0],\n",
       " [['b', 'a', 'h'], 1.0],\n",
       " [['b', 'a', 'i'], 1.0],\n",
       " [['b', 'a', 'j'], 1.0],\n",
       " [['b', 'a', 'k'], 1.0],\n",
       " [['b', 'a', 'l'], 1.0],\n",
       " [['b', 'a', 'm'], 1.0],\n",
       " [['b', 'a', 'n'], 1.0],\n",
       " [['b', 'a', 'o'], 1.0],\n",
       " [['b', 'a', 'p'], 1.0],\n",
       " [['b', 'a', 'q'], 1.0],\n",
       " [['b', 'a', 'r'], 1.0],\n",
       " [['b', 'a', 's'], 1.0],\n",
       " [['b', 'a', 't'], 1.0],\n",
       " [['b', 'a', 'u'], 1.0],\n",
       " [['b', 'a', 'v'], 1.0],\n",
       " [['b', 'a', 'w'], 1.0],\n",
       " [['b', 'a', 'x'], 1.0],\n",
       " [['b', 'a', 'y'], 1.0],\n",
       " [['b', 'a', 'z'], 1.0],\n",
       " [['b', 'b', 'a'], 1.0],\n",
       " [['b', 'b', 'b'], 1.0],\n",
       " [['b', 'b', 'c'], 1.0],\n",
       " [['b', 'b', 'd'], 1.0],\n",
       " [['b', 'b', 'e'], 1.0],\n",
       " [['b', 'b', 'f'], 1.0],\n",
       " [['b', 'b', 'g'], 1.0],\n",
       " [['b', 'b', 'h'], 1.0],\n",
       " [['b', 'b', 'i'], 1.0],\n",
       " [['b', 'b', 'j'], 1.0],\n",
       " [['b', 'b', 'k'], 1.0],\n",
       " [['b', 'b', 'l'], 1.0],\n",
       " [['b', 'b', 'm'], 1.0],\n",
       " [['b', 'b', 'n'], 1.0],\n",
       " [['b', 'b', 'o'], 1.0],\n",
       " [['b', 'b', 'p'], 1.0],\n",
       " [['b', 'b', 'q'], 1.0],\n",
       " [['b', 'b', 'r'], 1.0],\n",
       " [['b', 'b', 's'], 1.0],\n",
       " [['b', 'b', 't'], 1.0],\n",
       " [['b', 'b', 'u'], 1.0],\n",
       " [['b', 'b', 'v'], 1.0],\n",
       " [['b', 'b', 'w'], 1.0],\n",
       " [['b', 'b', 'x'], 1.0],\n",
       " [['b', 'b', 'y'], 1.0],\n",
       " [['b', 'b', 'z'], 1.0],\n",
       " [['b', 'c', 'a'], 1.0],\n",
       " [['b', 'c', 'b'], 1.0],\n",
       " [['b', 'c', 'c'], 1.0],\n",
       " [['b', 'c', 'd'], 1.0],\n",
       " [['b', 'c', 'e'], 1.0],\n",
       " [['b', 'c', 'f'], 1.0],\n",
       " [['b', 'c', 'g'], 1.0],\n",
       " [['b', 'c', 'h'], 1.0],\n",
       " [['b', 'c', 'i'], 1.0],\n",
       " [['b', 'c', 'j'], 1.0],\n",
       " [['b', 'c', 'k'], 1.0],\n",
       " [['b', 'c', 'l'], 1.0],\n",
       " [['b', 'c', 'm'], 1.0],\n",
       " [['b', 'c', 'n'], 1.0],\n",
       " [['b', 'c', 'o'], 1.0],\n",
       " [['b', 'c', 'p'], 1.0],\n",
       " [['b', 'c', 'q'], 1.0],\n",
       " [['b', 'c', 'r'], 1.0],\n",
       " [['b', 'c', 's'], 1.0],\n",
       " [['b', 'c', 't'], 1.0],\n",
       " [['b', 'c', 'u'], 1.0],\n",
       " [['b', 'c', 'v'], 1.0],\n",
       " [['b', 'c', 'w'], 1.0],\n",
       " [['b', 'c', 'x'], 1.0],\n",
       " [['b', 'c', 'y'], 1.0],\n",
       " [['b', 'c', 'z'], 1.0],\n",
       " [['b', 'd', 'a'], 1.0],\n",
       " [['b', 'd', 'b'], 1.0],\n",
       " [['b', 'd', 'c'], 1.0],\n",
       " [['b', 'd', 'd'], 1.0],\n",
       " [['b', 'd', 'e'], 1.0],\n",
       " [['b', 'd', 'f'], 1.0],\n",
       " [['b', 'd', 'g'], 1.0],\n",
       " [['b', 'd', 'h'], 1.0],\n",
       " [['b', 'd', 'i'], 1.0],\n",
       " [['b', 'd', 'j'], 1.0],\n",
       " [['b', 'd', 'k'], 1.0],\n",
       " [['b', 'd', 'l'], 1.0],\n",
       " [['b', 'd', 'm'], 1.0],\n",
       " [['b', 'd', 'n'], 1.0],\n",
       " [['b', 'd', 'o'], 1.0],\n",
       " [['b', 'd', 'p'], 1.0],\n",
       " [['b', 'd', 'q'], 1.0],\n",
       " [['b', 'd', 'r'], 1.0],\n",
       " [['b', 'd', 's'], 1.0],\n",
       " [['b', 'd', 't'], 1.0],\n",
       " [['b', 'd', 'u'], 1.0],\n",
       " [['b', 'd', 'v'], 1.0],\n",
       " [['b', 'd', 'w'], 1.0],\n",
       " [['b', 'd', 'x'], 1.0],\n",
       " [['b', 'd', 'y'], 1.0],\n",
       " [['b', 'd', 'z'], 1.0],\n",
       " [['b', 'e', 'a'], 1.0],\n",
       " [['b', 'e', 'b'], 1.0],\n",
       " [['b', 'e', 'c'], 1.0],\n",
       " [['b', 'e', 'd'], 1.0],\n",
       " [['b', 'e', 'e'], 1.0],\n",
       " [['b', 'e', 'f'], 1.0],\n",
       " [['b', 'e', 'g'], 1.0],\n",
       " [['b', 'e', 'h'], 1.0],\n",
       " [['b', 'e', 'i'], 1.0],\n",
       " [['b', 'e', 'j'], 1.0],\n",
       " [['b', 'e', 'k'], 1.0],\n",
       " [['b', 'e', 'l'], 1.0],\n",
       " [['b', 'e', 'm'], 1.0],\n",
       " [['b', 'e', 'n'], 1.0],\n",
       " [['b', 'e', 'o'], 1.0],\n",
       " [['b', 'e', 'p'], 1.0],\n",
       " [['b', 'e', 'q'], 1.0],\n",
       " [['b', 'e', 'r'], 1.0],\n",
       " [['b', 'e', 's'], 1.0],\n",
       " [['b', 'e', 't'], 1.0],\n",
       " [['b', 'e', 'u'], 1.0],\n",
       " [['b', 'e', 'v'], 1.0],\n",
       " [['b', 'e', 'w'], 1.0],\n",
       " [['b', 'e', 'x'], 1.0],\n",
       " [['b', 'e', 'y'], 1.0],\n",
       " [['b', 'e', 'z'], 1.0],\n",
       " [['b', 'f', 'a'], 1.0],\n",
       " [['b', 'f', 'b'], 1.0],\n",
       " [['b', 'f', 'c'], 1.0],\n",
       " [['b', 'f', 'd'], 1.0],\n",
       " [['b', 'f', 'e'], 1.0],\n",
       " [['b', 'f', 'f'], 1.0],\n",
       " [['b', 'f', 'g'], 1.0],\n",
       " [['b', 'f', 'h'], 1.0],\n",
       " [['b', 'f', 'i'], 1.0],\n",
       " [['b', 'f', 'j'], 1.0],\n",
       " [['b', 'f', 'k'], 1.0],\n",
       " [['b', 'f', 'l'], 1.0],\n",
       " [['b', 'f', 'm'], 1.0],\n",
       " [['b', 'f', 'n'], 1.0],\n",
       " [['b', 'f', 'o'], 1.0],\n",
       " [['b', 'f', 'p'], 1.0],\n",
       " [['b', 'f', 'q'], 1.0],\n",
       " [['b', 'f', 'r'], 1.0],\n",
       " [['b', 'f', 's'], 1.0],\n",
       " [['b', 'f', 't'], 1.0],\n",
       " [['b', 'f', 'u'], 1.0],\n",
       " [['b', 'f', 'v'], 1.0],\n",
       " [['b', 'f', 'w'], 1.0],\n",
       " [['b', 'f', 'x'], 1.0],\n",
       " [['b', 'f', 'y'], 1.0],\n",
       " [['b', 'f', 'z'], 1.0],\n",
       " [['b', 'g', 'a'], 1.0],\n",
       " [['b', 'g', 'b'], 1.0],\n",
       " [['b', 'g', 'c'], 1.0],\n",
       " [['b', 'g', 'd'], 1.0],\n",
       " [['b', 'g', 'e'], 1.0],\n",
       " [['b', 'g', 'f'], 1.0],\n",
       " [['b', 'g', 'g'], 1.0],\n",
       " [['b', 'g', 'h'], 1.0],\n",
       " [['b', 'g', 'i'], 1.0],\n",
       " [['b', 'g', 'j'], 1.0],\n",
       " [['b', 'g', 'k'], 1.0],\n",
       " [['b', 'g', 'l'], 1.0],\n",
       " [['b', 'g', 'm'], 1.0],\n",
       " [['b', 'g', 'n'], 1.0],\n",
       " [['b', 'g', 'o'], 1.0],\n",
       " [['b', 'g', 'p'], 1.0],\n",
       " [['b', 'g', 'q'], 1.0],\n",
       " [['b', 'g', 'r'], 1.0],\n",
       " [['b', 'g', 's'], 1.0],\n",
       " [['b', 'g', 't'], 1.0],\n",
       " [['b', 'g', 'u'], 1.0],\n",
       " [['b', 'g', 'v'], 1.0],\n",
       " [['b', 'g', 'w'], 1.0],\n",
       " [['b', 'g', 'x'], 1.0],\n",
       " [['b', 'g', 'y'], 1.0],\n",
       " [['b', 'g', 'z'], 1.0],\n",
       " [['b', 'h', 'a'], 1.0],\n",
       " [['b', 'h', 'b'], 1.0],\n",
       " [['b', 'h', 'c'], 1.0],\n",
       " [['b', 'h', 'd'], 1.0],\n",
       " [['b', 'h', 'e'], 1.0],\n",
       " [['b', 'h', 'f'], 1.0],\n",
       " [['b', 'h', 'g'], 1.0],\n",
       " [['b', 'h', 'h'], 1.0],\n",
       " [['b', 'h', 'i'], 1.0],\n",
       " [['b', 'h', 'j'], 1.0],\n",
       " [['b', 'h', 'k'], 1.0],\n",
       " [['b', 'h', 'l'], 1.0],\n",
       " [['b', 'h', 'm'], 1.0],\n",
       " [['b', 'h', 'n'], 1.0],\n",
       " [['b', 'h', 'o'], 1.0],\n",
       " [['b', 'h', 'p'], 1.0],\n",
       " [['b', 'h', 'q'], 1.0],\n",
       " [['b', 'h', 'r'], 1.0],\n",
       " [['b', 'h', 's'], 1.0],\n",
       " [['b', 'h', 't'], 1.0],\n",
       " [['b', 'h', 'u'], 1.0],\n",
       " [['b', 'h', 'v'], 1.0],\n",
       " [['b', 'h', 'w'], 1.0],\n",
       " [['b', 'h', 'x'], 1.0],\n",
       " [['b', 'h', 'y'], 1.0],\n",
       " [['b', 'h', 'z'], 1.0],\n",
       " [['b', 'i', 'a'], 1.0],\n",
       " [['b', 'i', 'b'], 1.0],\n",
       " [['b', 'i', 'c'], 1.0],\n",
       " [['b', 'i', 'd'], 1.0],\n",
       " [['b', 'i', 'e'], 1.0],\n",
       " [['b', 'i', 'f'], 1.0],\n",
       " [['b', 'i', 'g'], 1.0],\n",
       " [['b', 'i', 'h'], 1.0],\n",
       " [['b', 'i', 'i'], 1.0],\n",
       " [['b', 'i', 'j'], 1.0],\n",
       " [['b', 'i', 'k'], 1.0],\n",
       " [['b', 'i', 'l'], 1.0],\n",
       " [['b', 'i', 'm'], 1.0],\n",
       " [['b', 'i', 'n'], 1.0],\n",
       " [['b', 'i', 'o'], 1.0],\n",
       " [['b', 'i', 'p'], 1.0],\n",
       " [['b', 'i', 'q'], 1.0],\n",
       " [['b', 'i', 'r'], 1.0],\n",
       " [['b', 'i', 's'], 1.0],\n",
       " [['b', 'i', 't'], 1.0],\n",
       " [['b', 'i', 'u'], 1.0],\n",
       " [['b', 'i', 'v'], 1.0],\n",
       " [['b', 'i', 'w'], 1.0],\n",
       " [['b', 'i', 'x'], 1.0],\n",
       " [['b', 'i', 'y'], 1.0],\n",
       " [['b', 'i', 'z'], 1.0],\n",
       " [['b', 'j', 'a'], 1.0],\n",
       " [['b', 'j', 'b'], 1.0],\n",
       " [['b', 'j', 'c'], 1.0],\n",
       " [['b', 'j', 'd'], 1.0],\n",
       " [['b', 'j', 'e'], 1.0],\n",
       " [['b', 'j', 'f'], 1.0],\n",
       " [['b', 'j', 'g'], 1.0],\n",
       " [['b', 'j', 'h'], 1.0],\n",
       " [['b', 'j', 'i'], 1.0],\n",
       " [['b', 'j', 'j'], 1.0],\n",
       " [['b', 'j', 'k'], 1.0],\n",
       " [['b', 'j', 'l'], 1.0],\n",
       " [['b', 'j', 'm'], 1.0],\n",
       " [['b', 'j', 'n'], 1.0],\n",
       " [['b', 'j', 'o'], 1.0],\n",
       " [['b', 'j', 'p'], 1.0],\n",
       " [['b', 'j', 'q'], 1.0],\n",
       " [['b', 'j', 'r'], 1.0],\n",
       " [['b', 'j', 's'], 1.0],\n",
       " [['b', 'j', 't'], 1.0],\n",
       " [['b', 'j', 'u'], 1.0],\n",
       " [['b', 'j', 'v'], 1.0],\n",
       " [['b', 'j', 'w'], 1.0],\n",
       " [['b', 'j', 'x'], 1.0],\n",
       " [['b', 'j', 'y'], 1.0],\n",
       " [['b', 'j', 'z'], 1.0],\n",
       " [['b', 'k', 'a'], 1.0],\n",
       " [['b', 'k', 'b'], 1.0],\n",
       " [['b', 'k', 'c'], 1.0],\n",
       " [['b', 'k', 'd'], 1.0],\n",
       " [['b', 'k', 'e'], 1.0],\n",
       " [['b', 'k', 'f'], 1.0],\n",
       " [['b', 'k', 'g'], 1.0],\n",
       " [['b', 'k', 'h'], 1.0],\n",
       " [['b', 'k', 'i'], 1.0],\n",
       " [['b', 'k', 'j'], 1.0],\n",
       " [['b', 'k', 'k'], 1.0],\n",
       " [['b', 'k', 'l'], 1.0],\n",
       " [['b', 'k', 'm'], 1.0],\n",
       " [['b', 'k', 'n'], 1.0],\n",
       " [['b', 'k', 'o'], 1.0],\n",
       " [['b', 'k', 'p'], 1.0],\n",
       " [['b', 'k', 'q'], 1.0],\n",
       " [['b', 'k', 'r'], 1.0],\n",
       " [['b', 'k', 's'], 1.0],\n",
       " [['b', 'k', 't'], 1.0],\n",
       " [['b', 'k', 'u'], 1.0],\n",
       " [['b', 'k', 'v'], 1.0],\n",
       " [['b', 'k', 'w'], 1.0],\n",
       " [['b', 'k', 'x'], 1.0],\n",
       " [['b', 'k', 'y'], 1.0],\n",
       " [['b', 'k', 'z'], 1.0],\n",
       " [['b', 'l', 'a'], 1.0],\n",
       " [['b', 'l', 'b'], 1.0],\n",
       " [['b', 'l', 'c'], 1.0],\n",
       " [['b', 'l', 'd'], 1.0],\n",
       " [['b', 'l', 'e'], 1.0],\n",
       " [['b', 'l', 'f'], 1.0],\n",
       " [['b', 'l', 'g'], 1.0],\n",
       " [['b', 'l', 'h'], 1.0],\n",
       " [['b', 'l', 'i'], 1.0],\n",
       " [['b', 'l', 'j'], 1.0],\n",
       " [['b', 'l', 'k'], 1.0],\n",
       " [['b', 'l', 'l'], 1.0],\n",
       " [['b', 'l', 'm'], 1.0],\n",
       " [['b', 'l', 'n'], 1.0],\n",
       " [['b', 'l', 'o'], 1.0],\n",
       " [['b', 'l', 'p'], 1.0],\n",
       " [['b', 'l', 'q'], 1.0],\n",
       " [['b', 'l', 'r'], 1.0],\n",
       " [['b', 'l', 's'], 1.0],\n",
       " [['b', 'l', 't'], 1.0],\n",
       " [['b', 'l', 'u'], 1.0],\n",
       " [['b', 'l', 'v'], 1.0],\n",
       " [['b', 'l', 'w'], 1.0],\n",
       " [['b', 'l', 'x'], 1.0],\n",
       " [['b', 'l', 'y'], 1.0],\n",
       " [['b', 'l', 'z'], 1.0],\n",
       " [['b', 'm', 'a'], 1.0],\n",
       " [['b', 'm', 'b'], 1.0],\n",
       " [['b', 'm', 'c'], 1.0],\n",
       " [['b', 'm', 'd'], 1.0],\n",
       " [['b', 'm', 'e'], 1.0],\n",
       " [['b', 'm', 'f'], 1.0],\n",
       " [['b', 'm', 'g'], 1.0],\n",
       " [['b', 'm', 'h'], 1.0],\n",
       " [['b', 'm', 'i'], 1.0],\n",
       " [['b', 'm', 'j'], 1.0],\n",
       " [['b', 'm', 'k'], 1.0],\n",
       " [['b', 'm', 'l'], 1.0],\n",
       " ...]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letters = list(string.ascii_lowercase)\n",
    "letters +[' ', '.','!','?']\n",
    "seq3 = [[[i,j,k], 1.0] for i in letters for j in letters for k in letters]\n",
    "seq3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHich sequence is the best suited for text start? \n",
    "for s in seq2:\n",
    "    #calculate s[1]\n",
    "    s[1] = charscore(start, s[0][0]) + charscore(start[1:]+s[0][0], s[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(start, sequence):\n",
    "    for s in sequence:\n",
    "        in1 = start[1:]+s[0][0]\n",
    "        in2 = start[2:]+s[0][0]+s[0][1]\n",
    "        s[1] = charscore(start, s[0][0]) + charscore(in1, s[0][1]) + charscore(in2, s[0][2])\n",
    "    sortseq = sorted(sequence, key=lambda data:data[1])\n",
    "    return (sortseq[-1][0][0] + sortseq[-1][0][1] + sortseq[-1][0][2])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_text(start, sequence,cnt):\n",
    "    result = start\n",
    "    for i in range(cnt):\n",
    "        nxt = beam_search(start, sequence)\n",
    "        result = result +nxt\n",
    "        start = start[3:] + nxt\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The sun andtheres'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_text(start,seq3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The sun andtheresticallywheresiationallysistershopped'"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Beam search resulted in a sequence like below, with no spaces\n",
    "beam_text(start,seq3, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHich sequence is the best suited for text start? \n",
    "for s in seq3:\n",
    "    in1 = start[1:]+s[0][0]\n",
    "    in2 = start[2:]+s[0][0]+s[0][1]\n",
    "    s[1] = charscore(start, s[0][0]) + charscore(in1, s[0][1]) + charscore(in2, s[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The sun '"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seq3)\n",
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['t', 'h', 'e'], -4.4925756],\n",
       " [['y', 'o', 'u'], -4.188985],\n",
       " [['a', 'n', 'd'], -4.0476913]]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sorting items by their value\n",
    "sortseq = sorted(seq3, key=lambda data:data[1])\n",
    "sortseq[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and'"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortseq[-1][0][0] + sortseq[-1][0][1] + sortseq[-1][0][2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = m(VV(idxs.transpose(0,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last layer activations corresponding to each token\n",
    "Displaying last 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-12.2267\n",
       "-12.6297\n",
       " -5.9591\n",
       " -5.0845\n",
       " -6.8274\n",
       " -7.6905\n",
       " -8.2778\n",
       " -8.8224\n",
       " -4.3681\n",
       " -5.2212\n",
       "[torch.cuda.FloatTensor of size 10 (GPU 0)]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[-1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'r'"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#i = np.argmax(to_np(p))\n",
    "output = np.argmax(to_np(p[-1]))\n",
    "TEXT.vocab.itos[output]\n",
    "#output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 88 symbols... the last few tokes of this symbol are assigned to 0's \n",
    "#TEXT.vocab.stoi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "216px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
